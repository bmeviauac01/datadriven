{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Data-driven systems","text":""},{"location":"#data-driven-systems","title":"Data-driven systems","text":"<p>Lecture notes, seminar materials and homework exercises for course BMEVIAUAC01 Data-driven systems.</p> <p>Pull requests welcome</p> <p>As a student of this course you can earn extra points by contributing to the materials! Open a pull request to fix an error or contribute to the materials in any way! Check the link to the repository in the upper right corner.</p> <p>License</p> <p>The materials provided here are created for the students of course BMEVIAUAC01. The usage of these materials outside the scope of teaching or learning this particular course is only granted if the source and authors are contributed.</p> <p>These materials are to be viewed within the context of the course. For any other usage scenarios the material is provided as-is.</p>"},{"location":"db/","title":"Sample database scheme","text":""},{"location":"db/#sample-database-scheme","title":"Sample database scheme","text":"<p>The examples and seminars during the semester will use a sample database. The database is a simplified retail management system with products, customers, and orders. This description details the relational schema of the database; the MongoDB variant is the appropriate mirror of this scheme.</p>"},{"location":"db/#the-context-of-the-database","title":"The context of the database","text":"<p>The system is designed to help the retail process of products. The products are grouped into hierarchical categories. Customers can browse products, place orders, and track the status of the orders.</p> <p>Customers can have multiple sites (e.g., retail stores of one company with multiple addresses). The order can be completed to any of these sites. Each customer has exactly one \"main site,\" which is where the invoices are addressed. An order can have multiple items; each item has its status.</p> <p>An invoice is printed if the order is ready. An invoice cannot be changed once it is created. Different products have different VAT (value-added tax) rates. These VAT rates are subject to change over time, but these changes must not affect existing invoices.</p>"},{"location":"db/#data-scheme","title":"Data scheme","text":"<p>The model of the whole database is depicted below.</p> <p></p>"},{"location":"db/#tables-and-columns","title":"Tables and columns","text":"Table Column Description VAT ID Auto-generated primary key. Percentage The percentage of the value-added tax. PaymentMethod ID Auto-generated primary key. Method Short name of the payment method, e.g., cash or wire transfer. Deadline The deadline of the payment method, that is, the deadline for completing the transaction after the invoice is received. Status ID Auto-generated primary key. Name Short name of the status (e.g., new, processed). Category ID Auto-generated primary key. Name Name of the category, e.g., toys, LEGO, etc. ParentCategoryID Foreign key indicating the parent category; null if this is a top-level category. Product ID Auto-generated primary key. Name Product name. Price Product price without tax. Stock Amount of this product in stock. VATID Foreign key to the VAT table. CategoryID Foreign key to the Category table. Description XML description of the product. Customer ID Auto-generated primary key. Name Customer name. BankAccount Back account number of the customer. Login Login name for the webshop. Password Password for the webshop. Email Email address of the customer. MainCustomerSiteID The main site of the customer; a foreign key to the CustomerSite table. CustomerSite ID Auto-generated primary key. ZipCode The zip code of the address. City The city part of the address. Street The street and house number part of the address. Tel Telephone number. Fax Fax number. CustomerID Foreign key to the Customer table. Order ID Auto-generated primary key. Date Date when the order was placed. Deadline Deadline until the order must be completed. CustomerSiteID Foreign key to the CustomerSite table; the order is billed and shipped to this site. StatusID Foreign key to the Status table; the actual status of the order. PaymentMethodID Foreign key to the PaymentMethod table; the chosen method of payment. OrderItem ID Auto-generated primary key. Amount The amount ordered of the specific product. Price The unit price of the product; by default this is the price of the product, but can be altered (e.g. for bulk order). OrderID Foreign key to the Order table; identifier the order this item belongs to. ProductID Foreign key to the Product table; identifies the product that is ordered. StatusID Foreign key to the Status table; the actual status of the item. InvoiceIssuer ID Auto-generated primary key. Name Name of the company selling the products. ZipCode The zip code of the address. City The city part of the address. Street The street and house number part of the address. TaxIdentifier Tax identifier of the company. BankAccount Bank account number of the company. Invoice ID Auto-generated primary key. CustomerName The name of the customer; printed on the invoice. CustomerZipCode The zip code of the address. CustomerCity The city part of the address. CustomerStreet The street and house number part of the address. PrintedCopies Number of copies printed of this invoice. Cancelled Has the invoice been cancelled? PaymentMethod Payment method of the invoice (text name). CreationDate Date when the invoice was created. DeliveryDate Delivery date of the invoice. PaymentDeadline Deadline of the payment. InvoiceIssuerID Foreign key to the InvoiceIssuer table; the issuer of the invoice. OrderID Foreign key to the Order table; the order out of which this invoice was created. InvoiceItem ID Auto-generated primary key. Name Name of the product; printed on the invoice. Amount The amount ordered of the specific product. Price The unit price of the product; by default this is the price of the product, but can be altered (e.g., for bulk order). VATPercentage The effective percentage of the tax applied. InvoiceID Foreign key to the Invoice table; the invoice this item is part of. OrderItemID Foreign key to the OrderItem table; the item out of which this invoice item was created."},{"location":"db/#peculiarities","title":"Peculiarities","text":""},{"location":"db/#invoicing","title":"Invoicing","text":"<p>Invoices cannot be altered once issued; they can only be canceled. Therefore all data that appears on the order are copied into the <code>Invoice</code> and <code>InvoiceItem</code> tables once the invoice is created. There is only a single original copy of the invoice; therefore, the number of printed copies is recorded.</p>"},{"location":"db/#invoice-issuer","title":"Invoice issuer","text":"<p>The issuer of the invoices changes very infrequently. However, in case it changes, existing invoices must remain unaltered. The issuer of the invoice is recorded in a separate table, and only one is in effect at all times. Each invoice references the right <code>InvoiceIssuerId</code> at the time.</p>"},{"location":"db/#vat","title":"VAT","text":"<p>The <code>VAT</code> percentage of products can change at any time. However, existing invoices must not be altered. Therefore the actual VAT percentage is stored with the invoice when created and not referenced from the VAT table.</p>"},{"location":"db/#product-description","title":"Product description","text":"<p>Some products contain an additional XML description, such as the following example.</p> <pre><code>&lt;product&gt;\n  &lt;product_size&gt;\n    &lt;unit&gt;cm&lt;/unit&gt;\n    &lt;width&gt;150&lt;/width&gt;\n    &lt;height&gt;50&lt;/height&gt;\n    &lt;depth&gt;150&lt;/depth&gt;\n  &lt;/product_size&gt;\n  &lt;package_parameters&gt;\n    &lt;number_of_packages&gt;1&lt;/number_of_packages&gt;\n    &lt;package_size&gt;\n      &lt;unit&gt;cm&lt;/unit&gt;\n      &lt;width&gt;150&lt;/width&gt;\n      &lt;height&gt;20&lt;/height&gt;\n      &lt;depth&gt;20&lt;/depth&gt;\n    &lt;/package_size&gt;\n  &lt;/package_parameters&gt;\n  &lt;description&gt;\n    Requires battery (not part of the package).\n  &lt;/description&gt;\n  &lt;recommended_age&gt;0-18 m&lt;/recommended_age&gt;\n&lt;/product&gt;\n</code></pre>"},{"location":"db/mongodb/","title":"Using MongoDB","text":""},{"location":"db/mongodb/#using-mongodb","title":"Using MongoDB","text":"<p>MongoDB is a free, open-source database server. We are using the community edition and MongoDB for VSCode as the client.</p> <p>Download links:</p> <ul> <li>https://www.mongodb.com/download-center/community</li> <li>https://marketplace.visualstudio.com/items?itemName=mongodb.mongodb-vscode</li> </ul> <p>Installation instructions: https://docs.mongodb.com/manual/administration/install-community/</p>"},{"location":"db/mongodb/#starting-mongodb-server","title":"Starting MongoDB server","text":"<p>Depending on the installation model, the server might automatically start. If we opted out of this option, we could start the server with the following command within the installation directory. (Note, that the server application is the mongo\u200bd executable.)</p> <pre><code>mongod.exe --dbpath=\"&lt;workdir&gt;\"\n</code></pre> <p>The database will be stored in directory workdir. When started with the command above, the server is alive until the console is closed. We can shut down the server using Ctrl+C.</p> <p>Mongo with Docker</p> <p>Alternatively, you can run the mongo server in a docker container using the following command:</p> <pre><code>docker run --name datadriven-mongo -p 27017:27017 -d mongo\n</code></pre> <p>When running this way, the <code>-p 27017:27017</code> switch maps the container's internal port 27017 to localhost's port 27017, so it can be used just like an installed version.</p>"},{"location":"db/mongodb/#mongo-shell","title":"Mongo shell","text":"<p>The Mongo shell is a simple console client. The official documentation uses this shell in the examples; however, we will not use this app.</p>"},{"location":"db/mongodb/#mongodb-for-vscode","title":"MongoDB for VSCode","text":"<p>MongoDB for VSCode is a simple free extension for VSCode for accessing MongoDB databases.</p> <p>The extension displays out previous connections, or we can create a new one. By default, the address is <code>localhost</code>, and the port is <code>27017</code>.</p> <p></p> <p></p> <p>After the connection is established, the databases and collections are displayed in a tree-view on the left. To begin with, we will not have any database or collections. (We can create them manually: right-click on the server and Create Database and run a custom script.)</p> <p></p> <p>A collection can be opened by right-click / View Documents. This will open a new tab with results. If we want to search, then with the right click / Search For Documents... operation we can write JavaScript code in a new playground window.</p> <p>A document can be edited and deleted by right clicking a document. Edit is performed by editing the raw JSON document.</p> <p></p> <p>A new document can be inserted by right-clicking and writing the JSON content. It is advised to copy an existing document and change it to ensure that key names are correct.</p>"},{"location":"db/mongodb/#studio-3t","title":"Studio 3T","text":"<p>Studio 3T is a commercial MongoDB client with richer GUI features. It has a free version but requires registration. It's not available in lab computers, but you can install it on your own computer.</p>"},{"location":"db/mssql/","title":"Using Microsoft SQL Server","text":""},{"location":"db/mssql/#using-microsoft-sql-server","title":"Using Microsoft SQL Server","text":"<p>Microsoft SQL Server is accessed using SQL Server Management Studio. We are using the so-called LocalDB version that hosts the server locally for development purposes, but you can also use the Express edition (any version).</p> <p>Download links:</p> <ul> <li>LocalDB is installed with Visual Studio</li> <li>https://www.microsoft.com/en-us/sql-server/sql-server-editions-express</li> <li>https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms</li> </ul> <p>Video guide of the tools</p> <p>How to use these tools: https://web.microsoftstream.com/video/98a6697d-daec-4a5f-82b6-8e96f06302e8</p>"},{"location":"db/mssql/#using-sql-server-management-studio","title":"Using SQL Server Management Studio","text":"<p>In university computer laboratories, you can start the software from the start menu. Connection details are configured when the application starts. When using LocalDB, the Server name is <code>(localdb)\\mssqllocaldb</code>; for Express Edition the name is <code>.\\sqlexpress</code> (if installed with default settings). Either case use Windows Authentication.</p> <p>When the connection is established, the databases are listed on the left in the Object Explorer window under Databases. A database can be expanded, and the tables (along with other schema elements) are listed here.</p> <p>SQL code can be executed in a new Query window opened using  on the toolbar. The commands in the Query window are executed on the currently selected database. This database is selected from a dropdown in the toolbar (see in the image below with yellow). We can open multiple Query windows.</p> <p>The SQL command is executed using the  button on the toolbar. Only the selection is executed if any text is selected; otherwise, the entire window content is sent to the server. The result and errors are printed under the script.</p> <p></p>"},{"location":"db/mssql/#creating-a-new-database","title":"Creating a new database","text":"<p>If we have no database, we must create one first. In Object Explorer right-click Databases to open a dialog. We need to specify a name and leave all other options as-is. After creating a new database, we shall not forget to select the toolbar's current database for any Query window we have open!</p> <p></p>"},{"location":"db/mssql/#concurrent-transactions","title":"Concurrent transactions","text":"<p>To simulate concurrent transactions, we need two Query windows; open two by pressing the New Query button twice. You can align these windows next to each other by right-clicking the Query tab title and selecting New Vertical Tab Group.</p> <p></p>"},{"location":"db/mssql/#listing-and-editing-table-content","title":"Listing and editing table content","text":"<p>To view any database table's content, open the database in Object Explorer, locate the table under Tables, then right-click and chose Select Top 1000 Rows or Edit Top 200 Rows.</p> <p></p>"},{"location":"db/mssql/#intellisense-reload","title":"Intellisense reload","text":"<p>Intellisense often does not work in SQL Management Studio query windows. Press Control+Shift+R-t to trigger a reload of Intellisense cache. We also need to use this after creating a new object (e.g., a new stored procedure).</p>"},{"location":"db/mssql/#creating-stored-procedures-and-triggers","title":"Creating stored procedures and triggers","text":"<p>We can create new stored procedures or triggers by writing the T-SQL code to create them in a Query window. Once an item with the same name exists, we cannot create it but have to modify the existing one using the proper command.</p> <p>Existing stored procedures are listed in Object Explorer under our database in the Programability/Stored Procedures folder. (Newly created items do not appear in the folder, but we have to refresh the folder content by right-clicking and choosing Refresh.)</p> <p></p> <p>Triggers are found in Object Explorer under the table on which they are defined in the Triggers folder (system-level triggers are in the Programability folder under the database itself).</p> <p></p> <p>The code of any existing stored procedure or trigger can be opened by locating them (see above), then right-clicking and choosing the Modify command. This will open a new Query window with an <code>alter</code> command and the current code of the program.</p>"},{"location":"homework/","title":"Homework","text":""},{"location":"homework/#homework","title":"Homework","text":"<p>With these exercises you can earn points that are added to your exam score. Maximum 4 points per homework. In the exercises and the evaluation results, you will see a text \u201ciMsc\u201d for optional excerciser; these iMsc points are not counted! (These are special points for Hungarian curriculum). All non-iMsc exercises are available for points on this course, 5 homeworks, maximum 4 points per homework, maximum 20 points. Here you find the exercise descriptions; the submission of the solutions is expected via GitHub Classroom. If you fail to submit the exercies exactly as in the guide, or it is late, you get no points at all! Make sure to follow the guide and do everything in time!</p> <p>Working code</p> <p>You are expected to write code that actually works! Your code will be executed, and it is required to fulfill the specified task.</p>"},{"location":"homework/#the-exercises","title":"The exercises","text":"<ol> <li>MSSQL server-side programming</li> <li>Entity Framework</li> <li>MongoDB</li> <li>REST API and Web API</li> <li>GraphQL</li> </ol>"},{"location":"homework/#submission","title":"Submission","text":"<p>Each homework must be submitted in a personal git repository. Please refer to the detailed guideline here. You must carefully study these guidelines!</p> <p>IMPORTANT</p> <p>The submissions of the homework must follow these guidelines. Submissions not adhering to the expected format are not considered.</p> <p>Workflow errors, i.e., not following the guidelines (e.g., not assigning the right person, or not assigning at all) are penalized.</p>"},{"location":"homework/#screenshots","title":"Screenshots","text":"<p>Some of the exercises require you to create a screenshot. This screenshot is proof of the completion of the exercise. The expected content of these screenshots is detailed in the exercise description. The screenshot may include the entire desktop or just the required portion of the screen.</p> <p>The screenshots must be submitted as part of the solution code, uploaded to the git repository. The repositories are private; only you and the instructions can access them. If there is any content on the screenshot that is not relevant to the exercise and you would like to remove, you can obscure these parts.</p>"},{"location":"homework/#required-tools","title":"Required tools","text":"<ul> <li>Windows, Linux, or macOS: All tools are platform-independent, or a platform-independent alternative is available.</li> <li>GitHub account and a git client.</li> <li>For homework using the MSSQL platform:<ul> <li>Microsoft SQL Server. The free Express version is sufficient, or you may also use localdb installed with Visual Studio. A Linux version is also available. On macOS, you can use Docker.</li> <li>SQL Server Management Studio, or you may also use the platform-independent Azure Data Studio is</li> <li>Database initialization script: mssql.sql</li> </ul> </li> <li>For homework using a MongoDB database:<ul> <li>MongoDB Community Server</li> <li>VSCode</li> <li>MongoDB for VSCode</li> <li>Sample database initialization script: mongo.js</li> </ul> </li> <li>For the REST API homework: Postman</li> <li>For writing C# code (most homework, except the first one):<ul> <li>Microsoft Visual Studio 2022 with the settings here<ul> <li>When using Linux or macOS, you can use Visual Studio Code, the .NET SDK, and dotnet CLI.</li> </ul> </li> <li> <p>.NET 8.0 SDK</p> <p>.NET 8.0</p> <p>Mind the version! You need .NET SDK version 8.0 to solve these exercises.</p> <p>On Windows, it might already be installed along with Visual Studio (see here how to check it); if not, use the link above to install (the SDK and not the runtime). You need to install it manually when using Linux or macOS.</p> </li> </ul> </li> </ul>"},{"location":"homework/#submission-evaluation","title":"Submission evaluation","text":"<p>The evaluation of the exercises is semi-automatic. Your code will be executed; therefore, it is vital to follow the exercise descriptions precisely (e.g., use the provided code skeleton, change only the allowed parts of the code, etc.).</p> <p>You will receive a preliminary result about your submission in GitHub; see in the guideline here). If there are some issues you need to diagnose, the entire log of the execution is available for you on the GitHub Actions web page. A short introduction is provided here.</p> <p>Verification</p> <p>In some of the exercises, where the technology permits, you will find unit tests. These tests help you verify your work, but these are no substitution for your validation. When you upload your work, more exhaustive testing will evaluate your submission.</p>"},{"location":"homework/GitHub-Actions/","title":"Using GitHub Actions","text":""},{"location":"homework/GitHub-Actions/#using-github-actions","title":"Using GitHub Actions","text":"<p>The semi-automatic evaluation of the exercises uses GitHub Actions. It is a CI system capable of running jobs on git repositories. We use this system, for example, to compile your code and test it.</p> <p>You will receive a notification about the results in a pull request. But if you need more details, such as check the application logs, you can access these using the web interface of GitHub under Actions.</p> <p></p> <p>Here, you will see a list of Workflows. Each evaluation (each commit) is a separate item here (so the history is also available).</p> <p></p> <p>By selecting one (e.g., the last one is always at the top of the list), you see this workflow's details. To get to the logs, you need to click once more on the left. The log will be on the right side.</p> <p></p> <p>Each green checkmark is a successful step. These steps do not correspond to the exercises; these describe the evaluation process. These steps include preparations, such as setting up the .NET environment for compiling your code (since each workflow starts in a clean environment, these steps are performed each time).</p> <p>Most of these steps should be successful, even if your submission contains an error. The two exceptions when these tasks might fail due to your changes are: (1) if <code>neptun.txt</code> is missing, or (2) your C# code does not compile. The <code>neptun.txt</code> is mandatory, and no evaluation is performed until that is provided. The C# compilation is a step that must succeed; otherwise, your application cannot be started.</p> <p>There might be transient errors in these workflows. An example is when a download, such as the download of the .NET environment fails. The workflow execution can be repeated if this occurs. Retrying the execution may only help if the problem is indeed transient; a retry will not resolve a C# compilation error. (You can deduce the cause from the name of the step and the error message.)</p> <p></p> <p>You might also be able to access the application logs. E.g., when testing a .NET application, it is started, and the logs will be printed here.</p> <p>The image below shows the initialization of an Entity Framework application, where you can also see the translated and executed SQL commands. (You would see the same in Visual Studio Output while debugging.) The content here, obviously, depends on the actual exercise.</p> <p></p>"},{"location":"homework/GitHub/","title":"Submitting your work (GitHub)","text":""},{"location":"homework/GitHub/#submitting-your-work-github","title":"Submitting your work (GitHub)","text":"<p>We are using GitHub to submit the solutions. Each homework is submitted in a GitHub repository. The repository is created through a link included in the exercise description. The solution of the exercises are created within these repositories, then committed and pushed to GitHub. The submission is finished with a pull request assigned to the instructor (with GitHub username <code>akosdudas</code>).</p> <p>IMPORTANT</p> <p>The submission requirements detailed below and mandatory. Submissions not following these guidelines are not graded.</p>"},{"location":"homework/GitHub/#short-version-aka-tldr","title":"Short version, aka. TL;DR","text":"<p>The detailed description below shows the entire procedure. This summary is an overview of the whole process.</p> <ol> <li> <p>The exercises are solved in a dedicated GitHub repository created using a GitHub Classroom invitation link published in Moodle.</p> </li> <li> <p>Your solution is submitted on a new branch, not on master. You can create any number of committed on this branch. You need to push these commits to GitHub.</p> </li> <li> <p>You submit your final solution through a pull request assigned to the instructor.</p> </li> <li> <p>You can ask questions regarding the results and evaluation in the pull request comment thread. To notify your instructor use the @name annotation in the comment text.</p> </li> </ol>"},{"location":"homework/GitHub/#starting-your-work-git-checkout","title":"Starting your work: git checkout","text":"<ol> <li> <p>Register a GitHub account if you don't have one yet.</p> </li> <li> <p>Open the course page in Moodle and find the invitation URL. This link is different for each homework; make sure to use the right one.</p> </li> <li> <p>If needed, authorize the GitHub Classroom application to use your account data.</p> <p></p> </li> <li> <p>You will see a page where you can \"Accept the ... assignment\". Click the button.</p> <p></p> </li> <li> <p>Wait for the repository creation to finish. You will get the repository URL here.</p> <p>Note</p> <p>The repository will be private. No one but you and the instructor will see it.</p> <p></p> </li> <li> <p>Open the repository webpage by following the link. You will need this URL, so remember it.</p> <p></p> </li> <li> <p>Clone your repository. You will need the repository git URL, which you can get from the repository webpage following the Clone or download button.</p> <p>You may use any git client. The simplest one is GitHub Desktop if you do not have a favorite yet. You can list your repositories in this application directly from GitHub.</p> <p></p> <p>If you are using the shell or the console, the following command performs the clone (if the <code>git</code> command is available): <code>git clone &lt;repository link&gt;</code></p> </li> <li> <p>If the cloning is successful, DO NOT START WORKING YET! The solution should not be committed to the repository <code>master</code> branch. Instead, create a new branch with the name <code>solution</code>.</p> <p>In GitHub Desktop, use the Branch menu for creating a new one.</p> <p></p> <p>If using the console, use the following command: <code>git checkout -b solution</code></p> </li> <li> <p>Complete the exercises on this branch. You may have any number of commits and pushes.</p> <p>Check the name used for committing</p> <p>Before you make your first commit, check whether your name and email address are properly configured. You can check this using the following commands.</p> <pre><code>git config user.name\ngit config user.email\n</code></pre> <p>If the values are not correct, set your name and email address with the following commands executed in the repository directory. This will set the values for the repository. (It is recommended to set the email address to the one you use with GitHub.)</p> <pre><code>git config user.name \"John Doe\"\ngit config user.email \"john@doe.org\"\n</code></pre> <p>To avoid having to set this for all repositories, you may want to set the name and email address globally using the <code>--global</code> switch in the commands above.</p> <p>To commit using GitHub Desktop, first check if you are on the right branch. During the first push, the solution branch needs to be published.</p> <p></p> <p>When adding further commits, verify the branch. You can publish the commit using the Push origin button. The little number on this button shows you how many commits need pushing.</p> <p></p> <p>If you are using the console, use the following commands:</p> <pre><code># Check the current branch and the files modified\ngit status\n\n# Prepares all changes for commit\ngit add .\n\n# Commit\ngit commit -m \"f1\"\n\n# Push the new branch (first time)\ngit push --set-upstream origin solution\n\n# Push futher commits\ngit push\n</code></pre> </li> </ol>"},{"location":"homework/GitHub/#submitting-the-solution","title":"Submitting the solution","text":"<ol> <li> <p>When you are ready with the exercises, verify on the repository web page that you uploaded everything. You may need to switch branches.</p> <p></p> <p>GitHub web file upload</p> <p>We recommend that you do not use GitHub web file upload. If something is missing, add it to your local repository and commit and push again.</p> </li> <li> <p>When you are truly ready, open a pull request.</p> <p>Why the pull request?</p> <p>This pull request combines all changes you made and shows us the final result. This helps the instructor to evaluate your submission more easily by seeing all changes at once. This pull request means you submit your solution; hence this step cannot be omitted.</p> <p>To open the pull request, you need to go to the repository's GitHub web frontend. If you pushed recently, GitHub will offer you to create the pull request.</p> <p></p> <p>You may also open the pull request from the menu at the top. It is important to specify the correct branches: <code>master</code> is the target into which <code>solution</code> is merged.</p> <p></p> <p>When the pull request is created, you will see a little number \"1\" on the Pull request menu showing you that there is one open item there. YOU ARE NOT FINISHED YET!</p> <p></p> </li> <li> <p>The pull request will trigger a preliminary evaluation. You will see the result of this evaluation as a comment added to the pull request thread.</p> <p>This will be different for each homework. Your code will be executed and tested, and you will receive a preliminary result too.</p> <p></p> <p>If you need more information about the evaluation and the results, GitHub Actions can provide you more. A short introduction is provided here.</p> </li> <li> <p>If you are not satisfied with your work, you can make further changes. You only need to commit and push your changes. Any changes pushed will re-trigger the evaluation of the pull request. We ask that you trigger NO MORE THAN 5 evaluations!</p> <p>Making further changes without running the evaluation</p> <p>If you want to make changes to your submission and not have the re-evaluation run, you should convert the pull request to draft.</p> <p></p> <p>This state means work in progress. You can commit and push freely. These will not trigger any evaluation. Once ready, you must change the state back: go to the bottom of the PR and click \"Ready for review.\" This will set the PR back to its normal state and trigger an automated evaluation.</p> <p></p> <p>Maximum 5</p> <p>Evaluations that fail due to transient errors, such as network problems, are not counted into the 5 evaluations. But if you trigger more evaluation by mistake, or on purpose, it will be sanctioned. You are required to test your solution locally before submitting it.</p> </li> <li> <p>FINALLY, when you are ready, assign the pull request to the instructor. This step is considered as the submission of your work.</p> <p></p> <p>Without pull request</p> <p>If you have no pull request, or it is not assigned to the instructor, we consider it work in progress and not submitted.</p> <p>Done</p> <p>Now you are ready. After assigning the pull request, make no further changes. The instructor will evaluate the submission and close the pull request.</p> </li> </ol>"},{"location":"homework/GitHub/#questions-and-complaints-regarding-the-final-result","title":"Questions and complaints regarding the final result","text":"<p>If you have questions on concerns regarding the automated evaluation, use the pull request for communication with the instructor by asking questions via comments. To let the instructor know you have questions, please use <code>@akosdudas</code> mention in the PR comment. This will automatically send an email notification.</p> <p></p> <p>Please provide proof</p> <p>Please note that if you think the evaluation made a mistake, you must support your question/complaint with proof (e.g., show how you tested your solution and prove that it worked).</p>"},{"location":"homework/VisualStudio/","title":"Install Visual Studio & .NET SDK","text":""},{"location":"homework/VisualStudio/#install-visual-studio-net-sdk","title":"Install Visual Studio &amp; .NET SDK","text":"<p>In some of the exercises require Microsoft Visual Studio version 2022. The free Community edition is sufficient for solving these exercises.</p> <p>VS Code</p> <p>The exercises can also be solved using the platform-independent Visual Studio Code. The skeletons of the exercises are prepared for Visual Studio. If you are working with VS Code, you need to configure your environment.</p>"},{"location":"homework/VisualStudio/#visual-studio-workloads","title":"Visual Studio workloads","text":"<p>When installing Visual Studio, the ASP.NET and web development workload have to be selected.</p> <p></p> <p>An existing installation can be modified using the Visual Studio Installer.</p>"},{"location":"homework/VisualStudio/#check-and-install-net-sdk","title":"Check and install .NET SDK","text":"<p>Visual Studio might install certain versions of the .NET SDK. To check if you have the right version, use the <code>dotnet</code> CLI: in a console, execute the <code>dotnet --list-sdks</code> command. This command works on Linux and Mac too. It will print something similar:</p> <pre><code>C:\\&gt;dotnet --list-sdks\n6.0.300 [C:\\Program Files\\dotnet\\sdk]\n</code></pre> <p>If you see version 6.0 in this list, then you are good to go. Otherwise, install the SDK from here.</p>"},{"location":"homework/adonet/","title":"Exercise: ADO.NET data access","text":""},{"location":"homework/adonet/#exercise-adonet-data-access","title":"Exercise: ADO.NET data access","text":"<p>IMPORTANT</p> <p>Revoked from 2024!</p> <p>You may earn 2 points by completing this exercise.</p> <p>Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle. Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. You need to create a branch, named <code>solution</code>, and work on this branch. After completing the exercises and verifying them, commit and push your submission.</p> <p>Check the required software and tools here. This homework uses MSSQL database.</p>"},{"location":"homework/adonet/#exercise-0-neptun-code","title":"Exercise 0: Neptun code","text":"<p>Your very first task is to type your Neptun code into <code>neptun.txt</code> in the root of the repository.</p>"},{"location":"homework/adonet/#exercise-1-product-repository-2-points","title":"Exercise 1: Product repository (2 points)","text":"<p>Create repository class for managing the <code>Product</code> entities; use ADO.NET Connection technology. Open the sln file from the checked-out folder using Visual Studio. Find classes <code>Repository.ProductRepository</code> and <code>Model.Product</code>. Implement the following methods of class <code>ProductRepository</code>:</p> <ul> <li><code>Search(string name)</code>: find all products in the database matching the provided name, and return them as C# objects. If the name filter argument is <code>null</code>, the method should return all products; otherwise, it should match names that contain the specified string in a case-insensitive manner!</li> <li><code>FindById(int id)</code>: returns a single product matched by the ID, or returns <code>null</code> if not found.</li> <li><code>Update(Product p)</code> updates the properties of a product in the database based on the values received as parameter. Update the <code>Name</code>, <code>Price</code>, and <code>Stock</code> values, and you may ignore the rest.</li> <li><code>Delete(int id)</code> should delete the product specified by the ID from the database - if such product exists. The method shall return whether the delete was successful. (You need to delete the product record only; do not remove other referenced records. In case deletion is blocked due to foreign key constraints, do not catch the error; let the caller see the error.)</li> </ul> <p>You should mind the following requirements:</p> <ul> <li>Only make changes to class <code>ProductRepository</code>!</li> <li>In the repository code open the ADO.NET connection using the connection string in field <code>connectionString</code> (and do not use <code>TestConnectionStringHelper</code> here).</li> <li>You need to find the tax percentage of the product too. In the returned instance of <code>Model.Product</code> you must include the percentage of the referenced <code>VAT</code> record and not the ID of this VAT record! The name of the category of the product has to be retrieved similarly.</li> <li>You may only use ADO.NET.</li> <li>You must prohibit SQL injection.</li> <li>Make no changes to <code>Model.Product</code> in this exercise!</li> <li>Do not change the definition of class <code>ProductRepository</code> (do not change the class's name, nor the constructor or method declarations); only write the method bodies.</li> </ul> <p>There are unit tests available in the solution. You can run the unit tests in Visual Studio, or if you are using another IDE (e.g., VS Code and <code>dotnet cli</code>), then run the tests using the cli. You may update the database connection string in class <code>TestConnectionStringHelper</code> if needed.</p> <p>Tests</p> <p>The tests presume that the database is in its initial state. Re-run the database initialization script to restore this state.</p> <p>Do NOT change the unit tests. You may temporarily alter the unit tests if you need to, but make sure to reset your changes before committing.</p> <p>SUBMISSION</p> <p>Upload the changed C# source code.</p> <p>Create a screenshot displaying the successfully executed unit tests. You can run the tests in Visual Studio or using <code>dotnet cli</code>. Make sure that the screenshot includes the source code of the repository (as much as you can fit on the screenshot), and the test execution outcome! Save the screenshot as <code>f1.png</code> and upload it as part of your submission!</p> <p>If you are using <code>dotnet cli</code> to run the tests, make sure to display the test names too. Use the <code>-v n</code> command line switch to set detailed logging.</p> <p>The image does not need to show the exact same source code that you submit; there can be some minor changes. If the tests run successfully and you create the screenshot, then later you make some minor change to the source, there is no need for you to update the screenshot.</p> <p>YOU HAVE NOT FINISHED YET</p> <p>After pushing your code to the <code>solution</code> branch, create a PR and assign your instructor (github username: <code>akosdudas</code>) to it! (more details: on the assignment submission page)</p>"},{"location":"homework/adonet/#exercise-2-optional-optimistic-concurrency-handling-0-points","title":"Exercise 2 optional: Optimistic concurrency handling (0 points)","text":"<p>In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that.</p> <p>When updating a product in the database, the code shall identify and prohibit overwriting a previously unseen modification. Implement this behavior in <code>ProductRepository.UpdateWithConcurrencyCheck</code>. This method shall deny the update if it discovers a lost update concurrency issue.</p> <p>The specific sequence of events that we want to prohibit:</p> <ol> <li>User A queries a product.</li> <li>User B fetches the same product.</li> <li>User A changes a property, such as the price, then updates the database with this change.</li> <li>User B makes a change to the product properties (either the price property, or another one), and overwrites the changes made by user A without noticing it.</li> </ol> <p>Optimistic concurrency handling</p> <p>Use the technique of optimistic concurrency handling to resolve this issue. You must not use transactions here, since the query and the data update happens over a longer period without maintaining a database connection. Do not use multiple SQL statements either, as in-between the execution of multiple statements the database content can change resulting in your application not working with the latest data. Implement the method <code>ProductRepository.UpdateWithConcurrencyCheck</code>, and also update <code>Model.Product</code> as needed. You may not add any new columns to the database.</p> <p>You should mind the following requirements:</p> <ul> <li>Only make changes to the method <code>ProductRepository.UpdateWithConcurrencyCheck</code> and class <code>Model.Product</code>!</li> <li>The method shall indicate as return value whether the change was saved (that it, it discovered no concurrency issues).</li> <li>Explain the behavior in a C# comment in method UpdateWithConcurrencyCheck (in 2-3 sentences).</li> <li>Solve the exercise with using a single SQL command!</li> <li>You may only use ADO.NET.</li> <li>You must prohibit SQL injection.</li> <li>Do not change the definition of class <code>ProductRepository</code> (do not change the name of the class, nor the constructor or method declarations); only write the single method body.</li> <li>Do not change the constructor signature of the class <code>Model.Product</code> (number, order, or names of the parameters), but you may change the body. Do not alter any existing properties of the class, but you can add new ones.</li> </ul> <p>SUBMISSION</p> <p>Upload the changed C# source code. Do not forget the explanation comment!</p>"},{"location":"homework/ef/","title":"2. Entity Framework","text":""},{"location":"homework/ef/#2-entity-framework","title":"2. Entity Framework","text":"<p>You may earn 4 points by completing this exercise.</p> <p>Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle. Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. You need to create a branch, named <code>solution</code>, and work on this branch. After completing the exercises and verifying them, commit and push your submission.</p> <p>Check the required software and tools here. This homework uses MSSQL database.</p> <p>Entity Framework Core</p> <p>We are using Entity Framework Core in this exercise. This is different from Entity Framework used in the seminar exercises; this is a platform-independent technology.</p>"},{"location":"homework/ef/#exercise-0-neptun-code","title":"Exercise 0: Neptun code","text":"<p>Your very first task is to type your Neptun code into <code>neptun.txt</code> in the root of the repository.</p>"},{"location":"homework/ef/#exercise-1-database-mapping-using-code-first-model-and-queries-2-points","title":"Exercise 1: Database mapping using Code First model and queries (2 points)","text":"<p>Prepare the (partial) mapping of the database using Entity Framework Code First modeling. The Entity Framework Core package is part of the project, so you can start coding. The central class for database access is the DbContext. This class already exists with the name <code>ProductDBContext</code>.</p> <ol> <li> <p>Map the product entity. Create a new class with the name <code>DbProduct</code> with the following code. (The Db prefix indicates that this class is within the scope of the database. This will be relevant in the next exercise.) We rely on conventions as much as possible: use property names that match the column names to make mapping automatic.</p> <pre><code>using System.ComponentModel.DataAnnotations.Schema;\n\nnamespace ef\n{\n    [Table(\"Product\")]\n    public class DbProduct\n    {\n        [DatabaseGenerated(DatabaseGeneratedOption.Identity)]\n        public int ID { get; set; }\n        public string Name { get; set; }\n        public double Price { get; set; }\n        public int Stock { get; set; }\n    }\n}\n</code></pre> <p>Open the source code of class <code>ProductDbContext</code> and uncomment the <code>Products</code> property.</p> </li> <li> <p>Create a new class with the name <code>DbVat</code> in namespace <code>ef</code> for mapping the <code>VAT</code> database table similarly as seen before. Do not forget to add a new DbSet property into <code>ProductContext</code> with the name <code>Vat</code>.</p> </li> <li> <p>Map the Product - VAT connection.</p> <p>Add a new get-set property into class <code>DbProduct</code> with name <code>Vat</code> and type <code>DbVat</code>. Use the <code>ForeignKey</code> attribute on this property, to indicate the foreign key used to store the relationship (\"VatID\").</p> <p>Create the \u201cother side\u201d of this one-to-many connection from class <code>DbVat</code> to <code>DbProduct</code>. This should be a new property of type <code>System.Collections.Generic.List</code> with name <code>Products</code>. (See an example in the link above.)</p> </li> </ol> <p>There are unit tests available in the solution. The test codes are commented out because it does not compile until you write the code. Select the whole test code and use Edit / Advanced / Uncomment Selection. You can run the unit tests in Visual Studio, or if you are using another IDE (e.g., VS Code, or <code>dotnet cli</code>), then run the tests using the cli. You may update the database connection string in class <code>TestConnectionStringHelper</code> if needed.</p> <p>Tests</p> <p>The tests presume that the database is in its initial state. Re-run the database initialization script to restore this state.</p> <p>Do NOT change the unit tests. You may temporarily alter the unit tests if you need to, but make sure to reset your changes before committing.</p> <p>If the tests do not compile</p> <p>If the test code does not compile, you may have used slightly different property names. Fix these in your code and not in the tests!</p> <p><code>OnConfiguring</code></p> <p>You need no connection string in the DbContext. The constructor handles the connection to the database. Do not create <code>OnConfiguring</code> method in this class!</p> <p>SUBMISSION</p> <p>Upload the changed C# source code.</p> <p>Create a screenshot displaying the successfully executed unit tests. You can run the tests in Visual Studio or using <code>dotnet cli</code>. Make sure that the screenshot includes the source code of the DbContext and the test execution outcome! Save the screenshot as <code>f1.png</code> and upload as part of your submission!</p> <p>If you are using <code>dotnet cli</code> to run the tests, make sure to display the test names too. Use the <code>-v n</code> command line switch to set detailed logging.</p> <p>The image does not need to show the exact same source code that you submit; there can be some minor changes here and there. That is, if the tests run successfully and you create the screenshot, then later you make some minor change to the source, there is no need for you to update the screenshot.</p>"},{"location":"homework/ef/#exercise-2-repository-implementation-using-entity-framework-2-points","title":"Exercise 2: Repository implementation using Entity Framework (2 points)","text":"<p>This exercise can be solved after completing the first exercise.</p> <p>The Entity Framework DbContext created above has some drawbacks. For example, we need to trigger loading related entities using <code>Include</code> in every query, and the mapped entities are bound to precisely match the database schema. In complex applications, the DbContext is frequently wrapped in a repository that handles all peculiarities of the data access layer.</p> <p>Implement class <code>ProductRepository</code> that helps with listing and inserting products. You are provided with a so-called model class representing the product entity, only in a more user-friendly way: it contains the tax percentage value directly. An instance of this class is built from database entities, but represents all information in one instance instead of having to handle a product and a VAT record separately. Class <code>Model.Product</code> contains most properties of class <code>DbProduct</code>, but instead of the navigation property to <code>DbVat</code> it contains the referenced percentage value (<code>VAT.Percentage</code>) directly.</p> <p>Implement the methods of class `ProductRepository.</p> <ul> <li><code>List</code> shall return all products mapped to instances of <code>Model.Product</code>.</li> <li><code>Insert</code> shall insert a new product into the database. This method shall find the matching <code>VAT</code> record in the database based on the tax percentage value in the model class; if there is no match, it shall insert a new VAT record too! The method shall return the ID of the newly inserted ID (as generated by the database).</li> <li><code>Delete</code> should delete a product record matched by the ID. You should only delete the product record - no referenced records shall be removed. If delete is blocked by foreign keys, let the caller handle the error. The return value of the method should be false if no record with the specified ID exist; a true return value shall indicate successful deletion.</li> <li>Do not change the definition of class <code>ProductRepository</code> (do not change the name of the class, nor the constructor or method declarations); only write the method bodies.</li> <li>In the repository code use <code>ProductRepository.createDbContext()</code> to instantiate the DbContext  (do not use <code>TestConnectionStringHelper</code> here).</li> </ul> <p>SUBMISSION</p> <p>Upload the changed C# source code.</p> <p>YOU HAVE NOT FINISHED YET</p> <p>After pushing your code to the <code>solution</code> branch, create a PR and assign your instructor (github username: <code>akosdudas</code>) to it! (more details: on the assignment submission page)</p>"},{"location":"homework/ef/#exercise-3-optional-logical-deletion-with-entity-framework-0-points","title":"Exercise 3 optional: Logical Deletion with Entity Framework (0 points)","text":"<p>In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that.</p> <p>Deleting data from a database is an operation that can have numerous unintended consequences. Restoring deleted data is much more difficult, and sometimes it is not even possible without repercussions. Deleting data can result in the loss of the entire data history, making it impossible to know the state before deletion or to use it in various statistics. Moreover, there are cases where relationships with other tables and foreign key constraints exist, and deletion affects those tables as well.</p> <p>To overcome these problems, the most common solution is to implement a non-permanent deletion, known as a soft delete. In this case, a field (typically named <code>IsDeleted</code>) is used to indicate that the data has been deleted. Thus, the data remains in the database, but we can filter it to see whether it has been deleted.</p> <p>A naive implementation of filtering is not convenient. Imagine having to add a condition to every query or save operation to ensure that deleted items are not affected. To address this, it is advisable to use one of Entity Framework's features, the Global Query Filter. This allows us to define filter conditions that are automatically applied to every query globally by Entity Framework.</p> <p>Implement soft deletion for the previously created <code>DbProduct</code> class (there are multiple solutions; feel free to choose any of them):</p> <p>Modifiability</p> <p>Although the previous task had a restriction against overriding the <code>OnConfiguring</code> method, you are free to do so here if necessary (and you can also override other functions in the <code>DBContext</code> implementation)!</p> <ol> <li> <p>Add an <code>IsDeleted</code> variable that indicates to our application whether the entity is in a deleted state!</p> </li> <li> <p>Add a QueryFilter that filters out the products that have already been deleted in every query, so they are not returned!</p> </li> <li> <p>Modify the deletion behavior in the database generally by extending the <code>DbContext</code> save operations (EFCore provides several extension points for this) so that instead of performing a true deletion, it only changes the <code>IsDeleted</code> variable! Do not change the deletion operation in the repository for modification!</p> </li> </ol> <p>SUBMISSION</p> <p>Upload the modified C# source code.</p>"},{"location":"homework/graphql/","title":"5. GraphQL","text":""},{"location":"homework/graphql/#5-graphql","title":"5. GraphQL","text":"<p>You may earn 4 points by completing this exercise.</p> <p>Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle. Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. You need to create a branch, named <code>solution</code>, and work on this branch. After completing the exercises and verifying them, commit and push your submission.</p> <p>Check the required software and tools here.</p> <p>As preparation, create a new database as described in the practical material.</p>"},{"location":"homework/graphql/#exercise-0-neptun-code","title":"Exercise 0: Neptun code","text":"<p>Your very first task is to type your Neptun code into <code>neptun.txt</code> in the root of the repository.</p>"},{"location":"homework/graphql/#exercise-1-completing-the-project-and-queries-2-points","title":"Exercise 1: Completing the Project and Queries (2 points)","text":"<p>The goal of this homework is to gain practical experience with GraphQL by creating your own API. GraphQL is a powerful tool that enables querying and modifying data in a single, well-structured request, making API usage more flexible and efficient. The operation of a GraphQL API differs somewhat from traditional REST APIs: at the endpoint, clients can specify exactly what data they want to receive using flexible, controlled queries. The purpose of this exercise is to demonstrate how complex data can be accessed in a simple manner.</p> <p>In this homework, the familiar data model will be used, with queries involving orders, products, categories, and related information. The data model, DBContext, and entities are already available in the starter project.  </p> <p>In the first task, we will explore how to add GraphQL endpoints to an existing project using the Hot Chocolate server-side library. To do this, we will first add the necessary packages, expose endpoints, and create the class that will return the required data.</p> <ol> <li> <p>Create a <code>Query</code> class and add a <code>GetProducts</code> function with an <code>AdatvezDbContext</code> parameter. This function will serve as the <code>products</code> query's endpoint. The DBContext parameter will be injected by the DI container when the method is called. Implement the method to return the products with their categories.</p> </li> <li> <p>Add the following NuGet packages with the versions specified in parentheses:</p> </li> <li> <p>HotChocolate.AspNetCore (14.1.0)</p> </li> <li>HotChocolate.Data (14.1.0)</li> <li> <p>HotChocolate.Data.EntityFramework (14.1.0)</p> </li> <li> <p>In the entry point of the application (<code>Program.cs</code>), register the GraphQLServer service using the following code: (here, three things happen: the GraphQLServer service is registered, the <code>AdatvezDbContext</code> is registered for injection, and the <code>Query</code> class is registered to serve as the query endpoints.)</p> </li> </ol> <pre><code>builder.Services\n    .AddGraphQLServer()\n    .RegisterDbContextFactory&lt;AdatvezDbContext&gt;()\n    .AddQueryType&lt;Query&gt;();\n</code></pre> <ol> <li>Add routing (the ability to handle endpoints) and create a default endpoint using the following lines of code:</li> </ol> <pre><code>app.UseRouting();\napp.UseEndpoints(endpoints =&gt;\n{\n    endpoints.MapGraphQL();\n});\n</code></pre> <ol> <li>Start the application and navigate to <code>http://localhost:5000/graphql/</code>. Here, you will see an interactive tool called Banana Cake Pop, which allows you to run queries and mutations, view the schema, and browse the documentation for the API.  In the queries tab, you can test the execution of the above function with the following code:</li> </ol> <pre><code>query {\n    products {\n        name\n        category {\n            name\n        }\n    }\n}\n</code></pre> <p>Let's examine what happens:</p> <ul> <li> <p>When Hot Chocolate or another GraphQL server receives a request, each field is handled by a resolver, which is responsible for fetching or generating the corresponding data.</p> </li> <li> <p>The <code>products</code> field is implicitly linked to the <code>GetProducts</code> method, and Hot Chocolate automatically connects them.</p> </li> <li> <p>The integration of Hot Chocolate with Entity Framework (EF) allows the GraphQL API to directly query data from the database via EF, simplifying the implementation and operation of GraphQL endpoints.</p> </li> </ul> <ol> <li>Based on the previous task, create a query that can return products after applying a filter. In the <code>Query</code> class, add a <code>ProductsByCategory</code> function, which has a <code>categoryName</code> parameter of type <code>string</code> in addition to the <code>AdatvezDbContext</code> parameter. In the implementation, filter by the category name: return those products whose category name matches the provided parameter. Use the following query to test it:</li> </ol> <pre><code>query {\n    productsByCategory(categoryName: \"Months 0-6\") {\n        id\n        name\n        price\n    }\n}\n</code></pre> <ol> <li> <p>In the next task, we will see the power of GraphQL. When information needs to be gathered from multiple tables, a lot of unnecessary data could otherwise pass through the network. In contrast, if we know exactly what is needed, we can query and send only the required data. Create a query that returns orders and gathers the following information:</p> </li> <li> <p>The name of the customer  </p> </li> <li> <p>The names of the products in the order  </p> </li> <li> <p>The categories of the products  </p> </li> <li> <p>The quantity ordered for each product  </p> </li> </ol> <p></p><pre><code>query {\n    orders {\n        customerSite {\n            customer {\n                name\n            }\n        }\n        orderItems {\n            amount\n            product\n            {\n                name\n                category\n                {\n                    name\n                }\n            }\n        }\n    }\n}\n</code></pre> 1. Query Optimization: In the console that opens when the application starts, you can also view the query generated by Hot Chocolate using Entity Framework. You will see that the <code>SELECT</code> statement retrieves many fields that are not necessary, even though we explicitly specified the required fields in the GraphQL query.    For functions annotated with <code>[UseProjection]</code>, Hot Chocolate transforms the incoming queries directly for the database. To use this feature, two changes are required in addition to adding the annotation. First, the return type of the function should be <code>IQueryable&lt;Order&gt;</code>. The second change is to register the availability of projection with the following call:<p></p> <pre><code>builder.Services\n    .AddGraphQLServer()\n    .RegisterDbContextFactory&lt;AdatvezDbContext&gt;()\n    .AddQueryType&lt;Query&gt;()\n    //\u00faj sor:\n    .AddProjections(); \n</code></pre> <p>Look at the change in the SQL query in the console. The implementation can also be reduced, as explicit <code>Include</code> calls will no longer be necessary from this point on.</p> <p>SUBMISSION</p> <p>Upload the modified C# source code.</p> <p>Additionally, take a screenshot of the query and response created in the Banana Cake Pop interface, showing the required data. Save the screenshot as <code>f1.png</code> and include it as part of your solution!</p>"},{"location":"homework/graphql/#exercise-2-data-manipulation-using-grapql-2-points","title":"Exercise 2: Data manipulation using GrapQL (2 points)","text":"<p>GraphQL is not only used for querying data but also for modifications. In the following tasks, the goal is to practice data manipulation. Data manipulation is performed by a Mutation, which is an operation that modifies data, such as creating, updating, or deleting records in the database.</p>"},{"location":"homework/graphql/#modifying-a-product","title":"Modifying a Product","text":"<p>In the first data manipulation task, you will change the prices of existing products. Increase the price for products that belong to the category provided in the parameters.</p> <ol> <li> <p>Create a <code>Mutation</code> class named <code>ProductMutation</code>.</p> </li> <li> <p>Register the service using the <code>.AddMutationType&lt;ProductMutation&gt;()</code> method, similar to how the <code>Query</code> service was registered in the first exercise.</p> </li> <li> <p>The <code>ProductMutation</code> class should have a function <code>IncreaseProductPricesByCategory</code>, which will make the necessary modifications and return an <code>IQueryable&lt;Product&gt;</code>. This will show the new prices. The function should have three parameters: 1) the <code>AdatvezDBContext</code>, 2) a <code>categoryName</code> string, and 3) a <code>priceIncrease</code> double value, which indicates how much the price of the product should increase. The return value should be the collection of modified products. Ensure that the changes are also applied to the database.</p> </li> <li> <p>You can test it with the following example query (the parameter names should match those specified in the example call below: categoryName/priceIncrease):</p> </li> </ol> <pre><code>mutation {\n    increaseProductPricesByCategory(categoryName: \"LEGO\", priceIncrease: 1.1) {\n        name\n        price\n        category {\n                name\n            }\n        }\n}\n</code></pre> <pre><code>!!! note \"\"\n    The query consists of two parts. The first is the mutation call with the appropriate parameters:\n\n    ```json\n    increaseProductPricesByCategory(categoryName: \"LEGO\", priceIncrease: 1.1)\n    ```\n\n    The second part specifies the format in which we want to see the data after the call. This follows the syntax seen in the previous queries. In this case, for example, we want to display the modified product names, prices, and categories:\n\n    ```json\n    {\n        name\n        price\n        category {\n            name\n        }\n    }\n    ```\n</code></pre>"},{"location":"homework/graphql/#creating-an-order","title":"Creating an Order","text":"<p>In the next task, we will add the ability for the server to insert a new order. The mutation will only expect the product names and their desired quantities as parameters and will create the order accordingly.</p> <ol> <li> <p>Add a function to the <code>ProductMutation</code> class named <code>CreateOrder</code>. In addition to the usual <code>AdatvezDbContext</code>, it should have two parameters: the first one is a string list for product names named <code>productNames</code>, and the second is an integer list for quantities named <code>quantities</code>.</p> </li> <li> <p>Inside the function, create a new <code>Order</code> instance. For each product name in the provided parameters, create a new <code>OrderItem</code>. For each <code>OrderItem</code>, find the corresponding <code>Product</code> by its name and set the appropriate quantity (you can ignore other properties). The function should return the newly created <code>Order</code> instance.</p> </li> <li> <p>The function should throw an error if the number of products does not match the number of quantities, or if any of the product names cannot be found in the database.</p> </li> <li> <p>You can test it with the following query. The query will insert an order for two products, and then print the created order in the following structure:</p> </li> </ol> <pre><code>mutation {\n    createOrder(\n        productNames: [\"Lego City harbour\", \"Activity playgim\"],\n        quantities: [2, 4]\n    ) {\n        id\n        orderItems {\n        product {\n            name\n        }\n        amount\n        }\n    }\n}\n</code></pre> <p>Example \"SUBMISSION\"</p> <pre><code>Upload the modified C# source code.\n\nAdditionally, take a screenshot where you have executed the modifications using the Banana Cake Pop interface. Save the image as `f2.png` and include it as part of your solution!\n</code></pre> <p>YOU HAVE NOT FINISHED YET</p> <p>After pushing your code to the <code>solution</code> branch, create a PR and assign your instructor (github username: <code>akosdudas</code>) to it! (more details: on the assignment submission page)</p>"},{"location":"homework/graphql/#exercise-3-optional-advanced-graphql-functions-0-points","title":"Exercise 3 optional: Advanced GraphQL functions (0 points)","text":"<p>In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that.</p> <p>In the following tasks, we will utilize more advanced features provided by the Hot Chocolate GraphQL server, such as filtering, sorting, and pagination.</p> <p>To use Hot Chocolate's built-in filtering capabilities, we need to call <code>AddFiltering()</code> when registering the <code>Service</code>, and then apply the <code>[UseFiltering]</code> attribute to the method that serves the desired endpoint.</p> <p>In the following tasks, you will need to come up with the query and the corresponding .NET implementation. These should be submitted as part of the solution.</p> <ol> <li> <p>In the first task, the <code>productsByCategory</code> call can be replaced by enabling the built-in filtering for our <code>GetProducts</code> function.     Do this, and test it with a query that filters by category names.    The code completion will be very helpful in creating the query.    When submitting the solution, the query should be submitted in the <code>q3_1.txt</code> file, where you need to filter by the <code>\"Building Items\"</code> category.    The results should be in the same format as in Exercise 1.8.</p> <p>Filtering has a unique syntax. In the query, after the desired element, we specify the filtering conditions using a <code>where</code> expression. Here, according to the schema, we need to specify which values we want to filter by, whether we want to filter using combined conditions, etc.</p> <p></p><pre><code>query {\n    products (where .. ) {\n        name\n        category {\n            name\n        }\n    }\n}\n</code></pre> The great advantage of this approach is that we do not need to implement all possible filtering scenarios that could arise among API users. Instead, it is left to the users to specify how they want to filter, and the GraphQL language supports this. Hot Chocolate will generate the response filtered based on the format and conditions requested by the client.<p></p> <p>Furthermore, it is possible to create custom filtering conditions, which you can read more about here: https://chillicream.com/docs/hotchocolate/v14/fetching-data/filtering</p> </li> <li> <p>Your second task is to add sorting and pagination to the <code>GetOrders</code> function. Annotate the <code>GetOrders</code> function with the appropriate attributes, then set up the sorting and pagination options when registering GraphQL. To test it, you will need to modify the query, and constructing the query is also part of the task. When submitting the solution, you must also submit the query in the <code>q3_2.txt</code> file, where the query contains 2 <code>Order</code> items (due to pagination) and sorts them in descending order by <code>id</code>. The response to the query should arrive in the format below!</p> </li> </ol> <p>Pagination is a very important feature when querying large databases, as sending and processing entire tables is not ideal. Instead, a common solution is that the client requests data in batches, for example, 10 items at a time, and only requests the next batch when navigating to the next page.</p> <p>The paginated results do not come back as lists of the entities, but as so-called collections. You can read more about them here: https://chillicream.com/docs/hotchocolate/v14/fetching-data/pagination</p> <p>The documentation for sorting can be found here: https://chillicream.com/docs/hotchocolate/v14/fetching-data/sorting</p> <p><code>json     {         \"data\": {             \"orders\": {                 \"edges\": [                     {                         \"node\": {                             \"id\": 5,                             \"orderItems\": [                                 {                                     \"amount\": 2,                                     \"product\": {                                     \"name\": \"Lego City harbour\",                                     \"price\": 172268.75,                                     \"stock\": 12                                     }                                 },                                 {                                     \"amount\": 1,                                     \"product\": {                                     \"name\": \"Activity playgim\",                                     \"price\": 7488,                                     \"stock\": 21                                     }                                 }                             ]                         }                     }                 ]             }         }     }</code></p> <p>SUBMISSION</p> <p>Upload the modified C# source code. Save the queries from parts 1 and 2 of task 3 as <code>q3_1.txt</code> and <code>q3_2.txt</code>, place them in the root folder along with the images, and submit them.</p> <p>Additionally, take a screenshot where the relevant queries are run. Save the screenshot as <code>f3.png</code> and submit it as part of your solution!</p>"},{"location":"homework/mongodb/","title":"3. MongoDB","text":""},{"location":"homework/mongodb/#3-mongodb","title":"3. MongoDB","text":"<p>You may earn 4 points by completing this exercise.</p> <p>Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle. Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. You need to create a branch, named <code>solution</code>, and work on this branch. After completing the exercises and verifying them, commit and push your submission.</p> <p>Check the required software and tools here.</p> <p>Before you begin, create and initialize the database; use the steps the seminar exercises describe.</p>"},{"location":"homework/mongodb/#exercise-0-neptun-code","title":"Exercise 0: Neptun code","text":"<p>Your very first task is to type your Neptun code into <code>neptun.txt</code> in the root of the repository.</p>"},{"location":"homework/mongodb/#exercise-1-product-with-the-largest-total-value-2-points","title":"Exercise 1: Product with the largest total value (2 points)","text":"<p>The task is to find the product that has the largest total value within a product category. The total value is the price of the product multiplied by the amount of the product in stock. You need to implement the following method in class <code>ProductRepository</code>.</p> <pre><code>(string, double?) ProductWithLargestTotalValue(ObjectId categoryId)\n</code></pre> <ol> <li> <p>Let us check the test related to this exercise in file <code>TestExercise1.cs</code> to understand what is expected here.</p> <ul> <li>The method accepts a category filter as an argument; products have to be filtered for this category.</li> <li>The return value should be the name of the product (with the largest total value) and the total value itself.</li> <li>If there are no products in the specified category, the return value should be <code>(null, null)</code>.</li> </ul> </li> <li> <p>Use the aggregation pipeline of MongoDB. To see how this aggregation pipeline works, you can refer to the seminar material.</p> <p>You should build a pipeline consisting of the following stages:</p> <ul> <li> <p>Filter the products for the specified category. Use a $match (<code>Match</code>) stage to specify the filter.</p> </li> <li> <p>Calculate for each product the total value (multiply the price and the stock) using a $project (<code>Project</code>) stage. Make sure to include the name of the product, you will need it for the final result.</p> </li> <li> <p>Order the items based on this calculated total value descending. Use a $sort (<code>SortByDescending</code>) stage.</p> </li> <li> <p>Since it is the largest value that we need, take the first item after sorting. Do not forget that there might not be any product in the specified category. Therefore you should use <code>FirstOrDefault</code> to fetch this item.</p> </li> </ul> <p>If the syntax <code>(string, double?)</code> is unfamiliar:</p> <pre><code>return (\"test\", 0.0);\n</code></pre> <p>The function will return with these two values.</p> </li> <li> <p>Implement the repository method. The repository class receives the database as a parameter and saves the collection as a local variable in the class; use this field to manipulate the collection.</p> </li> </ol> <p>There are unit tests available in the solution. You can run the unit tests in Visual Studio, or if you are using another IDE (e.g., VS Code or <code>dotnet cli</code>), then run the tests using the cli. You may update the database connection string in class <code>TestDbFactory</code> if needed.</p> <p>Tests</p> <p>The tests presume that the database is in its initial state. Re-run the database initialization script to restore this state.</p> <p>Do NOT change the unit tests. You may temporarily alter the unit tests if you need to, but make sure to reset your changes before committing.</p> <p>SUBMISSION</p> <p>Upload the changed C# source code.</p> <p>You can run the tests in Visual Studio or using <code>dotnet cli</code>. Make sure that the screenshot includes the source code of the repository and the test execution outcome! Save the screenshot as <code>f1.png</code> and upload as part of your submission!</p> <p>If you are using <code>dotnet cli</code> to run the tests, make sure to display the test names too. Use the <code>-v n</code> command line switch to set detailed logging.</p> <p>The image does not need to show the exact same source code that you submit; there can be some minor changes. If the tests run successfully and you create the screenshot, then later you make some minor change to the source, there is no need for you to update the screenshot.</p>"},{"location":"homework/mongodb/#exercise-2-insert-a-new-product-2-points","title":"Exercise 2: Insert a New Product (2 points)","text":"<p>The task is to create a function for inserting a new product. Several conditions must be met during the insertion, which need to be checked before the operation. Implement the <code>InsertProduct(string name, string category, int vat)</code> function.</p> <p>The following conditions should be considered during insertion:</p> <ul> <li>If a product with the given name already exists, throw an exception (<code>ArgumentException</code>).</li> <li>If the specified category does not yet exist, throw an exception (<code>ArgumentException</code>).</li> <li> <p>If the given <code>vat</code> value is already associated with the name of another <code>Product</code> instance, use that name; otherwise, assign the name <code>\"VAT\"</code> to the product being inserted.</p> </li> <li> <p>Extend the repository class constructor to include categories. To do this, add the corresponding class for the category!</p> </li> <li> <p>Implement the function, ensuring that the parameter constraints are checked before performing the insertion! The tests in the <code>TestExercise2.cs</code> file will assist you.</p> </li> </ul> <p>SUBMISSION</p> <p>Upload the modified C# source code!</p> <p>Additionally, take a screenshot similar to the one in the first task, where you have run the relevant tests! Save the image as <code>f2.png</code> and upload it as part of your solution!</p> <p>YOU HAVE NOT FINISHED YET</p> <p>After pushing your code to the <code>solution</code> branch, create a PR and assign your instructor (github username: <code>akosdudas</code>) to it! (more details: on the assignment submission page)</p>"},{"location":"homework/mongodb/#exercise-3-optional-estimating-storage-space-0-points","title":"Exercise 3 optional: Estimating storage space (0 points)","text":"<p>The company is moving to a new location. We need to know whether the current stock can be moved and will fit into the new storage facility. Implement the method that calculates the total volume of all products in stock!</p> <p>The products have the necessary information in <code>description.product.package_parameters</code>:</p> <p></p> <p>Use this to calculate the total volume of all items:</p> <ul> <li>Use the information from <code>package_parameters</code> (and not from <code>product_size</code>).</li> <li>A product might have multiple packages; this information is available in <code>package_parameters.number_of_packages</code>. This number shall be used as a multiplicator. Each product has a single size, and if it has multiple packages, then all packages are of the same size.</li> <li>The final total: for all products \u03a3 (product stock * number of packages * width * height * depth).</li> <li>If a product does not have these information, it's volume should be calculated as 0.</li> <li>Mind, that the size also has a unit: either cm or m, but the final value is expected in cubic meter.</li> </ul> <p>Implement method <code>double GetAllProductsCumulativeVolume()</code> that returns a single scalar total of the volume in cubic meters. The calculation should be performed by the database (not in C#); use the aggregation pipeline.</p> <p>Sum aggregation</p> <p>You will need the <code>$group</code> aggregation stage. Although, we do not need to group the products, still, this will allow us to aggregate all of them. Map each product into the same group (that is, in the <code>$group</code> stage the <code>id</code> should be a constant for all items), then use the projection part to perform a <code>$sum</code> aggregation according to the formula above.</p> <p>Handling of the cm and m units can be solved with a conditional multiplication in the <code>sum</code>. If this does not work, you can use two aggregations: one for cm and one for m units, each with a filter in the pipeline then the aggregation afterwards.</p> <p>The required parts of the products are not mapped to C# classes yet. You need to do this. Note, that the field names in the BSON do not conform to the usual syntax, thus, when mapping to C# properties, you have to take care of name the properties identically, or use the <code>[BsonElement(elementName: \"...\")]</code> attribute. Make sure you do not break your solution in excercise 2. You should modify product insert method with the following: product dimensions should be 1x1x1 cm, and for other unspecified variables, you may choose any valid value.</p> <p>Use Fluent Api</p> <p>You must use the C# Fluent Api! Do not write the query using <code>BsonDocument</code>!</p> <p>You may test your implementation with the tests provided in class <code>TestExercise3</code>. The tests presume that the database is in its initial state.</p> <p>SUBMISSION</p> <p>Upload the changed C# source code.</p>"},{"location":"homework/mssql/","title":"1. MSSQL server-side programming","text":""},{"location":"homework/mssql/#1-mssql-server-side-programming","title":"1. MSSQL server-side programming","text":"<p>You may earn 4 points by completing this exercise.</p> <p>Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle. Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. You need to create a branch, named <code>solution</code>, and work on this branch. After completing the exercises and verifying them, commit and push your submission.</p> <p>Check the required software and tools here.</p>"},{"location":"homework/mssql/#prepare-the-database","title":"Prepare the database","text":"<p>Create a new database with a name that matches your Neptun code. Run the database initialization script to create the tables in this database.</p> <p>Neptun code is important</p> <p>The exercise will ask you for a screenshot that must contain the database name with your Neptun code!</p>"},{"location":"homework/mssql/#exercise-0-neptun-code","title":"Exercise 0: Neptun code","text":"<p>Your very first task is to type your Neptun code into <code>neptun.txt</code> in the root of the repository.</p>"},{"location":"homework/mssql/#exercise-1-password-expiry-maintenance-2-points","title":"Exercise 1: Password expiry maintenance (2 points)","text":"<p>Due to security reasons, we would like to enforce password expiry. For this, we will record the date when the password was last updated.</p> <ol> <li> <p>Add a new column to the <code>Customer</code> table with the name <code>PasswordExpiry</code> storing a date: <code>alter table [Customer] add [PasswordExpiry] datetime</code>.</p> </li> <li> <p>Create a trigger that automatically fills the <code>PasswordExpiry</code> date column when the password value is updated. The new value should be the current date plus one year. The trigger shall calculate the value. When a new Customer is registered (inserted into the table), the column should always be populated automatically. However, when data is updated, only update the date if the password is changed. (E.g. if only the address is altered, the date should not be updated.) The trigger should only update the date for the inserted/modified record (it should not set it for all records in the table)! In this exercise, you may suppose that there could be more than one record inserted/modified at any time.</p> </li> </ol> <p>Make sure to verify the behavior of the trigger under various circumstances.</p> <p>SUBMISSION</p> <p>Submit the code of the trigger in file <code>f1.sql</code>. This sql file should contain a single statement (a single <code>create trigger</code> command) without any <code>use</code> or <code>go</code> commands.</p> <p>Create a screenshot that displays sample records in the <code>Customer</code> table with the automatically populated date values. Make sure that the database name and your Neptun code are visible on the screenshot. Save the screenshot as <code>f1.png</code> and upload it as part of your submission!</p>"},{"location":"homework/mssql/#exercise-2-invoice-cancellation-2-points","title":"Exercise 2: Invoice Cancellation (2 points)","text":"<p>We would like to provide an option to cancel orders using a stored procedure. This procedure will invalidate an invoice identified by the customer's name and the order ID, then restore the inventory by iterating through the items associated with the order.</p> <ol> <li> <p>Create a stored procedure named cancel_invoice that accepts two parameters: the customer's name (named <code>name</code>) and the order ID (named <code>orderId</code>).</p> </li> <li> <p>The stored procedure should verify whether an invoice exists with the given information. If not, it should throw an exception. The exception's <code>error_number</code> should be 51000.</p> </li> <li> <p>If the data is valid, the stored procedure should retrieve all the products listed on the invoice, check the quantities ordered, and add those quantities back to the inventory. (HINT: You may need to gather data from multiple tables, or possibly use a cursor).</p> </li> </ol> <p>Test the procedure to ensure it works correctly!</p> <p>SUBMISSION</p> <p>Submit the trigger code in the <code>f2.sql</code> file. The SQL file should contain only a single statement (just one <code>create procedure cancel_invoice</code>), and should not include any <code>use</code> or <code>go</code> commands!</p> <p>Create a screenshot showing the execution of the stored procedure and its effects, as well as what happens when incorrect data is provided (you can use a window with two tabs, for example). The screenshot should display the name of your database (your Neptun code). Save the screenshot as <code>f2.png</code> and submit it as part of your solution!</p> <p>YOU HAVE NOT FINISHED YET</p> <p>After pushing your code to the <code>solution</code> branch, create a PR and assign your instructor (github username: <code>akosdudas</code>) to it! (more details: on the assignment submission page)</p>"},{"location":"homework/mssql/#exercise-3-optional-product-recommended-age-0-points","title":"Exercise 3 optional: Product recommended age (0 points)","text":"<p>In the evaluation, you will see the text \u201cimsc\u201d in the exercise title; this is meant for the Hungarian students. Please ignore that.</p> <p>The database contains an xml column with the name <code>Description</code> in the <code>Product</code> table. This column has values for some of the records.</p> <p>An example for the content is below:</p> <pre><code>&lt;product&gt;\n  &lt;product_size&gt;\n    &lt;unit&gt;cm&lt;/unit&gt;\n    &lt;width&gt;150&lt;/width&gt;\n    &lt;height&gt;50&lt;/height&gt;\n    &lt;depth&gt;150&lt;/depth&gt;\n  &lt;/product_size&gt;\n  &lt;description&gt;Requires battery (not part of the package).&lt;/description&gt;\n  &lt;recommended_age&gt;0-18 m&lt;/recommended_age&gt;\n&lt;/product&gt;\n</code></pre> <p>We want to extract the <code>recommended_age</code> and move it to a new column in the table.</p> <ol> <li> <p>Add a new column to the <code>Product</code> table with name <code>RecommendedAge</code> storing a text: <code>alter table [Product] add [RecommendedAge] nvarchar(200)</code>. (Do not submit this statement in the solution.)</p> </li> <li> <p>Create a T-SQL script that extracts the content of the <code>&lt;recommended_age&gt;</code> tag from the xml and moves the value into the <code>RecommendedAge</code> column of the table. If the xml description is empty or there is no <code>&lt;recommended_age&gt;</code> tag, the column's value should be <code>NULL</code>. Otherwise, take the tag's text content (without the tag name), copy the value into the column, and remove the tag from the xml. You can presume that there is at most one <code>&lt;recommended_age&gt;</code> element in the xml.</p> </li> </ol> <p>SUBMISSION</p> <p>Submit the T-SQL code in file <code>f3.sql</code>. Do not use a stored procedure in this exercise; create a simple T-SQL code block. This sql file should be executable by itself and should not contain any <code>use</code> or <code>go</code> commands.</p> <p>Create a screenshot that displays the content of the <code>Product</code> table after running the script. The new column and the populated values should be visible on the screenshot. Make sure that the database name and your Neptun code are visible on the screenshot. Save the screenshot as <code>f3.png</code> and upload it as part of your submission!</p>"},{"location":"homework/rest/","title":"4. REST API with Web API Technology","text":""},{"location":"homework/rest/#4-rest-api-with-web-api-technology","title":"4. REST API with Web API Technology","text":"<p>You may earn 4 points by completing this exercise.</p> <p>Use GitHub Classroom to get your git repository. You can find the invitation link in Moodle. Clone the repository created via the link. It contains a skeleton and the expected structure of your submission. You need to create a branch, named <code>solution</code>, and work on this branch. After completing the exercises and verifying them, commit and push your submission.</p> <p>The required software and tools for the solution can be found here.</p>"},{"location":"homework/rest/#exercise-0-neptun-code","title":"Exercise 0: Neptun code","text":"<p>As a first step, enter your Neptun code into the <code>neptun.txt</code> file located in the root folder.</p>"},{"location":"homework/rest/#exercise-1-simple-query-and-openapi-documentation-2-points","title":"Exercise 1: Simple query and OpenAPI documentation (2 points)","text":"<p>In the created and cloned repository, you'll find the initial code structure. Open it with Visual Studio and start the project. The basic program provides an empty solution. A console application should be started that hosts the web application. Try it out (while the program is running): open http://localhost:5000/api/product in your browser, where you should see the list of products in JSON format.</p> <p>Check the available code.</p> <ul> <li>The <code>Program.cs</code> initializes the application. This is an ASP.NET Core web application.</li> <li>For simplicity, there is no database access in the application. The <code>ProductRepository</code> provides data to be used for testing.</li> <li>The <code>ProductsController</code> instantiates the <code>IProductRepository</code> using dependency injection.</li> </ul> <p>Tasks:</p>"},{"location":"homework/rest/#simple-query","title":"Simple Query","text":"<ol> <li> <p>In the <code>DAL.ProductRepository</code> class, replace the value of the field named <code>Neptun</code> with your Neptun code. The string value should consist of 6 characters from your Neptun code.</p> <p>IMPORTANT</p> <p>A screenshot must be taken of the modified data, so this step is important.</p> </li> <li> <p>Create an API endpoint to check if a product with a given id exists. We will send a <code>HEAD</code> HTTP request to the <code>/api/product/{id}</code> URL for the query. The response should be HTTP 200 or 404 (without any extra content/body, only the response code is required).</p> </li> </ol>"},{"location":"homework/rest/#openapi-documentation","title":"OpenAPI Documentation","text":"<p>OpenAPI (formerly known as Swagger) is a documentation tool for REST APIs. Its purpose is similar to that of WSDL used for Web Services: to describe the API services in a standardized format. After completing the previous tasks, create an OpenAPI specification and documentation for the REST API description.</p> <ol> <li> <p>To complete the solution, follow the official Microsoft documentation: https://docs.microsoft.com/en-us/aspnet/core/tutorials/getting-started-with-swashbuckle</p> <ul> <li>Make sure to use the Swashbuckle option.</li> <li>The <code>swagger.json</code> should be generated by the application itself (you don't need to write it manually), and it should be accessible at <code>/swagger/v1/swagger.json</code>.</li> <li>Also set up the Swagger UI, which should be accessible at <code>/neptun</code>. You can achieve this by configuring the <code>RoutePrefix</code> in the <code>UseSwaggerUI</code> setup. Your Neptun code should be the prefix in all lowercase.</li> <li>We need to use swagger components not only in Development mode.</li> <li>(You do not need to deal with the \"Customize and extend\" section or other customizations.)</li> </ul> </li> <li> <p>Start the web application and check the <code>swagger.json</code> at http://localhost:5000/swagger/v1/swagger.json, and try out the SwaggerUI at http://localhost:5000/neptun.</p> </li> <li> <p>Test the \"Try it out\" feature of SwaggerUI: it actually sends the request to the web application, and you can see the real response.</p> <p></p> </li> <li> <p>Create the API endpoint that returns the desired product (<code>Product</code>) based on its ID; the request should be of type <code>GET</code> to the <code>/api/product/{id}</code> URL, and the response should either be 200 with the data or 404 if no such item exists. Verify this using SwaggerUI.</p> </li> </ol> <p>SUBMISSION</p> <p>Upload the modified source code. Make sure to note that the <code>csproj</code> file has also been modified with the added NuGet package!</p> <p>Take a screenshot of the Swagger UI displayed in the browser. Ensure that the URL shows that the SwaggerUI is served at the <code>/neptun</code> path with your own Neptun code. Save the image as <code>f1.png</code> and submit it as part of your solution!</p>"},{"location":"homework/rest/#exercise-2-product-operations-2-points","title":"Exercise 2: Product Operations (2 points)","text":"<p>The most common database operations related to products are inserting a new one, querying an existing product, modifying it, or deleting it, which are the CRUD (create, read, update, and delete) operations. We will create dedicated endpoints for these operations, allowing the API user to perform them. In this task, you need to implement the most common endpoints alongside the existing query.</p> <ol> <li> <p>Create an API endpoint that inserts a new product (<code>Product</code>) based on its ID; the request should be of type <code>POST</code> to the <code>/api/product</code> address, expecting the new <code>Product</code> value in the request body, and the response should be either 201 or 409 if such an item with the given name already exists.</p> </li> <li> <p>Create an API endpoint that modifies a product (<code>Product</code>) based on its ID; the request should be of type <code>PUT</code> to the <code>/api/product/{id}</code> address, expecting the modified <code>Product</code> value in the request body, and the response should be either 204 with no content or 404 if such an item does not exist.</p> </li> <li> <p>Create an API endpoint that deletes a product (<code>Product</code>) based on its ID; the request should be of type <code>DELETE</code> to the <code>/api/product/{id}</code> address, and the response should be either 204 with no content or 404 if such an item does not exist.</p> </li> <li> <p>Create an API endpoint that allows querying how many different products there are in total. (For example, to facilitate pagination, the frontend can calculate how many pages there will be.) This should also be a <code>GET</code> type request to the <code>/api/product/-/count</code> address. The returned data should be an instance of the <code>CountResult</code> class filled with the total count (in JSON format, of course).</p> Why is there a <code>/-</code> part in the URL? <p>To understand this, let's consider what the URL could be: we are curious about the number of products, so it could be <code>/api/product/</code>, but what comes after? It could be <code>/api/product/count</code>. However, this \"conflicts\" with URLs like <code>/api/product/123</code>, which is intended for querying a specific product. In practice, the two URLs could work together, because the product identifier is currently a number, so the framework recognizes that if the URL ends with <code>/123</code>, it needs to execute the endpoint expecting a product ID, but if it ends with <code>/count</code>, it should provide the count. But this only works if the ID is an integer. If the product identifier were text, there would be a problem. In such cases, a URL needs to be \"invented\" that does not collide. The <code>/-</code> part indicates that there is not a product ID traveling there.</p> <p>Note: The URL - controller method identification is more complex in reality than described above. The ASP.NET Core framework matches controller methods to incoming request URLs in order of priority. We have the ability to influence this priority using the <code>[Http*]</code> attributes <code>Order</code> property.</p> </li> </ol> <p>SUBMISSION</p> <p>Upload the modified source code.</p> <p>Additionally, take a screenshot from Postman (or another testing tool) showing the result of a successful product retrieval. The image should display all details of the request and response (request type, URL, response code, response content). Your Neptun code must appear in the response. Save the image as <code>f2.png</code> and submit it as part of your solution!</p> <p>YOU HAVE NOT FINISHED YET</p> <p>After pushing your code to the <code>solution</code> branch, create a PR and assign your instructor (github username: <code>akosdudas</code>) to it! (more details: on the assignment submission page)</p>"},{"location":"homework/rest/#exercise-3-optional-partial-update-of-product-0-points","title":"Exercise 3 optional: Partial Update of Product (0 points)","text":"<p>When modifying products, the previously used <code>PUT</code> call has several disadvantages. <code>PUT</code> is designed for updating the entire resource, meaning that to modify a product, the entire product must be sent. This is not only inefficient (for example, it requires retrieving and sending every existing property, regardless of their size) but also adds extra processing tasks. The <code>PATCH</code> verb is designed to provide partial update capabilities, allowing only the properties that need to be modified to be sent.</p> <p>In this task, you need to create an endpoint that allows for partial updates of products:</p> <ol> <li> <p>The request should be of type <code>PATCH</code> to the <code>/api/product/{id}</code> URL, and the response should either be 204 if successful, or 404 if the item does not exist.</p> </li> <li> <p>Implement the endpoint in the <code>ProductController</code> class that performs the partial update. The type of the parameter received by the endpoint should be a strongly typed variant of <code>JsonPatchDocument</code>. During testing, ensure that only the values sent are changed (for example, if the sent object does not include stock, that should not change).</p> </li> </ol> <p>SUBMISSION</p> <p>Upload the modified source code.</p> <p>Additionally, take a screenshot from Postman (or another testing tool) showing the result of a successful partial modification. The image should display all details of the request and response (request type, URL, response code, response content). Your Neptun code must appear in the response. Save the image as <code>f3.png</code> and submit it as part of your solution!</p>"},{"location":"lecture-notes/SQL_Cheat_Sheet/","title":"Query Syntax Cheat Sheet","text":""},{"location":"lecture-notes/SQL_Cheat_Sheet/#query-syntax-cheat-sheet","title":"Query Syntax Cheat Sheet","text":"<p>In this cheat sheet, the query syntax of various programming languages are compared.</p> <p>Warning</p> <p>When using C# LINQ do not forget about creating the database context in the <code>using</code> block!</p> <pre><code>using (var db = new AdatvezDbContext())\n{\n    // queries, etc.\n}\n</code></pre>"},{"location":"lecture-notes/SQL_Cheat_Sheet/#select","title":"SELECT","text":"<p>SQL:</p> <pre><code>SELECT column1, column2\nFROM table\nWHERE condition;\n</code></pre> <p>C# LINQ:</p> <p>Tip</p> <p>In the <code>Select()</code> method, an anonymous object is created. Its properties can be freely chosen to perform the projection operation.</p> <pre><code>//  Fluent syntax\nvar query = context.Table\n            .Where(item =&gt; condition)\n            .Select(item =&gt; new { item.Column1, item.Column2 });\n\n //  Query syntax       \nvar query = from item in context.Table\n            where condition\n            select new { item.Column1, item.Column2 };\n</code></pre> <p>Warning</p> <p>If we need entities referenced by navigation properties, the developer specifies this in the code using <code>Include()</code>. The system loads the requested references.</p> <p></p><pre><code>var prod = db.Products\n    .Include(p =&gt; p.VAT)\n    .Where(p =&gt; p.ID==23)\n    .SingleOrDefault();\nif(prodis not null){\n    Console.WriteLine(prod.VAT.ID);\n}\n</code></pre> C# MongoDb:<p></p> <p>Similar to Select in LINQ, use it when we use <code>.Project()</code> to project to specific attributes.</p> <pre><code>collection\n        Find(item=&gt;item.Column1 == value)\n        .Project(item =&gt; new\n        {\n            Attr1 = item.Coloumn1,\n            Attr2 = item.Coloumn2\n        }).ToList();\n</code></pre>"},{"location":"lecture-notes/SQL_Cheat_Sheet/#where","title":"WHERE","text":"<p>SQL:</p> <pre><code>SELECT *\nFROM table_name\nWHERE column_name = \"Example\";\n</code></pre> <p>Query where a field is <code>NULL</code>:</p> <pre><code>SELECT *\nFROM table_name\nWHERE column_name IS NULL;\n-- To check the opposite, use IS NOT NULL\n</code></pre> <p><code>ISNULL</code> function: If there is a value in the salary column, the query continues with that value. Otherwise, it uses the value provided as the second parameter (in this case, 0).</p> <pre><code>SELECT employee_id, employee_name, ISNULL(salary, 0) AS modified_salary\nFROM employees;\n</code></pre> <p>Subqueries can be named, and their results can be referenced. The example below (from the Microsoft SQL seminar) finds the product category with the most items.</p> <pre><code>SELECT TOP 1 Name, (SELECT COUNT(*) FROM Product WHERE Product.CategoryID = c.ID) AS cnt\nFROM Category c\nORDER BY cnt DESC\n</code></pre> <p>C# LINQ:</p> <p>Task: Select rows where the price is less than 1000:</p> <p></p><pre><code>// Fluent syntax\ndb.products.Where(p =&gt; p.Price &lt; 1000)\n// Query syntax\nfrom p in products\nwhere p.Price &lt; 1000\n</code></pre> C# MongoDb:<p></p> <p>Warning</p> <p>In MongoDB's <code>.Find()</code> operation, only conditions that apply to the document itself can be used; it is not possible to reference or join with another collection!</p> <p>Warning</p> <p>In MongoDb, you must always include a <code>.Find()</code> in the commands (unless using <code>Aggregate()</code>). A common parameterization because od this constraint is <code>.Find(_ =&gt; true)</code> of the <code>Find()</code> command.</p> <p>Warning</p> <p>The result of the <code>Find()</code> function is not the result set; it is just a descriptor for executing the query. To fetch the entire result set as a list, use <code>.ToList()</code>. (If you want to return a single element of a query you should use an other command, see below.)</p> <pre><code>collection.Find(item =&gt; item.Column == value).ToList();\n</code></pre>"},{"location":"lecture-notes/SQL_Cheat_Sheet/#limiting-the-result-set-count","title":"Limiting the result set count","text":"<p>SQL:</p> <p>In cases where you want to retrieve only one or just a few row from your queries. </p><pre><code>SELECT TOP 1 *\nFROM Product\nORDER BY Name\n</code></pre> C# (LINQ \u00e9s MongoDb):<p></p> <p>If you need only the first element or know there will be only one element, you can use <code>.First()</code>, <code>.FirstOrDefault()</code>, <code>.Single()</code>, or .<code>SingleOrDefault()</code>. Ensure the preceding query indeed returns a single data element when using <code>.Single()</code> or <code>.SingleOrDefault()</code>. (Else the query will throw an exception)</p> <p>We can also navigate through the results using Skip. <code>C# LINQ:</code></p> <p>Using <code>Take()</code> we can limit the number of results the query gives. </p><pre><code>// Take 10\ndb.products.Take(10)\n\n// Skip 10 then take 10\ndb.products.Skip(10).Take(10)\n</code></pre> C# MongoDb:<p></p> <p>Using <code>Limit()</code> we can limit the number of results the query gives.</p> <pre><code>// Take 10\ncollection.Find(...).Limit(10);\n// /Skip 10 then take 10\ncollection.Find(...).Skip(10).Limit(10);\n</code></pre>"},{"location":"lecture-notes/SQL_Cheat_Sheet/#group-by","title":"Group By","text":"<p>SQL:</p> <p>List the names of products starting with 'M' and their ordered quantities, including products with zero orders. </p><pre><code>SELECT p.Name, SUM(oi.Amount)\nFROM Product p\n     LEFT OUTER JOIN OrderItem oi ON p.id = oi.ProductID\nWHERE p.Name LIKE 'M%'\nGROUP BY p.Name\n</code></pre><p></p> <p>C# LINQ: Group elements by <code>VATID</code>. </p><pre><code>// Fluent szintaktika\ndb.products.GroupBy(p =&gt; p.VATID)\n// Query szintaktika\nfrom p in products\ngroup p by p.VATID\n</code></pre> C# MongoDb: Filter elements which are in the \"Balls\" category and group them by the Vat percentage. <pre><code>collection.Aggregate()\n    .Match(Builders&lt;Product&gt;.Filter.AnyEq(x =&gt; x.Categories, \"Balls\")) //  filter\n    .Group(x =&gt; x.VAT.Percentage, x =&gt; x) //  grouping\n    .ToList()\n</code></pre> Within the <code>Match()</code>, an alternative approach to lambda can be seen. Multiple different keywords and nested Filters can be written here, such as: <pre><code>collection.Find(x =&gt; x.Price == 123);\ncollection.Find(Builders&lt;Product&gt;.Filter.Eq(x =&gt; x.Price, 123)); // Eq, equals\n\ncollection.Find(x =&gt; x.Price != 123);\ncollection.Find(Builders&lt;Product&gt;.Filter.Ne(x =&gt; x.Price, 123)); //  Ne, not equals\n\ncollection.Find(x =&gt; x.Price &gt;= 123);\ncollection.Find(Builders&lt;Product&gt;.Filter.Gte(x =&gt; x.Price, 123)); //  Gte, greater than or equal to\n\ncollection.Find(x =&gt; x.Price &lt; 123);\ncollection.Find(Builders&lt;Product&gt;.Filter.Lt(x =&gt; x.Price, 123)); //  Lt, less than\n\ncollection.Find(x =&gt; x.Price &lt; 500 || x.Stock &lt; 10);\ncollection.Find(\n    Builders&lt;Product&gt;.Filter.Or(\n        Builders&lt;Product&gt;.Filter.Lt(x =&gt; x.Price, 500),\n        Builders&lt;Product&gt;.Filter.Lt(x =&gt; x.Stock, 10)\n    )\n);\n\ncollection.Find(x =&gt; !(x.Price &lt; 500 || x.Stock &lt; 10));\ncollection.Find(\n    Builders&lt;Product&gt;.Filter.Not(\n        Builders&lt;Product&gt;.Filter.Or(\n            Builders&lt;Product&gt;.Filter.Lt(x =&gt; x.Price, 500),\n            Builders&lt;Product&gt;.Filter.Lt(x =&gt; x.Stock, 10)\n        )\n    )\n);\n</code></pre><p></p> <p>In .Group(), you can specify projections similarly to the .Project() function.</p> <pre><code>var r=productsCollection\n    .Aggregate()\n    .Group(\n        //  grouping section\n        p =&gt; p.VAT.Percentage,\n        //  projection section\n        p =&gt; new{\n        vatp=p.Key\n        sumPrice=p.Sum(s=&gt;s.Price)\n    })\n    .ToList();\n</code></pre>"},{"location":"lecture-notes/SQL_Cheat_Sheet/#order-by","title":"Order By","text":"<p>SQL:</p> <pre><code>ORDER BY column1 ASC, column2 DESC;\n</code></pre> <p>C# LINQ:</p> <p>Two-level sorting</p> <pre><code>items.OrderBy(item =&gt; item.Column1)\n     .ThenByDescending(item =&gt; item.Column2)\n</code></pre> <p>C# MongoDb:</p> <pre><code>itemcollection.Sort(Builders&lt;CollectionItem&gt;\n                    .Sort\n                    .Ascending(item=&gt;item.Coloumn))\n</code></pre>"},{"location":"lecture-notes/SQL_Cheat_Sheet/#join","title":"JOIN","text":"<p>SQL:</p> <pre><code>SELECT table1.column, table2.column\nFROM table1\nJOIN table2 ON table1.column = table2.column;\n</code></pre> <p>C# LINQ:</p> <p>Generally, using navigation properties, associated classes can be reached. Explicit joining is necessary only if the task logic requires it or if there's no navigation property in one class pointing to another. </p><pre><code>// Fluent syntax:\nvar query = context.Table1\n    .Join(context.Table2,\n        item1 =&gt; item1.Column,\n        item2 =&gt; item2.Column,\n        (item1, item2) =&gt; new { item1.Column, item2.Column });\n// Query syntax:\nvar query = from item1 in context.Table1\n    join item2 in context.Table2\n    on item1.Column equals item2.Column\n    select new { item1.Column, item2.Column };\n</code></pre><p></p> <p>C# MongoDb:</p> <p><code>A method for server-side joins in MongoDb was not covered. It's done via LINQ after retrieving the entire datasets from the database.</code> After querying the data, typically, the matching is done client-side using dictionaries with .ToHashSet() and .Contains() methods (See MongoDB seminar excercise 1.5).</p>"},{"location":"lecture-notes/SQL_Cheat_Sheet/#distinct","title":"Distinct","text":"<p>SQL:</p> <pre><code>SELECT DISTINCT p.Name\nFROM Product p\n</code></pre> <p>C# LINQ: Task: Get every different product name. </p><pre><code>db.products.Select(p =&gt; p.Name).Distinct();\n</code></pre><p></p> <p>C# MongoDb: Task: All the categories of the products with larger price than 3000. </p><pre><code>var xd = productsCollection\n    .Distinct(p =&gt; p.CategoryID,p=&gt;p.Price&gt;3000)\n    .ToList();\n</code></pre><p></p>"},{"location":"lecture-notes/SQL_Cheat_Sheet/#coloumn-functions","title":"Coloumn functions","text":"<p>SQL:</p> <p>Task: Price of the most expensive item.</p> <pre><code>SELECT MAX(Price)\nFROM Product\n</code></pre> <p>Task: The most expensive items.</p> <pre><code>SELECT *\nFROM Product\nWHERE Price=(SELECT MAX(Price) FROM Product)\n</code></pre> <p><code>C# LINQ:</code></p> <p>Task: Number of products in the \"products\" list.</p> <pre><code>db.products.Count()\n\n//  For example, for Max, Min, Sum, similar approaches can be used\ndb.products.Select(p =&gt; p.Price).Average()\n</code></pre> <p>C# MongoDb:</p> <p>General aggregations can be constructed using pipelines. A pipeline can return multiple results from a document set (e.g., total, average, maximum, or minimum values)</p> <p>Max function:</p> <p>!!! tip Note:     Observe the constant grouping inside <code>Group</code>, which is done to calculate the column function over the entire collection.</p> <pre><code>collection.Aggregate().Group(p=&gt;1,p=&gt;p.Max(x=&gt;x.Stock)).Single();\n</code></pre> <p>Let's see this through the example of grouping. The following query lists the total values of OrderItem records related to an order if the total value exceeds 30,000.</p> <pre><code>var q=ordersCollection.Aggregate()\n    .Project(order =&gt; new\n    {\n        CustomerID = order.CustomerID,\n        Total = order.OrderItems.Sum(oi =&gt; oi.Amount * oi.Price)\n    })\n    .Match(order =&gt; order.Total &gt; 30000)\n    .ToList();\n</code></pre>"},{"location":"lecture-notes/SQL_Cheat_Sheet/#delete","title":"Delete","text":"<p>SQL:</p> <pre><code>DELETE\nFROM Product\nWHERE ID = 23\n</code></pre> <p>C# LINQ:</p> <pre><code>using (var db = new AdatvezDbContext())\n{\n    var deleteThis=db.Products\n        .Select(p=&gt;p.ID == \"23\")\n        .SingleOrDefault();\n    if(deleteThis!=null)\n    {\n        db.Products.Remove(deleteThis);\n        db.SaveChanges();\n    }\n}\n</code></pre> <p>C# MongoDb:</p> <p></p><pre><code>var deleteResult = collection.DeleteOne(x =&gt; x.Id == new ObjectId(\"...\"));\n</code></pre> Use <code>DeleteMany</code> if you want to delete multiple records.<p></p>"},{"location":"lecture-notes/SQL_Cheat_Sheet/#insert","title":"Insert","text":"<p>SQL:</p> <pre><code>INSERT INTO Product\nVALUES ('aa', 100, 0, 3, 2, null)\n</code></pre> <p>When inserting the results of an other query:</p> <pre><code>INSERT INTO Product (Name, Price)\nSELECT Name, Price\nFROM InvoiceItem\nWHERE Amount&gt;2\n</code></pre> <p>C# LINQ:</p> <pre><code>using (var db = new AdatvezDbContext())\n{\n    db.Table.Add(new dataItem { Name = \"Example\" });\n    db.SaveChanges();\n}\n</code></pre> <p>C# MongoDb:</p> <pre><code>var newProduct = new Product\n{\n    Name = \"Apple\",\n    Price = 890,\n    Categories = new[] { \"Fruits\" }\n};\ncollection.InsertOne(newProduct);\n</code></pre>"},{"location":"lecture-notes/SQL_Cheat_Sheet/#update","title":"Update","text":"<p>6</p> <p>Task: Increase the prices of products containing the word \"Lego\" in their names by 10%.</p> <pre><code>UPDATE Product\nSET Price = 1.1*Price\nWHERE Name LIKE '%Lego%'\n</code></pre> <p>If you want to assign values from another table in the SET command, as shown in the example below (from the Microsoft SQL seminar): Task: Copy the status of order ID 9 to all corresponding OrderItems.</p> <pre><code>UPDATE OrderItem\nSET StatusID = o.StatusID\nFROM OrderItem oi\nINNER JOIN Order o ON o.Id = oi.OrderID\nWHERE o.ID = 9;\n</code></pre> <p>C# LINQ:</p> <p>Warning</p> <p>Spoiler for the solution to Entity Framework seminar's task 3.</p> <p>Task: Write C# code based on LINQ that increases the prices of products in the \"LEGO\" category by 10%.</p> <pre><code>using (var db = new AdatvezDbContext())\n{\n    var legoProductsQuery = db.Products\n        .Where(p =&gt; p.Category.Name == \"LEGO\")\n        .ToList();\n    foreach (var p in legoProductsQuery)\n    {\n        p.Price = 1.1m * p.Price;\n    }\n    db.SaveChanges();\n}\n</code></pre> <p>C# MongoDb:</p> <p>Updating a single element:</p> <pre><code>collection.UpdateOne(\n    filter: x =&gt; x.Id == new ObjectId(\"...\"),\n    update: Builders&lt;Product&gt;.Update.Set(x =&gt; x.Stock, 5));\n</code></pre> <p>It's evident that after Update, different operators can be written similar to Filter. These operators include: Set, UnSet, SetOnInsert, CurrentDate, Mul, Min, Max, AddToSet (Detailed descriptions can be found in the lecture notes).</p> <p>Updating all items within category ID 13:</p> <pre><code>productsCollection.UpdateMany(\n    filter: p =&gt; p.CategoryID == 13,\n    update: Builders&lt;Product&gt;.Update.Mul(p =&gt; p.Price, 1.1));\n</code></pre> <p>If you want to search and update a matching element or insert one if there's none, you can use the <code>IsUpsert</code> function as below.</p> <pre><code>var catExpensiveToys = categoriesCollection.FindOneAndUpdate&lt;Category&gt;(\n    filter: c =&gt; c.Name == \"Expensive toys\",\n    update: Builders&lt;Category&gt;.Update.SetOnInsert(c =&gt; c.Name, \"Expensive toys\"),\n    options: new FindOneAndUpdateOptions&lt;Category, Category&gt; { IsUpsert = true, ReturnDocument = ReturnDocument.After });\n</code></pre>"},{"location":"lecture-notes/SQL_Cheat_Sheet/#summary-table","title":"Summary Table","text":"SQL C# LINQ C# MongoDb <code>SELECT</code> <code>Select()</code> <code>Project()</code> <code>WHERE</code> <code>Where()</code> <code>Find()</code> <code>GROUP BY</code> <code>GroupBy()</code> <code>Group()</code> <code>ORDER BY</code> <code>OrderBy()</code> <code>Sort()</code> <code>JOIN</code> Use navigation properties if possible, else: <code>Join()</code> <code>Join()</code> <code>DISTINCT</code> <code>Distinct()</code> <code>Distinct()</code> <code>Count()</code>, <code>Max()</code>, <code>Average()</code> <code>Count()</code>, <code>Max()</code>, <code>Average()</code> Use after <code>Aggregate()</code>: <code>Count()</code>, <code>Max()</code>, <code>Average()</code> <code>DELETE FROM</code> <code>.Remove()</code>, and <code>db.SaveChanges()</code> to save <code>.DeleteOne()</code>, <code>.DeleteMany()</code> <code>UPDATE ... SET</code> Modify the data and then <code>db.SaveChanges()</code> <code>.UpdateOne()</code>, <code>.UpdateMany()</code> <code>INSERT INTO</code> <code>.Add()</code> and then <code>db.SaveChanges()</code> <code>.InsertOne()</code>, <code>.InsertMany()</code>"},{"location":"lecture-notes/adonet/","title":"ADO.NET data access","text":""},{"location":"lecture-notes/adonet/#adonet-data-access","title":"ADO.NET data access","text":""},{"location":"lecture-notes/adonet/#what-is-adonet","title":"What is ADO.NET?","text":"<p>In data-driven applications, it is important for the data access layer to provide simple and convenient access to the database, making it easier to create complex queries. And all this should be as independent of the database engine itself as possible.</p> <p>The ADO.NET (ActiveX Data Object) developed by Microsoft is a data access class library designed to meet these needs. As part of .NET, it provides a rich toolset for building data-driven applications, providing easy access to relational databases, regardless of the specific type of database.</p> <p>ADO.NET is a powerful tool because it enables unified, database engine-independent access. The library contains interfaces and abstract classes that have many implementations (e.g., for Microsoft SQL Server or OleDB). And there are also third-party implementations compatible with ADO.NET for the databases that there is no built-in support.</p> <p>The Entity Framework is also based on ADO.NET in the background.</p> <p>The place of ADO.NET in a data-driven application architecture is as follows:</p> <p></p> <p>ADO.NET provides services in the data access layer and handles communication with the database engine in the background using drivers installed on the system and operating system services (such as a network connection).</p> <p>The typical building blocks of data access libraries are:</p> <ul> <li>Connection - the database specific connection to the server</li> <li>Command - injection-safe command sent to the server</li> <li>ResultSet - result set returned by the server</li> <li>Exception - errors thrown by the library</li> </ul> <p>These will be discussed in depth below.</p>"},{"location":"lecture-notes/adonet/#connection","title":"Connection","text":"<p>The ADO.NET library provides the <code>IDbConnection</code> interface to represent database connections. It contains the functions needed to manage the connection to the database, such as <code>Open()</code>, <code>Close()</code>, or <code>BeginTransaction()</code>. This interface is implemented by database engine-specific connection classes, such as the <code>SqlConnection</code> class, which implements the connection to Microsoft SQL Server.</p> <p>We need to know that setting up a new connection to a database is relatively expensive (opening a new network connection, negotiating protocols with the server, authentication, etc.). Therefore, we use connection pooling, where, after using and closing a connection, we do not discard it but put it back in a pool to reuse it later.</p> <p>The availability of connection pool depends on the implementation; it supported by the MS SQL Server and OleDD implementations. Connection pools are created for every connection string (so not per database). This is important if the application does not use a single static connection string (but connects on behalf of the user, for example).</p> <p>We also have to understand the problem of connection leak, which means that a connection is left open after use (we do not call <code>Close()</code>), so it is not returned to the pool, which prohibits future reuse. If we do leak connections this way, the pool will soon run out of connections, and the application will stop working due to not being able to talk to the database. This problem must be avoided by closing or disposing of connections safely (see sample code later).</p> <p>We need the aforementioned connection string to connect to the database. It is a text variable that describes the parameters used to connect to the database, such as a username, password, or server address. Connection strings have database server-specific syntax and may also be a point of attacks.</p> <p>The connection string can be stored as text in the configuration file, or the application code can build it. Below is a sample code creating a connection and using a <code>ConnectionStringBuilder</code>:</p> <pre><code>var builder = new SqlConnectionStringBuilder();\nbuilder.UserID = \"User\";\nbuilder.Password = \"Pw\";\nbuilder.DataSource = \"database.server.hu\";\nbuilder.InitialCatalog = \"datadriven\";\n\nvar conn = new SqlConnection(builder.ConnectionString);\n\nconn.Open();\n... // queries\nconn.Close(); // must close at the end - see a better solution later\n</code></pre>"},{"location":"lecture-notes/adonet/#command","title":"Command","text":"<p>After establishing the connection to the database, we want to run queries. To do this, ADO.NET provides the <code>IDbCommand</code> interface, which represents a command. The implementations of this interface, such as <code>SqlCommand</code>, just like the connection, are specific to the database server.</p>"},{"location":"lecture-notes/adonet/#creating-the-command","title":"Creating the command","text":"<p>By setting the following main properties of an <code>IDbCommand</code> we can configure how the given command will be interpreted:</p> <ul> <li><code>CommandType</code>: there are three types<ul> <li>StoredProcedure</li> <li>query the entire table (TableDirect)</li> <li>SQL query (Text) - default</li> </ul> </li> <li><code>CommandText</code>: the text of the command or the name of a stored procedure</li> <li><code>Connection</code>: database connection</li> <li><code>Transaction</code>: the transaction</li> <li><code>CommandTimeout</code>: timeout for waiting to the result (30 seconds by default)</li> <li><code>Parameters</code>: parameters to prevent SQL injection attack</li> </ul> <p>Note that the command must specify the connection. Also note, that the transaction is also a property of the command. This is because it is up to the developer to decide whether to consider a particular command as part of a transaction.</p>"},{"location":"lecture-notes/adonet/#execution","title":"Execution","text":"<p>Once we have a command object, we execute it. Depending on the expected return value, we can choose from several options (methods on the command object):</p> <ul> <li><code>ExecuteReader</code>: query multiple records</li> <li><code>ExecuteScalar</code>: query a single scalar value</li> <li><code>ExecuteNonQuery</code>: a query that does not return a result (e.g., <code>INSERT</code>) - instead, it returns the number of rows affected, e.g., in case of deletion, it is possible to decide whether the operation was successful (whether the record to be deleted was found)</li> <li><code>ExecuteXmlReader</code> (MS SQL Server): returns an XML document (<code>XmlReader</code> object), the result is a single XML field of a record</li> </ul> <p>You can also reuse commands after calling <code>Command.Prepare()</code>. It prepares the command to run on the server-side, but it is only worth it if we run the same statement (possibly with different parameter values).</p> <p>Sample code for using the command:</p> <pre><code>// establish the connection\n...\nvar command = new SqlCommand();\ncommand.Connection = connection;\n// setting command.Connection it not enecessary if we use the connection to instantiate\n// the command as: command = connection.CreateCommand()\ncommand.CommandType = CommandType.StoredProcedure;\ncommand.CommandText = \"SalesByCategory\"; // name of the stored procedure\n\n/* equivalent syntax\nvar command = new SqlCommand()\n{\n    Connection = connection,\n    CommandType = CommandType.StoredProcedure,\n    CommandText = \"SalesByCategory\"\n}*/\n\n// protection against SQL injection\nvar parameter = new SqlParameter();\nparameter.ParameterName = \"@CategoryName\"; // matches the stored procedures argument name\nparameter.SqlDbType = SqlDbType.NVarChar;\nparameter.Value = categoryName; // assign value from a C# variable\ncommand.Parameters.Add(parameter);\n\nvar reader = command.ExecuteReader();\n// see processing the results later\n</code></pre>"},{"location":"lecture-notes/adonet/#transactions","title":"Transactions","text":"<p>Transactions in ADO.NET do not need to be initiated with the <code>begin tran</code> SQL statement. ADO.NET provides methods to create and manage the transaction, as shown in the code snippet below. Also, note the <code>using</code> blocks to properly and securely close resources.</p> <pre><code>// ... creating the connection string\nusing (var connection = new SqlConnection(connectionString))\n{\n    connection.Open(); // let us not forget this, instantiation does not open the connection\n\n    var transaction = connection.BeginTransaction();\n    // parameter might include a name for the transaction and/or the isolation level\n\n    var command = new SqlCommand()\n    {\n        Connection = connection,\n        CommandText = \"INSERT into CarTable (Description) VALUES('...')\",\n        Transaction = transaction,\n    }\n\n    try\n    {\n        command.ExecuteNonQuery();\n        // MUST commit a successful transaction\n        transaction.Commit();\n\n        Console.WriteLine(\"Transaction finished!\");\n    }\n    catch(Exception commitException)\n    {\n        Console.WriteLine(\"Commit Exception: \", commitException.ToString());\n\n        // The rollback below is not necessary. The system performs it automatically\n        // for any non-committed transaction. The code below is only a possibility.\n        try\n        {\n            transaction.RollBack();\n        }\n        catch(Exception rollBackException)\n        {\n            Console.WriteLine(\"Rollback Exception: \", rollBackException.ToString());\n        }\n    }\n}\n</code></pre> <p>Transaction timeout</p> <p>The total time of all ADO.NET transactions is limited by the setting in the MachineConfig. This is a system-wide setting that applies to all .NET applications running on a system, so it is not a good idea to change this setting. Long-running transactions are to be avoided anyway.</p> <p>A transaction usually belongs to a single <code>Connection</code> object, but we can also create a transaction involving multiple persistent resource managers (other databases, message queues, anything that supports transactions). At this point, we would be talking about distributed transaction management, which requires an external transaction manager, such as the Microsoft Distributed Transaction Coordinator (MS DTC). Such cases should also be avoided generally.</p>"},{"location":"lecture-notes/adonet/#null-values","title":"NULL values","text":"<p>How do we know that the result of our query is an empty set? And how do we know that a column contains no value? In .NET we usually check if a value equals <code>null</code>. However, a <code>NULL</code> value in a database is represented differently by ADO.NET \u200b\u200bdepending on the underlying type (int, string, etc.). So how do we check to see if there is a value in the query result?</p> <ul> <li>If we want to check if the result set of a query contains any records, we can do so by examining the <code>bool</code> property <code>DataReader.HasRows</code>.</li> <li>To examine the value of a particular column in the result set, we can use <code>reader[\"column name\"].IsNull</code> or <code>reader.IsDbNull(index)</code>.</li> <li>And if we want to manually insert a value of <code>NULL</code> into a new record, we can use, for example, <code>SqlString.Null</code> or <code>DBNull.Value</code> in the C# code.</li> </ul>"},{"location":"lecture-notes/adonet/#query-result","title":"Query result","text":"<p>ADO.NET offers two ways of fetching data from a database and working with it: <code>DataReader</code> and <code>DataSet</code>. The main difference between the two solutions is how these use the database connection. These two models are, in general, called: connection-based (<code>DataReader</code>) and connection-less (<code>DataSet</code>) data access. In the connection-based model, the connection to the database is maintained throughout the queries as long as we work with the data. While in the connection-less model modifications are performed in a <code>DataSet</code>, which is synchronized with the database itself (establishing the connection only for the duration of the synchronization). Both options have advantages and disadvantages, which we will discuss in the following sections.</p>"},{"location":"lecture-notes/adonet/#datareader","title":"DataReader","text":"<p>Here, we need a connection to the database to fetch the required data from the database. The connection remains open only for a short time, during which we query fresh data and usually convert it to some other internal representation.</p> <p>Processing steps:</p> <ol> <li>open the connection</li> <li>run command(s) to query data</li> <li>process the results (typically: convert the data to business entities)</li> <li>close the reader</li> <li>close the connection</li> </ol> <p>The flow of data using the <code>DataReader</code> is as follows:</p> <p></p> <p>Sample code using a <code>DataReader</code>:</p> <pre><code>using(var conn = new SqlConnection(connectionString))\n{\n    var command = new SqlCommand()\n    {\n      Connection = conn,\n      CommandText = \"SELECT ID, NAME FROM Product\"\n    }\n\n    conn.Open();\n    using(var reader = command.ExecuteReader())\n    {\n        while(reader.Read())\n        {\n            Console.WriteLine(\"{0}\\t{1}\", reader[\"ID\"], reader[\"Name\"]);\n            // typically rather create a business entity and add it to a list in memory\n        }\n        // no need for reader.Close() thanks to the using block\n    }\n    // no need for conn.Close() thanks to the using block\n}\n</code></pre> <p>There are a few things worth paying attention to!</p> <ul> <li>The value of <code>reader[\"ID\"]</code> is an <code>object</code>, not a string or an int. We can use <code>reader.Get***(query_in_column_index)</code> instead, where we must specify the data type (<code>String</code>, <code>Int32</code>, etc.).</li> <li>If the types are not compatible (e.g., the column is <code>nvarchar</code> in the database, but we want to read it as <code>int32</code>), we will get a runtime exception.</li> <li>If there is a <code>NULL</code> value in the database, we will also get a runtime error when using the <code>reader.Get***</code> methods. Instead, we should use <code>reader.IsDBNull(query_in_column_index)</code> to verify, and if it is <code>true</code>, we can use the appropriate <code>null</code> value instead.</li> </ul> <p>Advantages</p> <ul> <li> the data is fetched directly from the database; hence, it is up-to-date</li> <li> less painful concurrency management, as fresh the data is fetched</li> <li> needs less memory (compared to DataSets; see later)</li> </ul> <p>Disadvantages</p> <ul> <li>needs an open network connection while operations are being performed - thus, it should not be too long</li> <li>poor scalability with the number of connections - therefore, the connections should be used only for a short time</li> </ul>"},{"location":"lecture-notes/adonet/#dataset","title":"DataSet","text":"<p>A <code>DataSet</code> can be considered as a kind of cache, or in-memory data storage. We use an <code>adapter</code> (such as <code>SqlDataAdapter</code>) to retrieve data from the database and store it in the <code>Dataset</code>, then we close the connection to the database. We can then make work with the data, even make changes to it within the <code>DataSet</code>, and then update the database with the changes using a new connection. It is worth noting that during the time between retrieval and update anyone can modify the same data in the database, thus the disadvantages of <code>DataSet</code> is having to manage conflicts and concurrent data access issues covered previously in transaction management.</p> <p>The steps of working with a DataSet:</p> <ol> <li>Open a connection</li> <li>Fill the <code>DataSet</code> with part of the database</li> <li>Close the connection</li> <li>Work with the <code>DataSet</code> (e.g., display and edit in a user interface) - this may take longer</li> <li>Open a new connection</li> <li>Synchronize changes</li> <li>Close the connection</li> </ol> <p>The operation of data access in this model using an <code>adapter</code> is shown in the figure below.</p> <p></p> <p>The flow of data using the <code>DataSet</code> is as follows:</p> <p></p> <p>Sample code for working with a <code>DataSet</code>:</p> <pre><code>var dataSet = new DataSet();\nvar adapter = new SqlDataAdapter();\n\n// open connection, populate the dataset, close the connection\nusing(var conn = new SqlConnection(connectionString))\n{\n    adapter.SelectCommand = new SqlCommand(\"SELECT * FROM Product\", conn);\n\n    conn.Open();\n    adapter.Fill(dataSet);\n}\n\n-------------------------------------------------------\n// working with the data\n// typically uinvolves UI; this is just a sample\nforeach(var row in dataSet.Tables[\"Product\"].Rows)\n{\n    Console.WriteLine(\"{0}\\t{1}\", row[\"ID\"], row[\"Name\"]);\n    row[\"Name\"] = \"new value\";\n}\n-------------------------------------------------------\n\n// at a later point in time, such as after a \"Save\" button in clicked\n// open connection, synchronize data, close connection\nusing(var conn = new SqlConnection(connectionString))\n{\n    conn.Open();\n    adapter.Update(dataSet);\n    //dataSet.AcceptChanges(); -- would only update the dataset, but not the database\n}\n</code></pre> <p>It is worth noting that the <code>adapter</code> only communicates with the database via <code>Command</code>s. An <code>adapter</code> can use multiple such <code>Command</code>s, so we can even work with multiple <code>Connections</code> towards multiple databases with the same <code>DataSet</code>.</p> <p>Advantages</p> <ul> <li> does not need a long-running connection</li> </ul> <p>Disadvantages</p> <ul> <li>there may be conflicts during saving the changes</li> <li>data in the <code>DataSet</code> may be stale</li> <li>has larger memory footprint - the reason why we do not use it in server applications</li> </ul>"},{"location":"lecture-notes/adonet/#risks","title":"Risks","text":""},{"location":"lecture-notes/adonet/#sql-injection","title":"SQL injection","text":"<p>SQL injection is a severe vulnerability in an application when a query is created without sanitizing the values of parameters. Parameter values can come from the client side, with user-selected or user-specified data. This can cause a problem if a malicious user writes an SQL command into a field from which we would expect something else. For example, we would expect a username, but instead <code>Monkey92); DROP TABLE Users; -</code> value is received. If we were to include this text and insert it into our SQL statement, we would also execute <code>drop table</code>, thereby deleting an entire table. This is a serious mistake!</p> <p>SOLUTION</p> <p>Using parameters (see the Command section for an example).</p>"},{"location":"lecture-notes/adonet/#connection-string","title":"Connection string","text":"<p>Creating a connection string has a flaw similar to SQL injection. Suppose we ask the user for some kind of data (e.g., username, password). In this case, we do not know exactly what we will get. The connection string consists of key-value pairs and many databases apply the last-wins principle. In practice, this means that if more than one value is specified in a string for the same key, the last one takes effect. That is, if after the username and password a key-value pair is added that already appears in the string before, the new value overwrites the old one. This carries a risk, since a malicious user is able to inject specified parameters into the connection string.</p> <p>SOLUTION</p> <p>Using <code>ConnectionStringBuilder</code> (see Connection section).</p>"},{"location":"lecture-notes/adonet/#connection-leak","title":"Connection leak","text":"<p>If we do not close all <code>Connections</code>, any time the code containing the not closed connection is executed, we will retrieve a <code>Connection</code> from the pool without returning it. When the pool is emptied, the application will be stuck not being able to talk to the database at all. This is an error that is hard to spot because it \"only\" happens after the application running for a certain amount of time - and almost never on the developer's machine.</p> <p>SOLUTION</p> <p><code>using</code> block to open the connection, as this will close the connection at the end of the block (see Transaction section example, or DataReader, or DataSet)</p> <p>A <code>DataReader</code> must be closed in the same way.</p>"},{"location":"lecture-notes/architecture/","title":"Data-driven systems and the three- or multi-tier architecture","text":""},{"location":"lecture-notes/architecture/#data-driven-systems-and-the-three-or-multi-tier-architecture","title":"Data-driven systems and the three- or multi-tier architecture","text":""},{"location":"lecture-notes/architecture/#what-is-a-data-driven-system","title":"What is a data-driven system?","text":"<p>Every software handles data in some sense since the computer memory stores data, and the software manipulates this data. But not all applications are data-driven. A system or an application is called data-driven if its  main purpose is to manage data.</p> <p>In other words, the data-drive application is created to store, display, and manage data. The end-user uses this application to access the data within.</p> <p>A chess game app also stores data in memory: the state of the chessboard. But the chess game app is not created to manipulate this data. The game is designed so that a user can play chess.</p> <p>In a data-driven system, the data itself defines how the application operates. For example, based on a data record's specific attributes, deleting this record may be allowed or may be prohibited. Another example is how the Neptun system enables registration to an exam. The semester schedule, which defines the exam period, is data stored within the system itself. This schedule, stored as data, determines whether the end-user (here: the student) can register for an exam. The fact that the exam period starts on a different day each year does not mean that the software logic (i.e., software code) changes; the data makes the software behave differently.</p>"},{"location":"lecture-notes/architecture/#data-driven-system-example","title":"Data-driven system example","text":"<p>The Neptun system is a typical example of a data-driven system. Its purpose is the management of all data related to courses, students, grades.</p> <p>Another example is Gmail: it manages emails, attachments, contacts, etc. Every functionality of the application is about managing and displaying these data. And, of course, the data is stored securely, and every change in the system is persisted (i.e., not lost).</p>"},{"location":"lecture-notes/architecture/#the-structure-of-a-data-driven-system","title":"The structure of a data-driven system","text":"<p>Let us consider Gmail as an example. We would like to build a system which is capable of:</p> <ul> <li>sending and receiving emails,</li> <li>has a web application and a connected mobile app too,</li> <li>the UI supports multiple display languages,</li> <li>we can attach files,</li> <li>attachments can be referenced from Google Drive too,</li> <li>we can delay sending a composed email,</li> <li>etc.</li> </ul> <p>Let us design</p> <p>How do you start developing such a complex application?</p> <p>This might be a complicated question. Let us begin with a more straightforward question. Supposing that the system already works, and now we want to add the delayed sending feature. How do we do this?</p> <p>We could start a timer when the send button is clicked, and this timer, after a minute, sends the email. This will not work if the browser is closed before the countdown is over.</p> <p>We could add the scheduled date of sending to the email as data. We can translate this as an architectural decision: delayed sending is not the responsibility of the user interface. We did not decide yet, which part of the application will be responsible, but we already know it must not be the UI.</p> <p>Let us consider a similar question.</p> <p>The received date of an email should be displayed to the user according to their preference, i.e., in Europe, 15:23 is the preferred date format, while other parts of the world might prefer 3:23 PM. Does this mean that the email as a data record has multiple received dates? Obviously not. The received date is a single date in a universal representation that is transformed by the UI to the appropriate format.</p> <p>To summarize, we established that there are functionalities that the UI is responsible for, and there are other functionalities that the UI has nothing to do with. Now we arrive at the three- or multi-layer architecture.</p>"},{"location":"lecture-notes/architecture/#the-three-or-multi-layered-architecture","title":"The three- or multi-layered architecture","text":"<p>Data-driven systems are usually built on the three- or multi-layered architecture. The two names are treaded as synonyms here. This architecture defines three main components:</p> <ul> <li>the presentation layer (or UI),</li> <li>the business layer,</li> <li>and the data access layer.</li> </ul> <p>Besides, the  architecture also includes:</p> <ul> <li>a database or external data sources;</li> <li>and the so-called cross-cutting concerns (see later).</li> </ul> <p>The application components are organized such that each component belongs to a single layer, and each layer has its responsibilities. This logical grouping of components enables the software developer to design components with clear responsibilities and well-defined boundaries.</p> <p>Why multi-layered when it only has three?</p> <p>The multi-layered terminology enables each of the previously listed layers to be further decomposed into sub-layers depending on complexity. In other words, an architecture is multi-layered if it has more than two layers. (In the two-layered architecture, the UI and the business logic are not separated.)</p> <p>The layers not only have their responsibilities, but also define their interface provided to the layers on top of them. The data access layer specifies the operations the business layer can use to retrieve data; similarly the business layer defines the functionalities the presentation layer can build upon. Each layer is allowed to communicate only with the layer directly beneath. For example, the presentation layer is not allowed to execute a SQL query in the database. At the same time, the implementation behind the well-defined communication interface can change enabling easier maintenance of the software codebase.</p> <p>By having the software split into layers, we can also move the layers to multiple servers (e.g., to handle larger loads). The simplest form is when the presentation layer runs in a browser on the user's machine, while the rest is hosted on a remote server. The database is also frequently offloaded to a dedicated server. Running the various layers on separate servers is usually motivated by performance reasons.</p> <p>Layer / tier</p> <p>The name of the architecture distinguishes the logical and physical separation of the components. Layers mean logical separation hosted on a single machine. Tiers indicate that at least some of the components have dedicated servers.</p> <p>A system built on a well-designed architecture can be used and maintained over a long period. The separation of layers is neither a burden nor a set of mandatory rules. Instead, the layered architecture is a helpful guide for the software developer. When we develop a three-layered architecture, we must understand the layers, roles, and responsibilities.</p> <p>The layered architecture does not mean that a functionality is present in a single layer. Most features offered to the end-user have some display in the presentation layer, handle data in the business logic layer, and store data in the database.</p> <p>The codebase of a three-layered architecture also reflects the separation of the layers. Depending on the capabilities and the conventions on the given platform, the layers all have a dedicated project or package. This structure also enforced one-way dependency, as the dependency-graph of projects/packages usually does not allow circles. That is, if the business layer uses the data access layer, the latter one cannot use the former one.</p> <p>The three-layered architecture is not the only possibility for implementing a data-driven application. Small and simple applications can be build using the two-layered architecture, while larger and more complex applications usually need further separation (e.g., using the microservices architecture).</p>"},{"location":"lecture-notes/architecture/#the-responsibilities-of-the-layers","title":"The responsibilities of the layers","text":"<p>Let us examine the layers in more detail. The following diagram represents the architecture.</p> <p></p> <p>Source</p> <p>Microsoft Application Architecture Guide, 2<sup>nd</sup> Edition, https://docs.microsoft.com/en-us/previous-versions/msp-n-p/ee658109%28v%3dpandp.10%29</p> <p>We will discuss the layers from bottom-to-top.</p>"},{"location":"lecture-notes/architecture/#data-sources","title":"Data sources","text":"<p>The most common data source is a database. It can be a relational-, or a NoSQL database. Its main purpose is the stable, reliable and persistent storage of data. This database is usually a software from a well-known third-party. This component is often hosted on a dedicated server accessible through a local network.</p> <p>Sometimes our application might also work with data outside of our database, hosted by third-party services, that we use similarly to databases. For example, you can attach files in Gmail from Google Drive. Gmail, in this example, fetches the list of available files from Google Drive for the user to select the attachment. Google drive is not a database, yet it is used as a data source.</p> <p>These kinds of external services are grouped with our database, in the architectural sense, because they provide data storage and retrieval services, just like a traditional database. We have no information about their internal operations, and there is no need for users to understand it either. Thus, these services are treated similarly in our architecture.</p> <p>Recently, more and more modern database management systems communicate over HTTP and often offer REST-like interfaces. These trends tend to blur the line between databases and external data sources.</p>"},{"location":"lecture-notes/architecture/#data-access-layer","title":"Data access layer","text":"<p>The responsibility of the data access layer, DAL in short, is to provide convenient access to our data. The main functionality offered here is the storage, retrieval, and modification of data items, data records.</p> <p>The data sources and the data access layer combined is often called the data layer.</p> <p>The data access components provide a bridge towards the databases. Their role is to hide the inherent complexity of data management, and provide these as a convenient service to the upper layers. This includes working with SQL commands, as well as mapping the scheme of the database to a different scheme, consumed by the business layer.</p> <p>When the data is not inside our database, the service agents provide similar services and handle the communication aspects with the external service.</p> <p>This entire layer is often built on a particular technology used to communicate with the database, such as ADO.NET, Entity Framework, or JDBC, JPA, etc. The source code in this layer is often tightly coupled with these data access technologies. It is essential to keep these implementations inside this layer and not let it \"leak\" out of here.</p> <p>IMPORTANT</p> <p>In well-designed systems SQL commands appear only in the data access layer; under no circumstances do other layers assemble or execute SQL queries.</p> <p>Since the data modeling scheme used in databases (i.e., the relational model) and the object-oriented modeling are based on different concepts, this layer is responsible for providing a mapping between the two worlds.  The foreign keys used by the relational scheme are transformed into associations and compositions, and we may even need to perform data conversion between data types supported by the various systems. We will re-visit these issues later.</p> <p>Communication with an external system, whether is it is a database or a third-party service, requires specific techniques. For example, establishing network connections, performing handshakes, and managing the lifetime of these connections is important for performance reasons. Establishing certain kinds of connections, such as HTTP, are usually simple; but connecting to a database server using proprietary protocols may be more complex. Therefore it is the responsibility of the data access layer to manage these connections and use appropriate techniques, such as connection pooling, when necessary. These details are often automatically controlled by the libraries we use.</p> <p>The management of concurrent data accesses and related problems is also the responsibility of this layer. We will discuss this in detail later. We should keep in mind that multiple users usually use a three-layered application/system at the same time (just think of the Neptun system or a webshop), thus concurrent modifications can happen. We will discuss how this is handled and what type of issues we have to resolve.</p>"},{"location":"lecture-notes/architecture/#business-layer","title":"Business Layer","text":"<p>The business layer is the \"heart\" of our application. The databases, the data access layer and the presentation layer are created so that the system can provide the services implemented in the business logic layer.</p> <p>This layer is always specific to the problem domain. The Neptun system manages exams, semester schedules, grades, etc.; a webshop will, on the other hand, manage products, orders, searches, etc.</p> <p>From a high-level point of view this layer is built of:</p> <ul> <li>business entities,</li> <li>business components,</li> <li>and business workflows.</li> </ul> <p>The entities contain the data of our domain. Depending on the goal of the system, entities might cover products and product rating (e.g., in a webshop), or courses and exams (e.g., in Neptun).</p> <p>The entities only store data; the components are responsible for manipulating these entities. The components implement the building blocks of the complex services offered by our system. Such a building block is, for example, finding a product in a webshop by name.</p> <p>The workflows are built on these basic services. The workflows represent the functionalities that the end-users will carry out. A workflow may use multiple components. A classic example of a workflow is the checkout procedure in a webshop: check the products, finalize the order, produce an invoice, send a confirmation email, etc.</p>"},{"location":"lecture-notes/architecture/#service-interfaces","title":"Service interfaces","text":"<p>The architecture diagram above has a services sub-layer. This is considered to be part of the business layer. Its purpose is to provide an interface through which the business logic layer's services can be accessed.</p> <p>Generally, all layers have such interfaces towards the layer built on top of them. The business layer is not unique in this sense. However, it is common nowadays that a business layer has not one, but multiple such interfaces published. The reason for multiple service interfaces is the presence of multiple presentation layers. Just take Gmail as an example: it has a web frontend and mobile apps too. The UIs are similar, but they do not provide identical behavior; therefore, the services consumed by the presentation layers also vary.</p> <p>It is equally common that our application offers a UI and has public API (application programming interface) for allowing third-party integration. These APIs often offer different functionalities than the user interface and also frequently use other transport technologies; hence the need for a dedicated service interface.</p> <p>By our application publishing an API, it can effectively act as a data source for third-party applications.</p> <p>We will talk more about publishing services over various APIs. We will consider the web services and the REST technologies too.</p>"},{"location":"lecture-notes/architecture/#presentation-layer","title":"Presentation layer","text":"<p>The terminologies presentation layer, UI, and user interface are commonly used interchanged. The responsibility of this layer is the presentation of the data to the end-user in a convenient fashion and the triggering of operations on these data.</p> <p>Visualizing the data must consider how the data is \"consumed.\" For example, when listing lots of records, the UI shall provide filtering and grouping too.</p> <p>Sorting and filtering</p> <p>Depending on the chosen technology stack, sorting and filtering may also involve other layers. When dealing with large data sets, it is usually not useful to send every record to the UI to perform filtering and sorting there. It would be an unnecessary network overhead, and some/most UI technologies are not exactly designed to handle large data sets. On the other hand, if the data set is not extensive, it is convenient to let the UI handle these aspects to provide faster and more fluent responses (not having to forward filtering to the database).</p> <p>While displaying the data, the presentation layer is also responsible for performing simple data transformations, such as the user-friendly display of dates. As we discussed previously, a date might be printed as \"15:23\" or as \"3:23 PM,\" or better yet, as \"15 minutes ago.\"</p> <p>Furthermore, the presentation layer also handles localization. Localization is about displaying all pieces of information according to a chosen culture, such as dates, currencies, numbers.</p> <p>And finally, the UI handles user interactions. When a button is clicked, the UI translates this to an operation it will request from the business layer.</p> <p>User input must be validated. Validation covers filling required fields, accepting only valid email addresses, handling expected number ranges, etc.</p> <p>Validation</p> <p>It is not enough to perform validation only in the user interface. Depending on the technology used, the UI can often be easily bypassed, and the services in the background can be called directly. If this happens and only the UI performs validation, invalid data can get into the system. Therefore the validation is repeated by the business layer too. Regardless, the UI should still perform validation to give instant feedback to the user.</p> <p>This layer is not discussed further in this course.</p>"},{"location":"lecture-notes/architecture/#cross-cutting-services-cross-cutting-concerns","title":"Cross-cutting services / Cross-cutting concerns","text":"<p>Cross-cutting services or cross-cutting concerns cover aspects of the application that are not specific to a single layer. When designing our system, we strive to have a unified solution to the problems raised here.</p>"},{"location":"lecture-notes/architecture/#security","title":"Security","text":"<p>Security-related services covert</p> <ul> <li>user authentication,</li> <li>authorization,</li> <li>tracing and auditing.</li> </ul> <p>Authentication answers the question \"who are you\" while authorization determines \"what you are allowed to do in this system.\"</p> <p>Authentication covers not only the authentication on the user interface. We need to authenticate ourselves in the database too, not to mention accessing a third-party external system. Therefore this aspect is present in multiple layers.</p> <p>We have various options. We can use custom authentication, directory-based authentication, or OAuth. After our system has authenticated a user, we can decide to use this identity in external services (e.g. Gmail fetches the user's files from Google Drive ) or use a central identity (e.g. sending an email notification in the name of a send-only central account).</p> <p>Authorization is about access control: whether users can perform specific actions in the system. The UI usually performs some level of authorization (e.g., to hide unavailable functionality), but as discussed with input validation, the business layer must repeat this process. It is crucial, of course, that these two procedures use the same ruleset.</p> <p>Tracing and auditing make sure that we can check who made specific changes in the system. Its main goal is to keep malicious users from erasing their tracks. Recording the steps and operations of a user may be formed in the business login layer as well as in the database.</p>"},{"location":"lecture-notes/architecture/#operation","title":"Operation","text":"<p>Keeping operational aspects in mind helps build maintainable software. The operational aspect usually covers error handling, logging, monitoring, and configuration management.</p> <p>Centralized error management should catch all types of errors that are raised in an application. These errors need to be recorded (e.g., by logging them), and usually, the end-user needs to be notified (e.g., whether she should retry or wait for something else). Recording all exceptions is vital because errors raised in the lower layers of the application are not \"seen\" by anyone (but the end-user probably) unless these are adequately treated and recorded.</p> <p>Logging and monitoring help both diagnostics and seeing whether a system behaves as intended. Logging is usually performed by writing a text log file. Monitoring, on the other hand, records so-called KPIs, key performance indicators. For example, KPIs are the memory usage, the number of errors, the number of pending requests, etc.</p> <p>And finally, configuration management is about the control of the system configuration. Such configuration is, for example, server addresses (e.g., the database IP). Or we may also want to configure and change the background color of our UI centrally. The standard approach regarding configuration is to not hard-code them but instead offer re-configuration without re-compiling the application when the operational circumstances change. This might involve configuration files or more complicated configuration management tools.</p> <p>We will not deal with these operational aspects in more detail. Often the chosen platform offers solutions to these problems.</p>"},{"location":"lecture-notes/architecture/#communication","title":"Communication","text":"<p>By communication, we mean the method and format of data exchange between the layers and the components. Choosing the correct approach depends not only on the architecture but on the deployment model too. If the layers are moved to separate servers, network-based communication is needed, while communication between components on the same server can be achieved with simpler methods.</p> <p>Today, most systems use network communication: most commonly to reach the database and other data sources, and frequently between the presentation layer and the service interfaces. Nowadays, most communication is HTTP-based, however when performance is a concern, TPC-based binary communication methods provide better alternatives. And in more complex systems where the layers themselves are distributed across servers too, messages queues are often used.</p> <p>Encryption is also a factor in communication. Communication over public networks must be encrypted. In case of the communication between the UI and the service interface, this typically means HTTPS/TLS.</p>"},{"location":"lecture-notes/architecture/#backend-and-frontend","title":"Backend and frontend","text":"<p>When we are talking about data-driven systems we often speak about backend and frontend. The frontend is mostly the user interface, that is, the presentation layer (a web application hosted in a browser, a native mobile app, a thick-client desktop app, etc). This is what the user interacts with. The backend is the service that provides the data to the UI: the APIs, the business layer, the data access, and the databases.</p> <p>Depending on the chosen frontend technology, parts of the user interface might be created by the backend, though. This is called server-side rendering.</p> <p>In this course, we will talk about backend technologies.</p>"},{"location":"lecture-notes/architecture/#questions-to-test-your-knowledge","title":"Questions to test your knowledge","text":"<ul> <li>What are the layers in the three-layered architecture? What are their responsibilities?</li> <li>What are the cross-cutting services?</li> <li>Decide, whether the following statements are true or false:<ul> <li>The presentation layer is responsible for validating data input.</li> <li>We shall try to avoid using SQL commands in the business layer.</li> <li>The layers in the three-layered architecture are always hosted on separate servers.</li> <li>The three-layered architecture becomes a multi-layered one when the business layer is moved to its own server.</li> <li>The layered architecture ensures that the implementation of the layers can change without this affecting the other layers.</li> <li>The frontend and the presentation layer are one and the same.</li> <li>Exception handling is important only in the business logic layer.</li> </ul> </li> </ul>"},{"location":"lecture-notes/async/","title":"Asynchronous queries and DTOs (sample WebAPI application)","text":""},{"location":"lecture-notes/async/#asynchronous-queries-and-dtos-sample-webapi-application","title":"Asynchronous queries and DTOs (sample WebAPI application)","text":"<p>This topic discusses server-side asynchronous queries and the use of DTOs (Data Transfer Objects) through an example application. The web application is an ASP.NET Core WebApi server using Entity Framework data access. The functionality discussed here is the management of a webshop cart.</p> <p>Author</p> <p>The original author of this lecture note in Hungarian is M\u00e1t\u00e9 ZERGI.</p>"},{"location":"lecture-notes/async/#asynchronous-execution","title":"Asynchronous execution","text":"<p>Most of our web applications use a database. When communicating with the database, we have to be aware that:</p> <ul> <li>the database might not always be available,</li> <li>the connection might not be stable and fast,</li> <li>and the database might be slow to respond.</li> </ul> <p>Therefore, we need to prepare to wait for the results queried from the database in our application. Using asynchronous techniques in the web application, we can make sure that the resources, such as the web server's threads, are used efficiently even while waiting for the database.</p> <p>Asynchronous execution vs. concurrent execution</p> <p>Asynchronous execution is not the same as concurrent execution. A web server always processes incoming requests concurrently (i.e., multiple requests are in the system at all times). On the other hand, asynchronous execution is about handling a single request efficiently by not blocking any thread for waiting to complete an I/O operation (such as database access, file access, or network communication).</p>"},{"location":"lecture-notes/async/#the-database-of-the-sample-application","title":"The database of the sample application","text":"<p>The sample application presented here uses a simplified database structure as follows.</p> <p></p> <p>For simplicity, the UserId of the carts is not a foreign key to a Users table, but a fixed constant of 1. Obviously, in a real-life example, UserId would be a foreign key.</p> <p>The Products stores the things the webshop sells; the Manufactureres contain the producers of these products; finally, OrderItems stores the content of the cart.</p>"},{"location":"lecture-notes/async/#server-application","title":"Server application","text":"<p>We would like to create a REST-compatible service for managing the webshop cart using ASP.NET Core WebApi and Entity Framework. Let us follow these steps:</p> <ol> <li>Create the C# model of the database tables,</li> <li>Create the database context,</li> <li>Create Data Transfer Objects that represent the information queried by the client in a format convenient for the client,</li> <li>Create WebApi controllers</li> </ol> <p>We will go through each of these steps.</p>"},{"location":"lecture-notes/async/#create-the-c-model-of-the-database-tables","title":"Create the C# model of the database tables","text":"<p>The C# classes that map the database tables are usually placed into a folder often called Models in ASP.NET Core.</p> <p>The following is the class for the Products table.</p> <pre><code>namespace WebshopApi.Models\n{\n    public class Product\n    {\n        public string Name { get; set; }\n        public int ManufacturerID { get; set; }\n        public int Price { get; set; }\n        public int ID { get; set; }\n    }\n}\n</code></pre> <p>The following is the class for the Manufacturers table.</p> <pre><code>namespace WebshopApi.Models\n{\n    public class Manufacturer\n    {\n        public string Name { get; set; }\n        public int ID { get; set; }\n    }\n}\n</code></pre> <p>The following is the class for the OrderItems table.</p> <pre><code>namespace WebshopApi.Models\n{\n    public class OrderItem\n    {\n        public int ID { get; set; }\n        public int ProductID { get; set; }\n        public int CartID { get; set; }\n        public int Pieces { get; set; }\n    }\n}\n</code></pre> <p>The following is the class for the Carts table.</p> <pre><code>namespace WebshopApi.Models\n{\n    public class Cart\n    {\n        public int ID { get; set; }\n        public int UserID { get; set; }\n    }\n}\n</code></pre> <p>Note, that the sole purpose of these classes is to map the data exactly as in the database.</p>"},{"location":"lecture-notes/async/#create-the-database-context","title":"Create the database context","text":"<p>After mapping the tables, we can now create the class that will represent our database: the DbContext class. This class must inherit from the Entity Framework Core DbContext class.</p> <pre><code>namespace WebshopApi.Models\n{\n    public class WebshopContext : DbContext\n    {\n        public WebshopContext(DbContextOptions&lt;WebshopContext&gt; options)\n            : base(options)\n        {\n        }\n\n        public DbSet&lt;Product&gt; Products { get; set; }\n        public DbSet&lt;Manufacturer&gt; Manufacturers { get; set; }\n        public DbSet&lt;Cart&gt; Carts { get; set; }\n        public DbSet&lt;OrderItem&gt; OrderItems { get; set; }\n    }\n}\n</code></pre> <p>Each table in the database corresponds to a DbSet property as defined above. Each DbSet specified the type of the entity it stores; e.g., <code>DbSet&lt;Products&gt;</code> will store entities of type <code>Product</code>.</p> <p>The <code>DbContextOptions</code> configures the access to the database, such as the connection string. This is usually configured in the <code>Startup</code> class:</p> <pre><code>public class Startup\n{\n    // ...\n\n    // This method is called by the runtime to populate the services of the DI container\n    public void ConfigureServices(IServiceCollection services)\n    {\n        services.AddDbContext&lt;WebshopContext&gt;(opt =&gt; \n                opt.UseSqlServer(@\"Data Source=(localdb)\\mssqllocaldb;Initial Catalog=Webshop;Integrated Security=True\"));\n        // ...\n    }\n}\n</code></pre>"},{"location":"lecture-notes/async/#defining-data-transfer-objects","title":"Defining Data Transfer Objects","text":"<p>We have the direct mapping of the database into C# classes. Let us now consider how does the cart of the webshop usually look like: it may contain multiple items. While the <code>OrderItem</code> class can represent a single item, our cart is a list of items. This list of items is what we shall describe with a so-called Data Transfer Object: it is a class that gathers data for the client.</p> <p>Definition: Data Transfer Object</p> <p>A container object that transfers data between application (here: between the client and the server).</p> <p>With the use of DTOs, we can pack all necessary information into one object, making it not only more convenient for the client, but also better in terms of performance:</p> <ul> <li>We only send information to the client that it really needs.</li> <li>Furthermore, a DTO can gather various information and send them all in one go.</li> </ul> <p>Let us consider, what information do we need to display the cart in the client: the products, the amount in the cart for each, and the total number of items.</p> <ol> <li> <p>Class <code>OrderItem</code> has superfluous data that the client does not need: <code>CartID</code> and <code>ID</code>. Removing these properties we can arrive at a class very similar to <code>OrderItem</code>; but it is still just one item of the cart. The class we can create this way is called <code>CartItem</code>.</p> </li> <li> <p>This <code>CartItem</code> has a<code>Product</code> that also stores some unnecessary data, and some properties that might need adding. For example, the manufacturer of the <code>Product</code> should contain the name of the manufacturer and not the <code>ManufacturerID</code>. Let us, therefore, create a new <code>Product</code> class, and our <code>CartItem</code> should store this class instead.</p> </li> <li> <p>These <code>CartItem</code> objects are gathered in a list, and let us add the total number of items in the cart. This will give us our last DTO, the <code>UserCart</code>. An instance of this <code>UserCart</code> is what that the client will receive.</p> </li> </ol> <p>The DTO classes are usually separated from the database entities. Let us put these classes in a DTOs folder.</p> <p><code>CartItem</code> class contains the data from an <code>OrderItem</code> without the unnecessary properties.</p> <pre><code>namespace WebshopApi.DTOs\n{\n    public class CartItem\n    {\n        public Product Product { get; set; } // This is the product that no longer has the ID of the manufacturer, but the name instead\n        public int Amount { get; set; } // The amount in the cart\n    }\n}\n</code></pre> <p>The the matching Product DTO:</p> <pre><code>namespace WebshopApi.DTOs\n{\n    public class Product\n    {\n        public string ProductName { get; set; } // The product name, e.g., AB123 Full HD TV\n        public string Manufacturer { get; set; } // A !!name!! of the manufacturer, e.g., BMETV\n        public int Price { get; set; } // Price of the product\n        public int ID { get; set; } // ID of the product\n    }\n}\n</code></pre> <p>Why is there an ID here?</p> <p>We might be curious why there is an <code>ID</code> here. An item in the cart is identified by the product itself. E.g., further details of the product in the cart can be queried by knowing this ID. We could create a new identifier for the item in the cart; but the product's ID is sufficient.</p> <p><code>UserCart</code> collects all items, and adds a total number.</p> <pre><code>namespace WebshopApi.DTOs\n{\n    public class UserCart\n    {\n        public List &lt;CartItem&gt; CartPieces { get; set; }\n        public int NumberOfItems { get; set; }\n    }\n}\n</code></pre> <p>By storing the <code>CartItem</code>s in a list, we make the job of the client easier. When rendering the contents of the cart the client code only needs to iterate through the array contents.</p> <p>This <code>UserCart</code> is created by gathering the required information, such as the product details, then adding up the number of items.</p>"},{"location":"lecture-notes/async/#creating-the-webapi-controller","title":"Creating the WebApi controller","text":"<p>The controllers are usually placed into the Controllers folder. Here, we have a single controller that uses the <code>WebshopDbContext</code> directly and handles the HTTP queries.</p> <p>This is where we can introduce asynchronous queries. Let us see an example right away:</p> <p>The following is a GET query to fetch all carts.</p> <pre><code>[HttpGet]\npublic async Task&lt;ActionResult&lt;IEnumerable&lt;Cart&gt;&gt;&gt; GetCarts()\n{\n    var carts = await _context.Carts.ToListAsync();\n    return carts;\n}\n</code></pre> <p>Let us note the <code>async</code> keyword in the declaration and the <code>Task</code> return type, along with the <code>await</code> instruction in the body. Together, these are called async-await. Let us make sense of all these:</p> <ol> <li>The method returns a list of <code>Cart</code> instances, that is, <code>IEnumerable&lt;Cart&gt;</code>;</li> <li>Which, according to WebApi controller conventions, is wrapped in an <code>ActionResult</code>;</li> <li>And this whole thing is wrapped in a <code>Task</code>. This one is due to the asynchronous behavior.</li> </ol> <p>Although this seems complicated, every part of this is for a different reason. Let us examine the asynchronous behavior: the <code>Task</code> type, and the <code>await</code> keyword. This definition of the method yields a so-called promise (some languages use this terminology) that represents the result of a task that will be completed in the future.</p> <p>Why do we need this? Because this makes the execution of the controller method asynchronous. When the execution arrives at an <code>await</code> keyword, the thread that processes the request, will stop further processing of this query and will start processing a new query instead. Ok, but again, why? Because we know that the operation \"behind\" the <code>await</code> will take time: it has to go to the database and fetch data from there. If the thread stopped here to wait for the result, it would be wasting resources. Instead of having the thread wait for the result, the task is handed off to a system in the background (the operating system and the .NET asynchronous I/O subsystem - will not go into details here), and we request notification when the results are available. Once this happens (the results from the database are, in fact, ready), the processing of the query will continue.</p> <p>In other words, the threads used by our application will always do useful work instead of waiting (or being suspended due to waiting). Consequently, this means that serving the HTTP requests need fewer operating system threads, therefore making better use of available computational resources.</p> <p>The previous method can be simplified in syntax by getting rid of the local variable and returning the <code>Task</code> directly. Functionally, this implementation works (almost) identically, but the one above makes the explanation easier.</p> <pre><code>[HttpGet]\npublic Task&lt;ActionResult&lt;IEnumerable&lt;Carts&gt;&gt;&gt; GetCarts()\n{\n    return _context.Carts.ToListAsync(); // no await and the method declaration has no async\n}\n</code></pre> <p>The ***Async methods</p> <p>The methods to fetch data from the database (e.g., <code>ToList</code>, <code>First</code>, <code>All</code>, <code>Find</code>, etc.) all have their <code>...Async</code> pairs. These methods provide the basis for asynchronous execution.</p> <p>We will not discuss the execution in more details. What we need to remember, is that in order for our controller method to be asynchronous, there must be an asynchronous operation \"underneath\" (here: in Entity Framework).</p> <p>Let us also see a complex example: gather all data of the cart:</p> <pre><code>[HttpGet(\"{id}\")]\npublic async Task&lt;ActionResult&lt;UserCart&gt;&gt; GetCart(int id)\n{\n    // asynchronous query to find the cart\n    var cartRecord = await _context.Carts.FindAsync(id); \n\n    if (cartRecord == null)\n        return NotFound();\n\n    // build the query\n    var productsquery =\n        from p1 in _context.Products\n        join m1 in _context.Manufacturers on p1.ManufacturerID equals m1.ID\n        select new Product(p1.ID, m1.Name, p1.Name, p1.Price); // create the Product DTO\n    // asynchronous evaluation\n    var products = await productsquery.ToListAsync().ConfigureAwait(false);\n\n    // asynchronous request to get order items\n    var orderitemsquery = from oi in _context.OrderItems\n                          where oi.CartID == cartRecord.ID\n                          select oi;\n    var orderitems = await orderitemsquery.ToListAsync().ConfigureAwait(false);\n\n    // further operations are synchronous, as every result in in memory already\n\n    // Find the products in the cart \n    // match them to the order items and crate a CartItem DTO\n    var cartitems = products.Join(orderitems, p =&gt; p.ID, oi =&gt; oi.ProductID,\n                                  (p, v) =&gt; new CartItem(p, v.Pieces)).ToList();\n\n    // Finally, the result is a UserCart DTO\n    return new UserCart()\n    {\n        CartPieces = cartitems,\n        NumberOfItems = cartitems.Count()\n    }\n}\n</code></pre> <p>Note, how all asynchronous method calls are <code>await</code>-ed! But once we have all the data from the database, we can continue in a synchronous fashion.</p> <p>The <code>ConfigureAwait</code> method</p> <p>The <code>ConfigureAwait(false)</code> gives us further options regarding performance optimization. With this option we signal that the <code>await</code>-ed result set can be processed by any available thread, not just the one that started the processing originally. In server-side applications this is usually the correct behavior, however, this is not true for all asynchronous operations (e.g., UI threads are usually special and in that case not any thread can continue). For more details, see: https://devblogs.microsoft.com/dotnet/configureawait-faq/.</p> <p>Finally, let us see an example using <code>FirstOrDefaultAsync</code> to process a POST query that alters the contents of the cart (adds or removed items):</p> <pre><code>// DTO describing the inputs of the operation\nnamespace WebshopApi.Models\n{\n    public class PostCartItemArgs\n    {\n        public int CartId { get; set; }\n        public int ProductId { get; set; }\n        public int Amount { get; set; }\n    }\n}\n\n//////////////////////////////////////////////////////////////////////////////\n\n// The HTTP handler in the controller\n[HttpPost]\npublic async Task&lt;IActionResult&gt; PostCartItem([FromBody] PostCartItemArgs data)\n{\n    // find the cart by ID\n    var cart = await _context.Carts.FindAsync(data.CartId).ConfigureAwait(false);\n\n    if (cart == null)\n        return NotFound();\n\n    // find the order items in this cart matching the provided product\n    var orderitemquery = from oi in _context.OrderItems\n                         where (oi.CartID == data.Id &amp;&amp; oi.ProductID == data.ProductId)\n                         select oi;\n\n    // FirstOrDefault so that if there is no match, the result is null\n    var orderitem = await orderitemquery.FirstOrDefaultAsync().ConfigureAwait(false);\n\n    if (orderitem == null)\n    {\n        // If there was no such item in the cart, add a new OrderItem\n        _context.OrderItems.Add(new OrderItems { CartID = data.Id, Amount = data.Amount, ProductID = data.ProductId });\n    }\n    else\n    {\n        // If there is an item in the cart\n        orderitem.Amount += data.Amount;\n\n        // If the amount is zero, it means, removed from the cart\n        if (orderitem.Amount == 0)\n            _context.OrderItems.Remove(orderitem);\n    }\n\n    await _context.SaveChangesAsync(); // await here too, since this will need to go to the database\n    return NoContent();\n}\n</code></pre> <p>Who waits for the <code>Task</code> result of the controller?</p> <p>Every <code>async</code> method has to be <code>await</code>-ed somewhere. When it comes to a WebApi controller, it will be the ASP.NET Core framework that invokes this method, and it will \"wait\" for the result before serializing it to JSON to send to the client.</p>"},{"location":"lecture-notes/async/#sample-code","title":"Sample code","text":"<p>The source of the sample application is available here: https://github.com/mzergi/WebshopApi/</p>"},{"location":"lecture-notes/di/","title":"Dependency Injection ASP.NET Core environment","text":""},{"location":"lecture-notes/di/#dependency-injection-aspnet-core-environment","title":"Dependency Injection ASP.NET Core environment","text":"<p>Zolt\u00e1n Benedek, 11.19.2022</p> <p>Definition</p> <p>Dependency Injection (DI) is programming technique that makes a class independent of its dependencies. It's a key enabler for decomposing an application into loosely coupled components. More precisely: Dependency Injection is a mechanism to decouple the creation of dependency graphs for a class from its class definition.</p> <p>Of course, the above definition is very abstract, and based on the short definition it's hard to understand what problems DI is trying to solve, and how DI is trying to solve them.</p> <p>In the following chapters, we will use an example to put DI into context and to learn the basics of the DI related services build into ASP.NET Core.</p> <p>Goals of DI</p> <ul> <li>Facilitated extensibility and maintainability</li> <li>Improved unit testability</li> <li>Facilitated code reuse</li> </ul> <p>Sample application</p> <p>The sample C# code is available here: https://github.com/bmeviauac01/todoapi-di-sample</p>"},{"location":"lecture-notes/di/#example-phase-1-service-class-with-wired-in-dependencies","title":"Example phase 1 - service class with wired in dependencies","text":"<p>In this example, based on code snippets we look at parts of a to-do list (TODO) application that sends to-do item related email notifications. Note: The code is minimalistic for succinctness.</p> <p>The \"entry point\" of our example is the <code>SendReminderIfNeeded</code> operation of the <code>ToDoService</code> class.</p> <pre><code>// Class for managing todo items\npublic class ToDoService\n{\n    const string smtpAddress = \"smtp.myserver.com\";\n\n    // It checks the todoItem object received as a parameter and sends an e-mail\n    // notification about the to-do item to the contact person specified by the\n    // todo item.\n    public void SendReminderIfNeeded(TodoItem todoItem)\n    {\n        if (checkIfTodoReminderIsToBeSent(todoItem))\n        {\n            NotificationService notificationService = new NotificationService(smtpAddress);\n            notificationService.SendEmailReminder(todoItem.LinkedContactId, todoItem.Name);\n        }\n    }\n\n    bool checkIfTodoReminderIsToBeSent(TodoItem todoItem)\n    {\n        bool send = true;\n        /* ... */\n        return send;\n    }\n    // ...\n}\n\n// Entity class, encapsulates information about a todo task\npublic class TodoItem\n{\n    // Database key\n    public long Id { get; set; }\n    // Name/description of the task\n    public string Name { get; set; }\n    // Indicates if the task has been completed\n    public bool IsComplete { get; set; }\n    // It's possible to assign a contact person to a task: -1 indicated no contact\n    // person is assigned, otherwise the id of the contact person\n    public int LinkedContactId { get; set; } = -1;\n}\n</code></pre> <p>In the code above (<code>ToDoService.SendReminderIfNeeded</code>) we see that the essential logic of sending an e-mail is to be found in the <code>NotificationService</code> class. Indeed, this class is at the center of our investigation. The following code snippet describes the code for the <code>NotificationService</code> class and its dependencies:</p> <pre><code>// Class for sending notifications\nclass NotificationService\n{\n    // Dependencies of the class\n    EMailSender _emailSender;\n    Logger _logger;\n    ContactRepository _contactRepository;\n\n    public NotificationService(string smtpAddress)\n    {\n        _logger = new Logger();\n        _emailSender = new EMailSender(_logger, smtpAddress);\n        _contactRepository = new ContactRepository();\n    }\n\n    // Sends an email notification to the contact with the given ID\n    // (contactId is a key in the Contacts table)\n    public void SendEmailReminder(int contactId, string todoMessage)\n    {\n        string emailTo = _contactRepository.GetContactEMailAddress(contactId);\n        string emailSubject = \"TODO reminder\";\n        string emailMessage = \"Reminder about the following todo item: \" + todoMessage;\n        _emailSender.SendMail(emailTo, emailSubject, emailMessage);\n    }\n}\n\n// Class supporting loggin\npublic class Logger\n{\n    public void LogInformation(string text) { /* ...*/ }\n    public void LogError(string text) { /* ...*/ }\n}\n\n// Class for sending e-mail notifications\npublic class EMailSender\n{\n    Logger _logger;\n    string _smtpAddress;\n\n    public EMailSender(Logger logger, string smtpAddress)\n    {\n        _logger = logger;\n        _smtpAddress = smtpAddress;\n    }\n    public void SendMail(string to, string subject, string message)\n    {\n        _logger.LogInformation($\"Sendding e-mail. To: {to} Subject: {subject} Body: {message}\");\n\n        // ...\n    }\n}\n\n// Class for Contact entity persistence\npublic class ContactRepository\n{\n    public string GetContactEMailAddress(int contactId)\n    {\n        // ...\n    }\n    // ...\n}\n</code></pre> <p>A few general thoughts:</p> <ul> <li>The <code>NotificationService</code> class has several dependencies (<code>EMailSender</code>, <code>Logger</code>, <code>ContactRepository</code> classes) and it implements its services based on these dependency classes.</li> <li>Dependency classes may have additional dependencies: <code>EMailSender</code> is a great example of this, it's dependent on the <code>Logger</code> class.</li> <li>Note: <code>NotificationService</code>, <code>EMailSender</code>, <code>Logger</code>, <code>ContactRepository</code> classes are considered service classes because they contain business logic, not just encapsulate data, such as <code>TodoItem</code>.</li> </ul> <p>As we could see the <code>SendEmailReminder</code> operation is actually served by an object graph, where <code>NotificationService</code> is the root object, it has three dependencies, and its dependencies have further dependencies. The following figure illustrates this object graph:</p> <p></p> <p>Note</p> <p>One may ask why we considered <code>NotificationService</code>, and not <code>ToDoService</code> as the root object. Actually it just depends on our viewpoint: for simplicity we considered ToDoService as an entry point (a \"client\") for fulfilling a request, so that we have less classes to put under scrutiny. In a real life application we probably would consider <code>ToDoService</code> as part of the dependency graph as well.</p> <p>Let's review the key features of this solution:</p> <ul> <li>The class instantiates its dependencies itself</li> <li>Class depends on the specific type of its dependencies (and not on interfaces, \"abstractions\")</li> </ul> <p>This approach has a couple of significant and rather painful drawbacks:</p> <ol> <li>Rigidity, lack of extensibility. <code>NotificationService</code> (without modification) cannot work with other mailing, logging and contact repository implementations (but only with the the wired in <code>EMailSender</code>, <code>Logger</code> and <code>ContactRepository</code> classes). That is, e.g. we can't use it with any other logging component, or e.g. use it with a contact repository that operates via a different data source/storage mechanism.</li> <li>Lack of unit testability. The <code>NotificationService</code> (without modification) cannot be unit tested. This would require replacing the <code>EMailSender</code>, <code>Logger</code> and <code>ContactRepository</code> dependencies with variants that provide fixed/expected responses  for a given input. Keep in mind that unit testing is about testing the behavior of a class independently from its dependencies. In our example, instead of using the database base ContactRepository, we would need a ContactRepository implementation that could serve requests very quickly from memory with values supporting the specific test cases.</li> <li>There is one more subtle inconvenience that is hard to notice at first sight. In our example we had to provide the <code>smtpAddress</code> parameter to the <code>NotificationService</code> constructor, so that it can forward it to its <code>EMailSender</code> dependency. However, <code>smtpAddress</code> is a parameter completely meaningless for <code>NotificationService</code>, it has nothing to do with this piece of information. Unfortunately, we are forced to pass <code>smtpAddress</code> thorough <code>NotificationService</code>, as <code>NotificationService</code> is the class instantiating the <code>EMailSender</code> object. We could eliminate this by somehow instantiating <code>EMailSender</code> independently of <code>NotificationService</code>.</li> </ol> <p>In the next steps, we redesign our solution so that we can eliminate most of the downsides of the current rigid approach.</p>"},{"location":"lecture-notes/di/#example-phase-2-service-class-with-manual-dependency-injection","title":"Example phase 2 - service class with manual dependency injection","text":"<p>Redesign our former solution the functional requirements are unchanged. The most important principles of transformation are the following:</p> <ul> <li>Dependencies will be based on abstractions/interfaces</li> <li>Classes will no longer instantiate their dependencies themselves</li> </ul> <p>Let's jump right into the code of the improved solution and then analyze the differences:</p> <pre><code>public class ToDoService\n{\n    const string smtpAddress = \"smtp.myserver.com\";\n\n    // Checks the todoItem object received as a parameter and sends an e-mail\n    // notification about the to-do item to the contact person specified by the\n    // todo item.\n    public void SendReminderIfNeeded(TodoItem todoItem)\n    {\n        if (checkIfTodoReminderIsToBeSent(todoItem))\n        {\n            var logger = new Logger();\n            var emailSender = new EMailSender(logger, smtpAddress);\n            var contactRepository = new ContactRepository();\n\n            NotificationService notificationService\n                = new NotificationService(logger, emailSender, contactRepository);\n            notificationService.SendEmailReminder(todoItem.LinkedContactId,\n                todoItem.Name);\n        }\n    }\n\n    bool checkIfTodoReminderIsToBeSent(TodoItem todoItem)\n    {\n        bool send = true;\n        /* ... */\n        return send;\n    }\n}\n\n// Class for sending notifications\nclass NotificationService\n{\n    // Dependencies of the class\n    IEMailSender _emailSender;\n    ILogger _logger;\n    IContactRepository _contactRepository;\n\n    public NotificationService(ILogger logger, IEMailSender emailSender,\n        IContactRepository contactRepository)\n    {\n        _logger = logger;\n        _emailSender = emailSender;\n        _contactRepository = contactRepository;\n    }\n\n    // Sends an email notification to the contact with the given ID\n    // (contactId is a key in the Contacts table)\n    public void SendEmailReminder(int contactId, string todoMessage)\n    {\n        string emailTo = _contactRepository.GetContactEMailAddress(contactId);\n        string emailSubject = \"TODO reminder\";\n        string emailMessage = \"Reminder about the following todo item: \" + todoMessage;\n        _emailSender.SendMail(emailTo, emailSubject, emailMessage);\n    }\n}\n\n#region Contracts (abstractions)\n\n// Interface for logging\npublic interface ILogger\n{\n    void LogInformation(string text);\n    void LogError(string text);\n}\n\n// Interface for sending e-mail\npublic interface IEMailSender\n{\n    void SendMail(string to, string subject, string message);\n}\n\n// Interface for Contact entity persistence\npublic interface IContactRepository\n{\n    string GetContactEMailAddress(int contactId);\n}\n\n#endregion\n\n#region Implementations\n\n// Class for logging\npublic class Logger: ILogger\n{\n    public void LogInformation(string text) { /* ...*/  }\n    public void LogError(string text) {  /* ...*/  }\n}\n\n// Class for sending e-mail\npublic class EMailSender: IEMailSender\n{\n    ILogger _logger;\n    string _smtpAddress;\n\n    public EMailSender(ILogger logger, string smtpAddress)\n    {\n        _logger = logger;\n        _smtpAddress = smtpAddress;\n    }\n    public void SendMail(string to, string subject, string message)\n    {\n        _logger.LogInformation($\"Sendding e-mail. To: {to} Subject: {subject} Body: {message}\");\n\n        // ...\n    }\n}\n\n// Class for Contact entity persistence\npublic class ContactRepository: IContactRepository\n{\n    public string GetContactEMailAddress(int contactId)\n    {\n        // ...\n    }\n    // ...\n}\n\n#endregion\n</code></pre> <p>We improved out previous solution in the following points:</p> <ul> <li>The <code>NotificationService</code> class no longer instantiates its dependencies itself, but receives them in constructor parameters.</li> <li>Interfaces (abstractions) have been introduced to manage dependencies</li> <li>The <code>NotificationService</code> class gets its dependencies in the form of interfaces. When a class receives its dependencies externally (e.g. via constructor parameters), it is called DEPENDENCY INJECTION (DI).</li> <li>In our case, the classes get their class dependencies in constructor parameters: this specific form of DI is called CONSTRUCTOR INJECTION. This is the most common - and most recommended - way to inject dependency. (Alternatively, for example, we could use property injection, which is based on a public property setter to set a specific dependency of a class).</li> </ul> <p>In our current solution, <code>NotificationService</code> dependencies are instantiated by the (direct) USER of the class (which is the <code>ToDoService</code> class). Primarily this is the reason why we are still facing with a few problems:</p> <ol> <li>The user of <code>NotificationService</code> objects, which is the <code>ToDoService</code> class, is still dependent on the implementation types (since it has to instantiate the <code>Logger</code>,<code>EMailSender</code> and <code>ContactRepository</code> classes).</li> <li>If we use the <code>Logger</code>, <code>EMailSender</code> and <code>ContactRepository</code> classes at multiple places in your application, we must instantiate them explicitly. In other words: at each and every place where have to create an <code>ILogger</code>, <code>IEMailSender</code> or <code>IContactRepository</code> implementation class, we have to make a decision which implementation to choose. This is essentially a special case of code duplication, the decision should appear only once in our code.<ul> <li>Our goal, in contrast, would be to determine at a single central location what type implementation to use for an abstraction (interface type) everywhere in the application (e.g. for <code>ILogger</code> create an <code>Logger</code> instance everywhere, for <code>IMailSender</code> create an <code>EMailSender</code> everywhere).</li> <li>This would allow us to easily review our abstraction-to-implementation mappings at one place.</li> <li>Moreover, if we want to change one of the mappings (e.g. using <code>AdvancedLogger</code> instead of <code>Logger</code> for <code>ILogger</code>) we could achieve that by making a single change at a central location.</li> </ul> </li> </ol>"},{"location":"lecture-notes/di/#example-phase-3-dependency-injection-based-on-net-dependency-injection","title":"Example phase 3 - dependency injection based on .NET Dependency Injection","text":"<p>We need some extra help from our framework to solve the two problems we concluded the previous chapter with: an Inversion of Control (IoC) container (also called as Dependency Injection container). Dependency Injection container is a widely used alternative name for the same tool/technique. In an IoC container we can store abstraction type -&gt; implementation type mappings, such as ILogger-&gt;Logger, IMailSender-&gt;EMailSender, etc. This is called the REGISTER step. And then based on these mappings create an implementation type for a specific abstraction type (e.g. <code>Logger</code> for an <code>ILogger</code>). This is called the RESOLVE step. In more detail:</p> <ol> <li>REGISTER: Register dependency mappings (e.g. <code>ILogger</code>-&gt; <code>Logger</code>, <code>IMailSender</code>-&gt; <code>EMailSender</code>) into an IoC container, once, at a centralized location, at application startup. This is the REGISTER step of the DI process.</li> <li>Note: This solves \"problem 2\" pointed out at the end of the previous chapter: the mappings are centralized, and not scattered all over the application code base.</li> <li>RESOLVE: When we need an implementation object at runtime in our application, we ask the container for an implementation by specifying the abstraction (interface) type (e.g., by providing <code>ILogger</code> as a key, the container returns an object of class <code>Logger</code>).<ul> <li>The resolve step is typically done at the \"entry point\" of the application (e.g. in case of WebApi on the receival of web requests, we will look into this later). The resolve step is performed only for the ROOT OBJECT (e.g. for the appropriate Controller class in case of WebApi). The container creates and returns a root object and all its dependencies and all its indirect dependencies: an entire object graph is generated. This process is called AUTOWIRING.</li> <li>Note: In case of Web API calls, the Resolve step is executed by the Asp.Net framework and is mostly hidden from the developer: all we see is that our controller class is automatically instantiated and all constructor parameters are automatically populated (with the help of the IoC container based on the mappings of the REGISTER step).</li> </ul> </li> </ol> <p>Fortunately, .NET has a built in IoC container based dependency injection service. Now we elucidate and illustrate the complete mechanism (register and resolve steps) using our enhanced e-mail notification solution as an example.</p>"},{"location":"lecture-notes/di/#1-register-step-registering-dependencies","title":"1) REGISTER step (registering dependencies)","text":"<p>In an Asp.Net Core environment, dependencies are registered in the 'Program.cs' file. This file has code that is executed at application startup. The code parts located here and which are relevant for us:</p> <pre><code>var builder = WebApplication.CreateBuilder(args);\n\n// ...\nbuilder.Services.AddSingleton&lt;ILogger, Logger&gt;();\nbuilder.Services.AddTransient&lt;INotificationService, NotificationService&gt;();\nbuilder.Services.AddScoped&lt;IContactRepository, ContactRepository&gt;();\nbuilder.Services.AddSingleton&lt;IEMailSender, EMailSender&gt;(\n    sp =&gt; new EMailSender(sp.GetRequiredService&lt;ILogger&gt;(), \"smtp.myserver.com\") );\n// ...\n</code></pre> <p>The first line creates a <code>builder</code> object, whose <code>Services</code> property is an object implementing the <code>IServiceCollection</code>  interface. This represents the IoC container created by the framework, this can be used to register our dependency mappings as well, namely the  AddSingleton, AddTransient and AddScoped operations of <code>IServiceCollection</code> interface can be used to register them.</p> <p>Note</p> <p>In .NET versions prior to .NET 6 the instead of <code>Program.cs</code> the <code>ConfigureServices</code> operation of the <code>Startup</code> class was used to register these dependencies.</p> <p>The</p> <pre><code>builder.Services.AddSingleton&lt;ILogger, Logger&gt;();\n</code></pre> <p>line registers an <code>ILogger</code>-&gt; <code>Logger</code> type mapping, and the <code>Logger</code> is registered as a singleton, as we used the AddSingleton operation for registration. This means that if we later ask the container for an <code>ILogger</code> object (provide <code>ILogger</code> as key at the resolve step), we will get a <code>Logger</code> object from the container, and always the same instance. The</p> <pre><code>builder.Services.AddTransient&lt;INotificationService, NotificationService&gt;();\n</code></pre> <p>line registers an <code>INotificationService</code>-&gt; <code>NotificationService</code> transient type mapping, as we used the AddTransient operation for registration. This means that if we later ask the container for an <code>INotificationService</code> object (provide <code>INotificationService</code> as key at the resolve step), we will get a separate newly created instance of <code>NotificationService</code> object from the container, for each query/resolve.</p> <pre><code>builder.Services.AddScoped&lt;IContactRepository, ContactRepository&gt;();\n</code></pre> <p>line registers an <code>IContactRepository</code>-&gt; <code>ContactRepository</code> scoped type mapping, as we used the AddScoped operation for registration. This means that if we later ask the container for an <code>IContactRepository</code> object (provide <code>IContactRepository</code> as key at the resolve step), we will get a <code>NotificationService</code> object, which will be the  same instance for the same scope, and a different instance for different scopes. For a Web API based application one web request is handled within one scope. Consequently, we receive the same instance of a class turning to the container multiple times within the same web request, but different ones when the web requests are different.</p> <p>We can see additional registrations in the sample application, which we will return to later.</p>"},{"location":"lecture-notes/di/#2-resolve-step-resolving-dependencies","title":"2) RESOLVE step (resolving dependencies)","text":""},{"location":"lecture-notes/di/#the-basics","title":"The basics","text":"<p>Let's sum up where we are now: we have our abstraction to implementation type mappings registered into the ASP.NET Core IoC container at application startup. Our mappings are the following:</p> <ul> <li>ILogger -&gt; Logger as singleton</li> <li>INotificationService -&gt; NotificationService as transient</li> <li>IContactRepository -&gt; ContactRepository as scoped</li> <li>IEMailSender -&gt; EMailSender as singleton</li> </ul> <p>From now on, whenever we need an instance of an implementation type for an abstraction, we can ask the container for it using the abstraction type as the key. How do we specifically do it in a .NET Core application? .NET Core provides an <code>IServiceProvider</code> reference to us, and we can use different forms of the <code>GetService</code> operation of this interface. E.g.:</p> <pre><code>void SimpleResolve(IServiceProvider sp)\n{\n    // Returns an instance of the Logger class, as we have\n    // registered the Logger implementation type for our ILogger abstraction.\n    var logger1 = sp.GetService(typeof(ILogger));\n\n    // Same as the previous example. The difference is that we have provided\n    // the type as a generic parameter. This is a more convenient approach.\n    // To use this we have to import the Microsoft.Extensions.DependencyInjection\n    // namespace via the using statement.\n    // Returns an instance of the Logger class, see explanation above.\n    var logger2 = sp.GetService&lt;ILogger&gt;();\n\n    // GetService returns null if no type mapping is found for the specific type (ILogger)\n    // GetRequiredService throws an exception instead.\n    var logger3 = sp.GetRequiredService&lt;ILogger&gt;();\n    // ...\n}\n</code></pre> <p>In the above example, code comments explain the behavior in detail. In each case, an abstraction type is to be provided for the <code>GetService</code>/<code>GetRequiredService</code> operation (either via the <code>typeof</code> operator, or via a generic parameter), and the operation returns with an instance of an implementation type based on the type mappings registered in the container.</p>"},{"location":"lecture-notes/di/#object-graph-resolution-autowiring","title":"Object graph resolution, autowiring","text":"<p>In the previous example, the container was able to instantiate the <code>Logger</code> class at the resolve step without any major 'headaches', since it has no additional dependencies: it has a single default constructor. Now consider the resolution of <code>INotificationService</code>:</p> <pre><code>public void ObjectGraphResolve(IServiceProvider sp)\n{\n    var notifService = sp.GetService&lt;INotificationService&gt;();\n    // ...\n}\n</code></pre> <p>At the resolve step (GetService call), the container must create a <code>NotificationService</code> object. In doing so, it has to provide valid values for its constructor parameters, which actually means that has to resolve the class's direct and indirect dependencies, recursively:</p> <ul> <li>The NotificationService class has a three-parameter constructor (that is, it has three dependencies): <code>NotificationService (ILogger logger, IEMailSender emailSender, IContactRepository contactRepository)</code>. The <code>GetService</code> resolves constructor parameters one by one based on IoC container mapping registrations:<ul> <li><code>ILogger</code> logger: a <code>Logger</code> object is provided by the container, always the same instance (as ILogger-&gt;Logger mapping is registered as singleton)</li> <li><code>IEMailSender</code> emailSender: an <code>EMailSender</code> object is provided by the container, a different instance in each case (as mapping is registered as transient)<ul> <li>The <code>EMailSender</code> constructor has an <code>ILogger logger</code> parameter, that has to be resolved as well: a <code>Logger</code> object is provided by the container, always the same instance (as registered as singleton)</li> </ul> </li> <li><code>IContactRepository</code> contactRepository: a <code>ContactRepository</code> object is provided by the container, a different instance for different scopes (Web API e.g. for different Web API calls), as mapping is registered as scoped.</li> </ul> </li> </ul> <p>Summing up: the <code>GetService&lt;INotificationService&gt;()</code> call above creates a fully parameterized <code>NotificationService</code> object with all of its direct and indirect dependencies, the call returns an object graph for us:</p> <p></p> <p>As we have seen in this example, IoC containers/DI frameworks are capable of determining the dependency requirements of objects (by examining at their constructor parameters), and then creating entire object graphs based on upfront abstraction-&gt;implementation container type mappings. This process is called autowiring.</p>"},{"location":"lecture-notes/di/#dependency-resolution-for-aspnet-web-api-classes","title":"Dependency resolution  for ASP.NET Web API classes","text":"<p>Besides making our solution IoC container based, we make a few further changes to our todo app. We eliminate our <code>ToDoService</code> class, and move its functionality in a slightly different form into  an Asp.Net Core based <code>ControllerBase</code> derived class. This controller class will serve as our entry point and also as a root object, bringing our solution very close to a real life example (let it be a Web API, Web MVC app or a Web Razor Pages app). We could also have kept <code>ToDoService</code> in the middle of our call/dependency chain, but we try to keep things as simple as possible for our demonstration purposes. Furthermore, we also introduce an Entity Framework <code>DbContext</code> derived class called <code>TodoContext</code> to be able to demonstrate how it can be injected into repository classes in a typical application. Our new object graph will look like this:</p> <p></p> <p>In the previous two chapters, we have assumed that a <code>IServiceProvider</code> object is available to call <code>GetService</code>. If we create a container ourselves, then this assumption is valid. However, only in the rarest cases do we create a container directly. In a typical ASP.NET Web API application, the container is created by the framework and is not directly accessible to us. Consequently, access to `IServiceProvider ', with the exception of a few startup and configuration points, is not available. The good news is that actually we don't need access to the container. The core concept of DI is that we perform dependency resolution only at the application entry point for the \"root object\". In case of Web API apps, the entry point is a call to an operation of a Controller class serving the specific API request. When a request is received, the framework determines and creates the Controller / ControllerBase child class based on the Url and routing rules. If the controller class has dependencies (has constructor parameters), they are also resolved based on the container registration mappings, including indirect dependencies. The complete object graph is created, the root object is the controller class.</p> <p>Let's take a look at this in practice by refining our previous example with the addition of a <code>TodoController</code> class:</p> <pre><code>[Route(\"api/[controller]\")]\n[ApiController]\npublic class TodoController : ControllerBase\n{\n    // Dependencies of the TodoController class\n    private readonly TodoContext _context; // this is a DbContext\n    private readonly INotificationService _notificationService;\n\n    // Dependencies are received as constructor parameters\n    public TodoController(TodoContext context, INotificationService notificationService)\n    {\n        _context = context;\n        _notificationService = notificationService;\n\n        // Fill wit some initial data\n        if (_context.TodoItems.Count() == 0)\n        {\n            _context.TodoItems.Add(new TodoItem { Name = \"Item1\" });\n            _context.TodoItems.Add(new TodoItem { Name = \"Item2\", LinkedContactId = 2});\n            _context.SaveChanges();\n        }\n    }\n\n    // API call handling function for sending an e-mail notification\n    // Example for use: a http post request to this url (e.g. via using PostMan):\n    //     http://localhost:58922/api/todo/2/reminder\n    // , which sends an e-mail notif to the e-mail address appointed of the\n    // contact person referenced by the todo item.\n    [HttpPost(\"{id}/reminder\")]\n    public IActionResult ReminderMessageToLinkedContact(long id)\n    {\n        // Look up todo item\n        var item = _context.TodoItems.Find(id);\n        if (item == null)\n            return NotFound();\n\n        // Rend reminder e-mail\n        _notificationService.SendEmailReminder(item.LinkedContactId, item.Name);\n\n        // Actually we don't create anything here, simply return an OK\n        return Ok();\n    }\n\n    // ... further operations\n}\n</code></pre> <p>Requests under the <code>http://&lt;base_address&gt;/api/todo</code> url are routed to the <code>TodoController</code> class based on the routing rules. The mail sending request (<code>http://&lt;base_address&gt;/api/todo/&lt;todo-id&gt;/reminder</code>) is routed to its <code>TodoController.ReminderMessageToLinkedContact</code> operation. A <code>TodoController</code> object is instantiated by the framework, creating a new instance for each request. The <code>TodoController</code> class has two dependencies provided as constructor parameters. The first is a <code>TodoContext</code> object, which is a <code>DbContext</code> derived class. The other is an <code>INotificationService</code>, (which we already covered in our previous example). As we saw in the previous section, the DI framework can create these objects based on the container registered mappings (with all their indirect dependencies), and then pass them to the <code>TodoController</code> as constructor parameter, where they are stored in member variables. The entire object graph is created, with <code>TodoController</code> as the root object. This object graph is to serve the specific web API request.</p> <p>Note</p> <p>The resolution of <code>TodoContext</code> is only possible if it's pre-registered in the IoC container. We will discuss this in the next chapter.</p>"},{"location":"lecture-notes/di/#entity-framework-dbcontext-container-registration-and-resolution","title":"Entity Framework DbContext container registration and resolution","text":"<p>In applications, especially in Asp.Net Core based ones, there are two ways to use DbContext:</p> <ul> <li>Each time it is needed, we create and dispose it with the help of a using block. This can result in the creation of multiple DbContext instances serving an incoming request (which is absolutely OK).</li> <li>We create one <code>DbContext</code>  for a specific incoming request and share it for the classes involved in serving the request. In this case, we think of the <code>DbContext</code> instance as a unit of work serving the request.</li> </ul> <p>To accomplish this latter approach, ASP.NET Core provides a handy built-in DI based solution: when we configure our container with the type mappings at startup, we also register our DbContext class, which is then later automatically injected for our Controller and other (typically repository) dependencies.</p> <p>Let's see how our <code>TodoContext</code> (<code>DbContext</code> derived) class is registered in our example. The place of the registration is the usual <code>Program.cs</code> file (<code>Startup.ConfigureServices</code> for .NET versions prior to .NET 6):</p> <pre><code>// ...\nbuilder.Services.AddDbContext&lt;TodoContext&gt;(opt =&gt; opt.UseInMemoryDatabase(\"TodoList\"));\n// ...\n</code></pre> <p><code>AddDbContext</code> is an extension method defined by the framework for the <code>IServiceCollection</code> interface. This allows convenient registration of our <code>DbContext</code> class. We do not see into the implementation of <code>AddDbContext</code>, but actually it simply performs a scoped registration of our context type into the container:</p> <pre><code>services.AddScoped&lt;TodoContext, TodoContext&gt;();\n</code></pre> <p>As shown in the example, <code>TodoContext</code> is not registered via an abstraction (no <code>ITodoContext</code> interface exists) , but via the TodoContext implementation type itself. DI frameworks / IoC containers support the key part of a mapping to be a specific type, e.g. the implementation type itself. Use this approach only when justified, e.g. when we don't need extensibility for the specific type, and introducing an abstraction (interface) would only complicate the solution.</p> <p>In an Asp.Net Core environment, we don't introduce an interface for our <code>DbContext</code> derived class: instead, we always register it with the type of its class to the IoC container (in our example <code>TodoContext</code>-&gt; <code>TodoContext</code> mapping). <code>DbContext</code> itself can work with many persistent providers (e.g. MSSQL, Oracle, in-memory, etc.), so in many cases it does not make sense to put it behind further abstractions. In those cases when we need to abstract data access, we do not introduce an interface to access <code>DbContext</code>. Instead, we use the Repository design pattern, and we introduce  interfaces for each repository implementations classes, and then register their mappings to the IoC container (e.g. <code>ITodoRepository</code>-&gt; <code>TodoRepository</code>). The repository classes either instantiate the <code>DbContext</code> objects themselves or the <code>DbContext</code> is injected as constructor parameter).</p> <p>Note</p> <p>This document does not intend to make a standpoint over the often disputed question, whether it makes or does not make sense introducing a repository layer in an Entity Framework based application. For illustration purposes, our TodoApi application uses a mixed solution in this sense: controller/service classes use DbContext directly to persist TodoItem objects, and use the Repository pattern to handle Contacts. Don't mix the two approaches in a real-life application.</p> <p>The example above also shows that you can also provide a lambda expression when registering <code>DbContext</code> (in case <code>TodoContext</code>) using <code>AddDbContext</code>:</p> <pre><code>opt =&gt; opt.UseInMemoryDatabase(\"TodoList\")\n</code></pre> <p>This lambda expression is called by the container later at the resolve step - that is, every time when a <code>TodoContext</code> is instantiated. An option object is provided as a parameter (in this example, the <code>opt</code> argument): this allows us to configurate the instance created by the container. In our example, calling the <code>UseInMemoryDatabase</code> operation creates an in-memory based database called \"TodoList\".</p>"},{"location":"lecture-notes/di/#advanced-dependency-injection-registration-example","title":"Advanced dependency injection registration example","text":"<p>Not compulsory material.</p> <p>Let's cover code service registration related parts of <code>Program.cs</code> we skipped previously.</p> <p>The registration of <code>EMailSender</code> looks quite tricky:</p> <pre><code>builder.Services.AddSingleton&lt;IEMailSender, EMailSender&gt;(\n    sp =&gt; new EMailSender(sp.GetRequiredService&lt;ILogger&gt;(), \"smtp.myserver.com\") );\n</code></pre> <p>Let's take a look at the constructor of <code>EMailSender</code> to be able to better understand the situation:</p> <pre><code>public EMailSender(ILogger logger, string smtpAddress)\n{\n    _logger = logger;\n    _smtpAddress = smtpAddress;\n}\n</code></pre> <p><code>EMailSender</code> will need to be instantiated by the container when resolving <code>IEMailSender</code>, and the constructor parameters must be specified appropriately. The logger parameter is completely \"OK\", and the container can resolve it based on the ILogger-&gt; Logger container mapping registration. However, there is no way to find out the value of the <code>smtpAddress</code> parameter. To solve this problem, ASP.NET Core proposes an \"options\" mechanism for the framework, which allows us to retrieve the value from some configuration. Covering the \"options\" topic would be a far-reaching thread for us, so for simplification we applied another approach. The <code>AddSingleton</code> (and other Add ... operations) have an overload in which we can specify a lambda expression. This lambda is called by the container later at the resolve step (that is, when we ask the container for an <code>IEMailSender</code> implementation) for each instance. With the help of this lambda we manually create the <code>EMailSender</code> object, so we have the chance to provide the necessary constructor parameters. In fact, the container is really \"helpful\" with us:  it provides an <code>IServiceCollection</code> object as the lambda parameter for us (in this example it's called <code>sp</code>), and based on container registrations we can conveniently resolve types with the help of the already covered <code>GetRequiredService</code> and <code>GetService</code> calls.</p>"},{"location":"lecture-notes/di/#further-topics","title":"Further topics","text":""},{"location":"lecture-notes/di/#dependency-injectionioc-containers-in-general","title":"Dependency Injection/IoC containers in general","text":"<p>The particularities of the DI container built in ASP.NET Core:</p> <ul> <li>It provides basic services required by most applications (e.g., does not support property injection).<ul> <li>If you need more DI related functionality, you can use another IoC container Asp.Net Core can work with.</li> <li>Several Dependecy Injection / IoC container class libraries exist that can be used with .NET, with .NET Framework, or with both. A few examples: AutoFac, DryIoc, LightInject, Castle Windsor, Ninject, StructureMap, SimpleInjector, MEF, ...</li> </ul> </li> <li>It's implemented in the Microsoft.Extensions.DependencyInjection NuGet package.<ul> <li>For Asp.Net Core applications, it is automatically installed when the Asp.Net project is created. In fact, as we have seen, Asp.Net Core middleware heavily relies on it, it's a key pillar of runtime configuration and extensibility.</li> <li>For other .NET applications (e.g. a simple .NET Core based console app), you need to add it manually by installing the Microsoft.Extensions.DependencyInjection NuGet package for the project. \u00a0\u00a0  * Note: the NuGet package can be used with the (full) .NET Framework as well as it supports .NET Standard.</li> </ul> </li> </ul>"},{"location":"lecture-notes/di/#the-service-locator-antipattern","title":"The Service Locator antipattern","text":"<p>Dependency injection is not the only way of using an IoC container. Another technique called Service Locator exists. Dependency Injection is based on the mechanism of passing the dependencies of a class as constructor parameters. Service Locator uses another approach: the classes directly access the IoC container in their methods to resolve their dependencies. Keep in mind that this approach is considered an anti-pattern. The reason is simple: every time time a class needs a dependency, it has to turn to a container, so much of our code will depend on the container itself! In contrast, when dependency injection is used, dependency resolution is performed \"once\" at the application entry point for \"root objects\" (e.g. for the controller class in case of a Web API call), the rest of our code is completely independent of the container. Note that in our previous example, in our TodoController, NotificationService, EMailSender, Logger, and ContactRepository classes, we did not refer the container (neither via an IServiceProvider, nor by any other means).</p>"},{"location":"lecture-notes/di/#aspnet-core-framework-services","title":"Asp.Net Core framework services","text":"<p>Asp.Net Core has several built in services. E.g. it has support for Web API, and support for Razor Pages or MVC based web applications. These all rely on the DI services of Asp.Net Core.</p> <p>In case of an Asp.Net Web API application at application startup we have to run this piece of code (this is automatically added by VS at project creation):</p> <pre><code>builder.Services.AddControllers();\n</code></pre> <p>Note</p> <p>In case of .NET version preceding .NET 6 <code>services.AddMvc()</code> had to be called from the <code>ConfigureServices</code> operation of our <code>Startup</code> class.</p> <p><code>AddControllers</code> is a built in extension method for the <code>IServiceProvider</code> interface, which registers numerous (far more than 100!) service and configuration classes into the container required by the internals of the Web API middleware/pipeline.</p>"},{"location":"lecture-notes/di/#disposing-service-objects","title":"Disposing service objects","text":"<p>The container calls <code>Dispose</code> for the objects it creates if the object implements the <code>IDisposable</code> interface.</p>"},{"location":"lecture-notes/di/#resources","title":"Resources","text":"<ul> <li>https://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection</li> <li>https://stackify.com/net-core-dependency-injection/amp</li> <li>https://medium.com/volosoft/asp-net-core-dependency-injection-best-practices-tips-tricks-c6e9c67f9d96</li> </ul>"},{"location":"lecture-notes/ef/","title":"Defining relationships in Entity Framework","text":""},{"location":"lecture-notes/ef/#defining-relationships-in-entity-framework","title":"Defining relationships in Entity Framework","text":"<p>We store both entities and the relationships that connect them in relational databases. This allows us to query related entities through joining tables, expressed with the <code>join</code> SQL command. Entity Framework, which is an ORM framework, provides us with built-in support for conveniently managing these relationships.</p>"},{"location":"lecture-notes/ef/#defining-relationships","title":"Defining relationships","text":"<p>Convention-based mapping</p> <p>Entity Framework has conventions that enable mapping relationships automatically without explicit configuration. We will not rely on this feature here; instead will define the relationships explicitly.</p> <p>Let us look at our example classes and the relationship among them:</p> <pre><code>public class Product\n{\n    public int ID;\n    public string Name;\n    public int Price;\n    public int VATID;\n\n    public VAT VAT { get; set; }\n}\n\npublic class VAT\n{\n    public int ID;\n    public int Percentage;\n    public ICollection&lt;Product&gt; Product { get; set; }\n}\n</code></pre> <p>We can set up the configuration in the <code>OnModelCreating</code> function inherited from the <code>DBContext</code> base class. We can use the following functions for configuring an entity:</p> <pre><code>modelBuilder.Entity&lt;Example&gt;\n    .HasOne()/.HasMany()\n    .WithOne()/.WithMany()\n</code></pre> <p>In the example above, we can see a one-to-many connections, which can be described as follows:</p> <pre><code>modelBuilder.Entity&lt;Product&gt;()\n            .HasOne(d =&gt; d.VAT)\n            .WithMany(p =&gt; p.Product)\n            .HasForeignKey(d =&gt; d.VatId);\n</code></pre> <p>The connection between the two entities is provided by the foreign key of the <code>Product</code> table pointing to <code>VAT</code> table (which foreign key, naturally, also appears in the database as a column). In the C# code, a <code>VAT</code> object reference appears in class <code>Product</code>. Similarly, <code>VAT</code> objects have a list of connected <code>Product</code> instances. These C# properties are called navigation properties.</p>"},{"location":"lecture-notes/ef/#explicit-joining","title":"Explicit joining","text":"<p>The <code>DBContext</code> offers the tables as <code>DBSets</code>, on which we can perform LINQ operations. One such operation is the <code>join</code> function. Two <code>DBSets</code> can be joined via the appropriate foreign key. Similar to it's SQL equivalent, the following LINQ expression declaratively describes what we want to get.</p> <pre><code>var query = \n    from p in dbContext.Product\n    join v in dbContext.Vat on p.VatId equals v.Id\n    where p.Name.Contains(\"test\")\n    select v.Percentage;\n\n// Displays the generated SQL query\nConsole.WriteLine(query.ToQueryString());    \n</code></pre> <p>The generated SQL query will look similar to this:</p> <pre><code>SELECT [v].[Percentage]\nFROM [Product] AS [p]\nINNER JOIN [VAT] AS [v] ON [p].[VatId] = [v].[ID]\nWHERE [p].[Name] LIKE N'%test%'\n</code></pre> <p>We rarely need to use explicit joins. As a matter of fact, we should avoid using them when navigation properties are available.</p>"},{"location":"lecture-notes/ef/#navigation-property","title":"Navigation property","text":"<p>Since we have configured the relationship between the <code>Product</code> and <code>VAT</code> EF entities in our <code>DbContext</code>, we can use the <code>VAT</code> property in the <code>Product</code> class: this is the navigation property. The joining \"behind\" the navigation property is handled automatically by EF without us having to define it in the query. This simplifies our previous query to:</p> <pre><code>var query =\n    from p in dbContext.Product\n    where p.Name.Contains(\"test\")\n    select p.VAT.Percentage;\n\n// Displays the generated SQL query\nConsole.WriteLine(query.ToQueryString());\n</code></pre> <p>Below we can see the generated SQL query, which differs from the previous one only in the type of join, but otherwise, we get to the same solution.</p> <pre><code>SELECT [v].[Percentage]\nFROM [Product] AS [p]\nLEFT JOIN [VAT] AS [v] ON [p].[VatId] = [v].[ID]\nWHERE [p].[Name] LIKE N'%test%'\n</code></pre> <p>Prefer navigation properties</p> <p>In EF, we should always strive to use the navigation properties when possible. We should avoid performing explicit joins.</p>"},{"location":"lecture-notes/ef/#include","title":"Include","text":"<p>In the previous example, only one scalar result was queried. But what happens to the navigation properties when we query an entire entity? For example:</p> <pre><code>var prod = dbContext.Product.Where(p =&gt; p.Name.Contains(\"test\")).First();\n\nConsole.WriteLine(prod.Name); // this works, it will print the name\nConsole.WriteLine(prod.VAT.Percentage); // accessing the referenced entity via the navigation property\n</code></pre> <p>In this example, we would get a runtime error in the last line. Why is that? Despite the navigation property being configured, EF does not load referenced entities by default. We can work with them in queries (as we wrote <code>p.VAT.Percentage</code> in a previous query), but if we query a <code>Product</code> entity, it does not include the referenced <code>VAT</code> entity. The referenced record(s) could be fetched. But it is up to the developer to decide if they really need them. Just consider, if all the referenced entities were fetched automatically (even transitively), the database would have to look up hundreds or thousands or records to get a single entity and all of it's referenced data via navigation properties. This is unnecessary in most cases.</p> <p>If we really need the referenced entities, then we need to specify this in the code using <code>Include</code> as follows:</p> <pre><code>var query =\n    from p in dbContext.Products.Include(p =&gt; p.VAT)\n    where p.Name.Contains(\"test\")\n    select p;\n\n// or an alternative syntax for the same:\n// var query = products\n//               .Include(p =&gt; p.VAT)\n//               .Where(p =&gt; p.Name.Contains(\"test\"));\n\nConsole.WriteLine(query.ToQueryString());\n</code></pre> <p>If we look at the generated SQL statement, it shows both the appropriate <code>join</code> and the required data appearing within the <code>select</code> statement.</p> <pre><code>SELECT [p].[Id], [p].[CategoryId], [p].[Description], [p].[Name], [p].[Price], [p].[Stock], [p].[VatId], [v].[ID], [v].[Percentage]\nFROM [Product] AS [p]\nLEFT JOIN [VAT] AS [v] ON [p].[VatId] = [v].[ID]\nWHERE [p].[Name] LIKE N'%test%'\n</code></pre> <p>Automatic lazy loading of referenced entities</p> <p>In Entity Framework, it is possible to turn on lazy loading, which causes entities to be loaded through navigation properties on demand. The loading is performed in a lazy way (that is, only when needed) without an explicit <code>Include</code>. While this solution is convenient for the developer, it comes at a price: loading data when needed (when the code reaches a statement referencing the property) will typically result in several separate database queries. In the <code>Include</code> solution, you can see above that a single query loads both the <code>Product</code> and <code>VAT</code> data. If we used lazy loading, there would be a query for the <code>Product</code> data and another one for the referenced <code>VAT</code> properties at a later time. Thus, lazy loading is usually worse in terms of performance.</p>"},{"location":"lecture-notes/linq/","title":"LINQ: Language Integrated Query","text":""},{"location":"lecture-notes/linq/#linq-language-integrated-query","title":"LINQ: Language Integrated Query","text":"<p>Consider the following classes and lists of such instances.</p> <pre><code>class Product\n{\n    public int ID;\n    public string Name;\n    public int Price;\n    public int VATID;\n}\n\nclass VAT\n{\n    public int ID;\n    public int Percentage;\n}\n\nList&lt;Product&gt; products = ...\nList&lt;VAT&gt; vat = ...\n</code></pre> <p><code>System.Linq</code></p> <p>To access Linq functionality we need the <code>System.Linq</code> namespace:</p> <pre><code>using System.Linq;\n</code></pre>"},{"location":"lecture-notes/linq/#linq-operations","title":"LINQ operations","text":"<p>The examples below, when available, show both syntaxes.</p>"},{"location":"lecture-notes/linq/#filtering","title":"Filtering","text":"<pre><code>products.Where(p =&gt; p.Price &lt; 1000)\n\nfrom p in products\nwhere p.Price &lt; 1000\n</code></pre>"},{"location":"lecture-notes/linq/#projection","title":"Projection","text":"<pre><code>products.Select(p =&gt; p.Name)\n\nfrom p in products\nselect p.Name\n</code></pre>"},{"location":"lecture-notes/linq/#join","title":"Join","text":"<pre><code>from p in products\njoin v in vat on p.VATID equals v.Id\nselect p.Price * v.Percentage\n\nproducts.Join(vat, p =&gt; p.VATID, v =&gt; v.Id, (p, v) =&gt; p.Price * v.Percentage)\n</code></pre>"},{"location":"lecture-notes/linq/#sorting","title":"Sorting","text":"<pre><code>products.OrderBy[Descending](p =&gt; p.Name)\n.ThenBy[Descending](p =&gt; p.Price)\n\nfrom p in products\norderby p.Name, p.Price [descending]\n</code></pre>"},{"location":"lecture-notes/linq/#set-operations","title":"Set operations","text":"<pre><code>products.Select(p =&gt; p.Name).Distinct()\n\nproducts.Where(p =&gt; p.Price &lt; 1000)\n.Union( products.Where(p =&gt; p.Price &gt; 100000) )\n\n// similarly Except, Intersect\n</code></pre>"},{"location":"lecture-notes/linq/#aggregation","title":"Aggregation","text":"<pre><code>products.Count()\n\nproducts.Select(p =&gt; p.Price).Average()\n\n// similarly Sum, Min, Max\n</code></pre>"},{"location":"lecture-notes/linq/#first-last","title":"First, last","text":"<pre><code>products.First()\n\nproducts.Last()\n\nproducts.Where(p =&gt; p.Id==12).FirstOrDefault()\n\nproducts.Where(p =&gt; p.Id==12).SingleOrDefault()\n</code></pre>"},{"location":"lecture-notes/linq/#paging","title":"Paging","text":"<pre><code>products.Take(10)\n\nproducts.Skip(10).Take(10)\n</code></pre>"},{"location":"lecture-notes/linq/#contains-exists","title":"Contains (exists)","text":"<pre><code>products.Any(p =&gt; p.Price == 1234)\n\nproducts.Where(p =&gt; p.Price == 1234).Any()\n</code></pre>"},{"location":"lecture-notes/linq/#grouping","title":"Grouping","text":"<pre><code>from p in products\ngroup p by p.VATID\n\nproducts.GroupBy(p =&gt; p.VATID)\n</code></pre>"},{"location":"lecture-notes/linq/#advanced-projections","title":"Advanced projections","text":"<p>During projection we can transform the results into various formats.</p>"},{"location":"lecture-notes/linq/#whole-entity","title":"Whole entity","text":"<pre><code>from p in products\n...\nselect p\n</code></pre> <p>The result set is of type <code>IQueryable&lt;Product&gt;</code>, so we get Product instances.</p>"},{"location":"lecture-notes/linq/#specified-field","title":"Specified field","text":"<pre><code>from p in products\n...\nselect p.Name\n</code></pre> <p>The result set is of type <code>IQueryable&lt;string&gt;</code>, so we only get the names.</p>"},{"location":"lecture-notes/linq/#named-types","title":"Named types","text":"<pre><code>from p in products\n...\nselect new MyType(p.Name, p.Price)\n</code></pre> <p>The result set is of type <code>IQueryable&lt;MyType&gt;</code>, when MyType is a class we have to define and has a matching constructor.</p>"},{"location":"lecture-notes/linq/#anonym-types","title":"Anonym types","text":"<pre><code>from p in products\nwhere p.Price &gt; 1000\nselect new { ID = p.ID, Name = p.Name };\n</code></pre> <p>Anonym types can be instantiated using the syntax <code>new { }</code>. The compiler will effectively create a class definition with the properties we specified. This is generally used when we only need two or three properties, and we have no need for the entire entity.</p> <p>A similar use case for anonym types is when we calculate a property inside the query, such as the name of the product and the full price:</p> <pre><code>from p in products\njoin v in vat on p.VATID equals v.Id\nselect new { Name = p.Name, FullPrice = p.Price * v.Percentage }\n</code></pre>"},{"location":"lecture-notes/linq/#linq-expressions-and-ienumerableiqueryable","title":"LINQ expressions and IEnumerable/IQueryable","text":"<p>Depending on the data source we are using Linq on the result of a query, such as <code>products.Where(p =&gt; p.Price &lt; 1000)</code> yields a variable of type <code>IEnumerable&lt;T&gt;</code> or <code>IQueryable&lt;T&gt;</code>. Neither of these contain the result sets; they are only descriptors, that is, the operation has not yet been evaluated yet. This is called deferred execution, as the execution will only happen when the result is effectively used:</p> <ul> <li>when the result set is iterated (e.g. <code>foreach</code>),</li> <li>when a specific item is accessed (see later, e.g. <code>.First()</code>),</li> <li>when we as for a list instead (<code>.ToList()</code>).</li> </ul> <p>This operation is useful, because this allows us to chain LINQ operations after each other, such as:</p> <pre><code>var l = products.Where(p =&gt; p.Price &lt; 1000)\n                .Where(p =&gt; p.Name.Contains('s'))\n                .OrderBy(p =&gt; p.Name)\n                .Select(p =&gt; p.Name)\n...\n\n// variable l does not contain the result\n\nforeach(var x in l) // this is when the execution will happen\n   { ... }\n</code></pre> <p>Force evaluation</p> <p>If we want to force the execution at any given moment, we usually use <code>.ToList()</code>. But this has to be considered first and only used when necessary.</p>"},{"location":"lecture-notes/linq/#more-information-and-further-examples","title":"More information and further examples","text":"<p>Lambda expressions: https://www.tutorialsteacher.com/linq/linq-lambda-expression</p> <p>Linq: https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/</p>"},{"location":"lecture-notes/mongodb/","title":"MongoDB basics, operations, and the MongoDB .NET Driver","text":""},{"location":"lecture-notes/mongodb/#mongodb-basics-operations-and-the-mongodb-net-driver","title":"MongoDB basics, operations, and the MongoDB .NET Driver","text":""},{"location":"lecture-notes/mongodb/#nosql-databases","title":"NoSQL databases","text":"<p>NoSQL databases are data management systems that do not work with the relational data model. The NoSQL name can be a little misleading, as the concept has little to do with the SQL language - the main difference is rather the representation of the data. Why do we need such new databases when we already have the relational model and relational databases? A small database with a simple schema can be easily described in the relational model. But our applications evolve: new functionalities are added, making the schema more complex. More and more data is added to the database making the maintenance inefficient.</p> <p>Relational databases need constant schema changes and updates, which can be cumbersome. Constant data migrations can be a pain. Furthermore, relational databases can be bottlenecks from the scalability perspective - but we will not discuss this aspect in more detail.</p> <p>NoSQL databases offer a solution to these problems. Instead of the rigid schema, NoSQL databases have a flexible schema. In other words, we will require less consistency regarding the data.</p>"},{"location":"lecture-notes/mongodb/#basic-concepts-of-mongodb","title":"Basic concepts of MongoDB","text":"<p>MongoDB is a client-server database system that has a non-relational schema. The mongod (Mongo daemon) process on the right is the database server. The other side is our application, where a client connects to the database using a network connection. This connection uses the so-called wire protocol, which is a MongoDB proprietary communication protocol. The protocol transmits data and queries in JSON format represented in a binary fashion as BSON.</p> <p></p>"},{"location":"lecture-notes/mongodb/#logical-structure","title":"Logical structure","text":"<p>The top layer of a MongoDB database system is the so-called cluster. The servers are organized into these clusters. We will not discuss clusters here; these are tools for enabling scalability. The databases are the mongod processes, which host the databases. A MongoDB server/cluster stores multiple databases. And the databases contain collections. If we want to map these concepts to a relational model, then the collections correspond to the tables, and the rows/records in a table correspond to the documents of the collection.</p> <p>Let us investigate these further.</p>"},{"location":"lecture-notes/mongodb/#document","title":"Document","text":"<p>The document is the unit of storage in MongoDB. A document is a JSON (-like) file: it contains key-value pairs. MongoDB itself stores it as a BSON in binary format.</p> <pre><code>{\n    name: \"sue\",\n    age: 26,\n    status: \"A\",\n    groups: [ \"news\", \"sports\"]\n}\n</code></pre> <p>The keys can have arbitrary names with a few limitations, such that they have to be unique and cannot begin with the <code>$</code> character. The names are case sensitive. Values can be string, number, date, binary, embedded document, <code>null</code>, or even as the <code>groups</code> in the example shows, an array - a relational database cannot represent an array in such a simple way.</p> <p>Mapping to the object-oriented world, a document is an object. MongoDB documents have a maximum size of 16MB and this is not a configurable parameter.</p>"},{"location":"lecture-notes/mongodb/#collection","title":"Collection","text":"<p>Collections are analogous to relational database tables, but without a schema. Collections need no definition; the system creates them upon first use. Collections are a place for \"similar\" documents. Although there is no schema, indexes can still be defined on the collections to support fast searching. Since there is no schema, there are no domain integrity requirements enforced either.</p>"},{"location":"lecture-notes/mongodb/#database","title":"Database","text":"<p>The database has the same purpose as in the relational model. It gathers all data of our application. Access management is also configured on the database level. The name of databases is case sensitive and lowercase by convention.</p>"},{"location":"lecture-notes/mongodb/#key","title":"Key","text":"<p>The <code>_id</code> field is the unambiguous identifier of each document. Other keys cannot be defined. This field does not need to be specified during insert; the client driver or the server can generate a new value (a 12 byte <code>ObjectId</code> by default).</p> <p>Uniqueness can be guaranteed with the use of indices. We can define an index and mark it as unique to create a key-like field. These unique indices can contain multiple fields too.</p> <p>There are no references to keys in MongoDB. A document can reference another document by copying it's key, but the system has no consistency guarantees for these (e.g., the referenced document can be deleted).</p>"},{"location":"lecture-notes/mongodb/#mongodb-operations-and-the-mongodb-net-driver","title":"MongoDB operations and the MongoDB .NET Driver","text":"<p>The following code snippets use the official Nuget package MongoDB.Driver.</p>"},{"location":"lecture-notes/mongodb/#establishing-a-connection","title":"Establishing a connection","text":"<p>To access the MongoDB database, you first need a connection. A <code>MongoClient</code> class represents the connection. We need the server address to establish a connection (see https://docs.mongodb.com/manual/reference/connection-string/ for details on the connection string).</p> <pre><code>var client = new MongoClient(\"mongodb://localhost:27017\");\n</code></pre> <p>The connection should be treated as a singleton and not disposed of.</p> <p>Connection lifetime</p> <p>The connection is typically stored in a global static variable, or an IoC (Inversion of Control) / DI (Dependency Injection) store.</p> <p>Although the database name may be in the connection string (e.g. <code>mongodb://localhost:27017/datadriven</code>), it is used only for authentication. Thus, after establishing the connection, we need to specify what database we will use.</p> <pre><code>var db = client.GetDatabase(\"datadriven\");\n</code></pre> <p>The database does not need to exist in advance. The above call will automatically create if the database does not already exist.</p>"},{"location":"lecture-notes/mongodb/#managing-collections","title":"Managing collections","text":"<p>Unlike a relational database, in MongoDB, our operations are always performed on a single collection, so the selection of a collection is not part of the issued command (as in the <code>from</code> in SQL), but a prerequisite for the operation. You can get a specific collection by calling <code>GetCollection</code>; its generic parameter is the C# class implementing the document type.</p> <pre><code>var collection = db.GetCollection&lt;BsonDocument&gt;(\"products\");\n</code></pre> <p>The basic concept of the .NET MongoDB driver is to map every document to a .NET object. This is also called ODM (Object Document Mapping). ODM is the equivalent of ORM in the NoSQL database world.</p> <p>\"Raw\" json</p> <p>In other languages and platforms, MongoDB drivers do not always map to objects. Sample codes found on the Internet often show communication via \"raw\" JSON documents. Let's try to avoid this, as we learned in ORM, that object-oriented mapping is more convenient and secure.</p> <p>In the previous example, a document of the type \"BsonDocument\" is used. <code>BsonDocument</code> is a generic document representation in which we can store key-value pairs. It is uncomfortable and unsafe to use; thus we usually do try to avoid it. See the suggested solution soon.</p> <p>You can run queries on the variable representing the collection, such as inserting a document and then listing the contents of the collection. The collection will be created automatically the first time you use it, so you don't have to define it.</p> <pre><code>collection.InsertOne(new BsonDocument()\n{\n    { \"name\", \"Apple\" },\n    { \"categoryName\", \"Apple\" },\n    { \"price\", 123 }\n});\n\n// listing all documents: a search criteria is needed\n// this is an empty criteria matching all documents\nvar list = collection.Find(new BsonDocument()).ToList();\nforeach(var l in list)\n    Console.WriteLine(l);\n</code></pre> <p>Naming convention</p> <p>Field names in the document start with lowercase letters like <code>price</code> or <code>categoryName</code> (this is the so-called camel case spelling). This is a convention of the MongoDB world for historical reasons. Unless there is a good reason, do not deviate from it.</p>"},{"location":"lecture-notes/mongodb/#mapping-documents-to-c-objects","title":"Mapping documents to C# objects","text":"<p>As with relational databases, we can work with objects and classes in MongoDB. The .NET driver for MongoDB offers this conveniently.</p> <p>The first step is to define the C# class(es) to map the contents of the database. Since there is no schema for the database and table, we cannot generate C# code based on the schema (as we did with the Entity Framework). So in this world, we tend to follow the Code First approach, which is to write C# code and have the system translate it to database collections.</p> <p>Let us define the following classes to represent Products.</p> <pre><code>public class Product\n{\n    public ObjectId Id { get; set; } // this will be the identifier with name _id\n    public string Name { get; set; }\n    public float Price { get; set; }\n    public int Stock { get; set; }\n    public string[] Categories { get; set; } // array field\n    public VAT VAT { get; set; } // embedded document\n}\n\npublic class VAT // this class is only ever embedded, hence needs to id\n{\n    public string VATCategoryName { get; set; }\n    public float Percentage { get; set; }\n}\n</code></pre> <p>Note that the name of the field was <code>price</code> before, but in C# it starts with a capital letter, according to Pascal Case: <code>Price</code>. The MongoDB .NET driver integrates with the C# language and the .NET environment and respects its conventions so that the names in the class definition and the field names in the MongoDB documents will be mapped automatically: the <code>Price</code> class property will be <code>price</code> in the document.</p>"},{"location":"lecture-notes/mongodb/#customizing-the-mapping","title":"Customizing the mapping","text":"<p>The C# class - MongoDB document mapping is automatic, but it can also be customized. There are several ways to deviate from the conventions.</p> <p>The easiest way is to use custom attributes in the class definition:</p> <pre><code>public class Product\n{\n    // maps to field _id\n    [BsonId]\n    public ObjectId Identifier { get; set; }\n\n    // can specify the name explicitly\n    [BsonElement(\"price\")]\n    public string TotalPrice { get; set; }\n\n    // properties can be ignored\n    [BsonIgnore]\n    public string DoNotSave { get; set; }\n}\n</code></pre> <p>Our other option is to register so-called convention packs at a higher level. The convention pack describes the rules of mapping. (A set of conventions also defines the default behavior.)</p> <p>For example, you can specify the following to map the field names to camel case and exclude data members with a default value (defined in the C# language) from the document.</p> <pre><code>// define convention pack\nvar pack = new ConventionPack();\npack.Add(new CamelCaseElementNameConvention());\npack.Add(new IgnoreIfDefaultConvention(true));\n\n// register the convention pack\n// the first parameter is a name to reference this pack\n// the last argument is a fitlering criteria when to use this convention\nConventionRegistry.Register(\"datadriven\", pack, t =&gt; true);\n</code></pre> <p>We also have more sophisticated customizations, such as defining conversion logic for translation between a C# representation and a MongoDB representation, and specifying how to save inheritance hierarchies. For more details, see the official documentation: https://mongodb.github.io/mongo-csharp-driver/2.8/reference/bson/serialization/.</p>"},{"location":"lecture-notes/mongodb/#queries","title":"Queries","text":"<p>We will use the collection from now on by mapping it to the <code>Product</code> class. This is the recommended solution; the <code>BsonDocument</code> based solution is used only when necessary.</p> <p>The simplest query we have already seen is to list all the documents:</p> <pre><code>var collection = db.GetCollection&lt;Product&gt;(\"products\");\n\nvar list = collection.Find(new BsonDocument()).ToList();\nforeach (var p in list)\n    Console.WriteLine($\"Id: {p.Id}, Name: {p.Name}\");\n</code></pre> <p>Listing is done using the <code>Find</code> method. The name illustrates MongoDB's philosophy: listing an entire collection is not practical, so there is no simple syntax for it. <code>Find</code> requires a search criteria, which is an empty condition here to matches everything.</p> <p>There are several ways to describe search criteria.</p> <p>With <code>BsonDocument</code> based filtering, the filtering condition must be written according to the MongoDB syntax. We generally will avoid this because the MongoDB .NET driver provides a more convenient solution for us.</p> <p>In most cases, we can use Lambda expressions to describe the filtering.</p> <pre><code>collection.Find(x =&gt; x.Price &lt; 123);\n</code></pre> <p>In this case, the Lambda expression is a delegate of type <code>Predicate &lt;T&gt;</code>, that is, expects a <code>Product</code> and returns <code>bool</code>. Thus in the example above, the <code>x</code> variable represents a <code>Products</code> instance. Of course, this search also works for more complex cases.</p> <pre><code>collection.Find(x =&gt; x.Price &lt; 123 &amp;&amp; x.Name.Contains(\"red\"));\n</code></pre> <p>The filtering described by the Lambda expressions hides what search syntax we actually have in MongoDB. For example, the above <code>Contains</code> search condition will actually mean a search with a regular expression.</p> <p>In MongoDB's own language, the previous filter looks like this:</p> <pre><code>{\n  \"price\": {\n    \"$lt\": 123.0\n  },\n  \"name\": \"/red/s\"\n}\n</code></pre> <p>Note that this description is itself a document. If we wanted to write the filter condition ourselves, we would have to create this descriptor in a <code>BsonDocument</code>. The document keys describe the fields used for filtering, and the values are the filter criteria. In some cases, the condition is a scalar value such as a regular expression (or if we filter for equality); in other cases, the condition is an embedded document, as with the <code>&lt;</code> condition. Here, the <code>$lt</code> key is a special key that denotes the less than operator and the value to the right of the operator is 123.0. The regular expression should be specified according to JavaScript RegExp Syntax. The conditions listed in this way are automatically evaluated in and <code>and</code> fashion.</p> <p>Instead of the Lambda expression, we can create a similar description without having to compile a filter condition in \"text\" form. The .NET driver for MongoDB gives us the ability to use a so-called builders.</p> <pre><code>collection.Find(\n    Builders&lt;Product&gt;.Filter.And(\n        Builders&lt;Product&gt;.Filter.Lt(x =&gt; x.Price, 123),\n        Builders&lt;Product&gt;.Filter.Regex(x =&gt; x.Name, \"/red/s\"),\n    )\n);\n</code></pre> <p>The above syntax is a bit more eloquent than the Lambda expression, but it is closer to the MongoDB philosophy, and better describes what we want. We can view this syntax as SQL, a declarative, goal-oriented, but platform-specific description. However, it is also type-safe.</p> <p>The <code>Builders&lt;T&gt;</code> generic class is an auxiliary class that we can use to build filtering and other MongoDB specific definitions. <code>Builders&lt;Product&gt;.Filter</code> can be used to define filtering conditions that match the Product C# class. First, we create an and connection, within which we have two filtering conditions. The operators are the less than and regular expressions seen before. We pass two parameters to these functions: the field to be filtered and the operand.</p> <p>Note that no string-based field names were used here or in the Lambda expressions. We can refer to the class fields with the C# Expression syntax. This is practical because we avoid typing field names.</p> <p>Note that all ways of describing the search criteria are identical. The MongoDB driver maps each syntax to its internal representation. Lambda expression-based requires fewer characters and fits better into C#, while the builder approach is used to express MongoDB features better. You can use either one.</p>"},{"location":"lecture-notes/mongodb/#using-query-results","title":"Using query results","text":"<p>The result of the <code>collection.Find(...)</code> function is not yet the result set, but only a descriptor to execute the query. There are generally three ways to retrieve and process the result.</p>"},{"location":"lecture-notes/mongodb/#listing","title":"Listing","text":"<p>Get the complete result set as a list: <code>collection.Find(...).ToList()</code>.</p>"},{"location":"lecture-notes/mongodb/#get-firstsingle-item","title":"Get first/single item","text":"<p>If you only need the first item, or know that there will be only one item, you can use <code>collection.Find(...).First()</code>, <code>.FirstOrDefault()</code>, or <code>.Single()</code>, <code>.SingleOrDefault()</code> functions.</p>"},{"location":"lecture-notes/mongodb/#cursor","title":"Cursor","text":"<p>If the result set contains multiple documents, it is advisable to iterate it using a cursor. MongoDB limits the size of the response to a query, so if we query too many records, we may get an error instead of a result. To overcome this, we use the cursors where we always get only a subset of the documents.</p> <pre><code>var cur = collection.Find(...).ToCursor();\nwhile (cur.MoveNext()) // cursor stepping\n{\n    foreach (var t in cur.Current) // the value of the cursor is not a single document, but a list in itself\n    { ... }\n}\n</code></pre>"},{"location":"lecture-notes/mongodb/#operators-for-filtering","title":"Operators for filtering","text":"<p>The filter criteria apply to the fields in the document, and the filter criteria are always constant. Thus it is not possible, for example, to compare two fields, and we cannot refer to other collections. There is a so-called MongoDB aggregation pipeline, which allows you to formulate more complex queries, but for now, let us focus on simple queries.</p> <p>The filter condition compares a field in the document to a constant we specify. The following options are most commonly used.</p>"},{"location":"lecture-notes/mongodb/#comparison-operators","title":"Comparison operators","text":"<pre><code>collection.Find(x =&gt; x.Price == 123);\ncollection.Find(Builders&lt;Product&gt;.Filter.Eq(x =&gt; x.Price, 123)); //Eq, as in equals\n\ncollection.Find(x =&gt; x.Price != 123);\ncollection.Find(Builders&lt;Product&gt;.Filter.Ne(x =&gt; x.Price, 123)); // Ne, as in not equals\n\ncollection.Find(x =&gt; x.Price &gt;= 123);\ncollection.Find(Builders&lt;Product&gt;.Filter.Gte(x =&gt; x.Price, 123)); // Gte, as in greater than or equal to\n\ncollection.Find(x =&gt; x.Price &lt; 123);\ncollection.Find(Builders&lt;Product&gt;.Filter.Lt(x =&gt; x.Price, 123)); // Lt, as in less than\n</code></pre>"},{"location":"lecture-notes/mongodb/#boolean-operators","title":"Boolean operators","text":"<pre><code>collection.Find(x =&gt; x.Price &gt; 500 &amp;&amp; x.Price &lt; 1000);\ncollection.Find(\n    Builders&lt;Product&gt;.Filter.And(\n        Builders&lt;Product&gt;.Filter.Gt(x =&gt; x.Price, 500),\n        Builders&lt;Product&gt;.Filter.Lt(x =&gt; x.Price, 1000)\n    )\n);\n\ncollection.Find(x =&gt; x.Price &lt; 500 || x.Stock &lt; 10);\ncollection.Find(\n    Builders&lt;Product&gt;.Filter.Or(\n        Builders&lt;Product&gt;.Filter.Lt(x =&gt; x.Price, 500),\n        Builders&lt;Product&gt;.Filter.Lt(x =&gt; x.Stock, 10)\n    )\n);\n\ncollection.Find(x =&gt; !(x.Price &lt; 500 || x.Stock &lt; 10));\ncollection.Find(\n    Builders&lt;Product&gt;.Filter.Not(\n        Builders&lt;Product&gt;.Filter.Or(\n            Builders&lt;Product&gt;.Filter.Lt(x =&gt; x.Price, 500),\n            Builders&lt;Product&gt;.Filter.Lt(x =&gt; x.Stock, 10)\n        )\n    )\n);\n</code></pre>"},{"location":"lecture-notes/mongodb/#value-is-one-of-multiple-alternatives","title":"Value is one of multiple alternatives","text":"<pre><code>collection.Find(x =&gt; x.Id == ... || x.Id = ...);\ncollection.Find(Builders&lt;Product&gt;.Filter.In(x =&gt; x.Id, new[] { ... }));\n// similarly Nin, as in not in oper\u00e1tor\n</code></pre>"},{"location":"lecture-notes/mongodb/#value-exists-not-null","title":"Value exists (not null)","text":"<pre><code>collection.Find(x =&gt; x.VAT != null);\ncollection.Find(Builders&lt;Product&gt;.Filter.Exists(x =&gt; x.VAT));\n</code></pre> <p>Exists filtering</p> <p>Does exist, that is, non-null filtering is special because there are two ways to have a null value in MongoDB: if the key exists in the document and it has a value of null; or if the key does not exist at all.</p>"},{"location":"lecture-notes/mongodb/#filtering-fields-of-embedded-document","title":"Filtering fields of embedded document","text":"<p>Embedded documents can be used for filtering in the same way. The following are all valid, and it does not matter if the embedded document (VAT) does not exist:</p> <pre><code>collection.Find(x =&gt; x.VAT.Percentage &lt; 27);\ncollection.Find(Builders&lt;Product&gt;.Filter.Lt(x =&gt; x.VAT.Percentage, 27));\n\ncollection.Find(Builders&lt;Product&gt;.Filter.Exists(x =&gt; x.VAT.Percentage, exists: false));\n// does not exists, that is, in C#, equals null\n</code></pre>"},{"location":"lecture-notes/mongodb/#filtering-based-on-an-array-field","title":"Filtering based on an array field","text":"<p>Any field in the document can be an array value, as in the example <code>string [] Categories</code>. In MongoDB, we can define filtering based on an array field using the <code>Any*</code> criterion.</p> <pre><code>// products of this category\ncollection.Find(Builders&lt;Product&gt;.Filter.AnyEq(x =&gt; x.Categories, \"Balls\"));\n\n// products that are assigned to at least one category not listed by name\ncollection.Find(Builders&lt;Product&gt;.Filter.AnyNin(x =&gt; x.Categories, new[] { \"Balls\", \"Rackets\" }));\n</code></pre> <p>Any...</p> <p>The <code>Any*</code> conditions look at every element of an array but match only once with respect to the document. So, if multiple elements of an array match a condition, we only get the document once in the result set.</p>"},{"location":"lecture-notes/mongodb/#query-execution-pipeline","title":"Query execution pipeline","text":"<p>MongoDB queries are executed through a pipeline. We won't go into details about this, but in addition to simple filtering, we'll see a few examples frequently used in queries.</p>"},{"location":"lecture-notes/mongodb/#paging-sorting","title":"Paging, sorting","text":"<p>For paging, we specify the maximum number of matching documents we request:</p> <pre><code>collection.Find(...).Limit(100);\n</code></pre> <p>And for the items on the following page, we skip the items already seen on the first page:</p> <pre><code>collection.Find(...).Skip(100).Limit(100);\n</code></pre> <p><code>Skip</code> and <code>Limit</code> are meaningless in this form because without sorting, the \"first 100 elements\" query is not deterministic. So for these types of queries, it is necessary to provide an appropriate sorting requirement. Sorting is defined using <code>Builders&lt;T&gt;</code>.</p> <pre><code>collection.Find(...)\n    .Sort(Builders&lt;Product&gt;.Sort.Ascending(x =&gt; x.Name))\n    .Skip(100).Limit(100);\n</code></pre> <p>Paging issue</p> <p>The above paging mechanism is still not entirely correct. For example, if a product is deleted in between the query of the first and second pages, the products will shift by one, and there may be a product that will be skipped. This is, in fact, not a problem just with MongoDB. Consider how you would solve this problem.</p>"},{"location":"lecture-notes/mongodb/#number-of-documents","title":"Number of documents","text":"<p>There are two ways to query the number of documents that match a query:</p> <pre><code>collection.CountDocuments(Builders&lt;Product&gt;.Filter.AnyEq(x =&gt; x.Categories, \"Balls\"));\n\ncollection.Find(Builders&lt;Product&gt;.Filter.AnyEq(x =&gt; x.Categories, \"Balls\")).CountDocuments();\n</code></pre>"},{"location":"lecture-notes/mongodb/#aggregation-pipeline","title":"Aggregation pipeline","text":"<p>Aggregation operations process multiple documents and return some calculated results from them. MongoDB provides three ways to perform aggregation operations:</p> <ul> <li>Aggregation pipelines,</li> <li>Single Purpose Aggregation Operations,</li> <li>and Map-reduce functions.</li> </ul> <p>Since MongoDB version 5.0, Map-reduce is an obsolete method because the aggregation pipeline is better in terms of usability and speed.</p> <p>For Single Purpose Aggregation Operations, MongoDB provides us with <code>IMongoCollection&lt;TDocument&gt;.EstimatedDocumentCount()</code>, <code>IMongoCollection&lt;TDocument&gt;.Count()</code> and <code>IMongoCollection&lt;TDocument&gt;.Distinct()</code> functions, which all perform simple aggregation on a single collection.</p> <p></p> <p>Source</p> <p>https://docs.mongodb.com/manual/images/distinct.bakedsvg.svg</p> <p>General aggregations can be performed by defining a pipeline manually. An aggregation pipeline is built up from stages, each serving a specific action (filter, group, count, calculate, etc.) on its input documents. A pipeline can also return multiple results from a set of documents (e.g., total, average, maximum, or minimum values).</p> <p>Let's look at this through an example of grouping.</p> <pre><code>// products in the \"Balls\" category grouped by VAT percentage\nforeach (var g in collection.Aggregate()\n                            .Match(Builders&lt;Product&gt;.Filter.AnyEq(x =&gt; x.Categories, \"Balls\")) // filtering\n                            .Group(x =&gt; x.VAT.Percentage, x =&gt; x) // grouping\n                            .ToList())\n{\n    Console.WriteLine($\"VAT percentage: {g.Key}\");\n    foreach(var p in g)\n        Console.WriteLine($\"\\tProduct: {p.Name}\");\n}\n</code></pre>"},{"location":"lecture-notes/mongodb/#insert-modify-delete","title":"Insert, Modify, Delete","text":"<p>After queries, let's get to know data modification constructs.</p>"},{"location":"lecture-notes/mongodb/#inserting-a-new-document","title":"Inserting a new document","text":"<p>To insert a new document, you need the object representing the new document. We can add this to the collection.</p> <pre><code>var newProduct = new Product\n{\n    Name = \"Apple\",\n    Price = 890,\n    Categories = new[] { \"Fruits\" }\n};\ncollection.InsertOne(newProduct);\n\nConsole.WriteLine($\"Inserted record id id: {newProduct.Id}\"); // after insert the ID of the document will be available in the C# instance\n</code></pre> <p>Note that the <code>Id</code> field is not assigned. This will be set by the client driver. If we want, we can give it a value, but it is not customary.</p> <p>Remember, there is no schema in MongoDB, so the inserted document may be completely different from the rest of the items in the collection. Note that not all fields are assigned values. Because there are no integrity criteria, any insertion will be successful, but there may be problems with queries (for example, assuming that the <code>Stock</code> field is always set).</p> <p>You can use the <code>InsertMany</code> function to insert multiple documents, but remember that there are no transactions, so adding multiple documents is an independent operation. If, for any reason, an error occurs during the insertion, the successfully inserted documents will remain in the database. However, each document is saved atomically, so no \"half\" document can be added to the database in the event of an error.</p>"},{"location":"lecture-notes/mongodb/#delete-documents","title":"Delete documents","text":"<p>To delete, you need to define a filter condition and execute it with the <code>DeleteOne</code> or <code>DeleteMany</code> functions. The difference is that <code>DeleteOne</code> only deletes the first matching document, while <code>DeleteMany</code> deletes all. If you know that only one document can match this condition (for example, deleting it by ID), you should use <code>DeleteOne</code> as the database does not have to perform an exhaustive search.</p> <p>The deletion condition can be described by the syntax familiar to the search.</p> <p>Deletion is different from Entity Framework. Here, the entity does not have to be loaded; instead, we specify a filtering condition.</p> <pre><code>var deleteResult = collection.DeleteOne(x =&gt; x.Id == new ObjectId(\"...\"));\nConsole.WriteLine($\"Deleted: {deleteResult.DeletedCount} records\");\n</code></pre> <p>If you want to retrieve the deleted element, you can use <code>FindOneAndDelete</code>, which returns the deleted entity.</p>"},{"location":"lecture-notes/mongodb/#updating-documents","title":"Updating documents","text":"<p>Perhaps the most interesting feature of MongoDB is the update of documents. While the functionalities showed before (queries, inserts, deletions) are similar to most databases (either relational or NoSQL), MongoDB supports a much broader range of modification operations.</p> <p>There are two ways to change a document: replace the entire document with a new one or update its parts.</p>"},{"location":"lecture-notes/mongodb/#complete-document-replacement","title":"Complete document replacement","text":"<p>To replace a document completely, we need a filtering condition to specify which document we want to replace; and we need a new document.</p> <pre><code>var replacementProduct = new Product\n{\n    Name = \"Apple\",\n    Price = 890,\n    Categories = new[] { \"Fruit\" }\n};\nvar replaceResult = collection.ReplaceOne(x =&gt; x.Id == new ObjectId(\"...\"), replacementProduct);\nConsole.WriteLine($\"Updated: {replaceResult.ModifiedCount}\");\n</code></pre> <p>A single document is matched and replaces it with another document. The operation itself is atomic, that is, if it is interrupted, no half document is saved. You can use the <code>FindOneAndReplace</code> method to get the pre-swap document.</p> <p>Interesting</p> <p>It is also possible to change the document ID during update (the replacement document can have a different ID).</p>"},{"location":"lecture-notes/mongodb/#document-update-operators","title":"Document update operators","text":"<p>Document update operators can change the value of a document's fields atomically without replacing the entire document. We use the help of the <code>Builder&lt;T&gt;</code> to describe the modifying operations.</p> <p>Set your stock to a constant value:</p> <pre><code>collection.UpdateOne(\n    filter: x =&gt; x.Id == new ObjectId(\"...\"),\n    update: Builders&lt;Product&gt;.Update.Set(x =&gt; x.Stock, 5));\n</code></pre> <p>The first parameter of the <code>UpdateOne</code> function is the filter condition. You can use any of the syntax described before. The second parameter is the descriptor of the update operation, which you can build with <code>Builders&lt;T&gt;</code>.</p> <p>In the example code above, the argument names are specified (<code>filter:</code> and <code>update:</code>) to make it clear what the parameter represents. This is optional, but it increases readability (at the expense of code length).</p> <p>The operation can update multiple fields at the same time.</p> <pre><code>collection.UpdateOne(\n    filter: x =&gt; x.Id == new ObjectId(\"...\"),\n    update: Builders&lt;Product&gt;.Update\n                .Set(x =&gt; x.Stock, 5)\n                .CurrentDate(x =&gt; x.StockUpdated)\n                .Unset(x =&gt; x.NeedsUpdate)\n);\n</code></pre> <p>Typical modifier operators are:</p> <ul> <li><code>Set</code>: Set the value of the field;</li> <li><code>SetOnInsert</code>: like <code>Set</code> but executed only when a new document is inserted (see upsert below);</li> <li><code>Unset</code>: delete field (remove key and value from document);</li> <li><code>CurrentDate</code>: set the current date;</li> <li><code>Inc</code>: increment value;</li> <li><code>Min</code>,<code>Max</code>: change the value of a field if the value entered is smaller / larger than the current value of the field;</li> <li><code>Mul</code>: value multiplication;</li> <li><code>PopFirst</code>,<code>PopLast</code>: remove first / last element from an array;</li> <li><code>Pull</code>: remove value from an array;</li> <li><code>Push</code>: add value to an array at the end (further options in the same operator: array sorting, keeping the first n element of an array);</li> <li><code>AddToSet</code>: add a value to an array if it does not already exist.</li> </ul> <p>The above operations are meaningful even if the specified field does not exist. Depending on the type of operator, the database will make changes to a default value. For example, for <code>Inc</code> and <code>Mul</code>, the field will be set to 0 and then modified. For array operations, an empty array is modified. For other operations, you can look up the behavior in the documentation.</p> <p>Multiple documents can be modified at the same time using this method. The requested update operations are performed on all documents that match the filter criteria.</p> <p>For example: in view of the summer season, put all balls on sale with a 25% discount.</p> <pre><code>collection.UpdateMany(\n    filter: Builders&lt;Product&gt;.Filter.AnyEq(x =&gt; x.Categories, \"Balls\"),\n    update: Builders&lt;Product&gt;.Update.Mul(x =&gt; x.Price, 0.75)\n                                   .AddToSet(x =&gt; x.Categories, \"On sale\"));\n</code></pre> <p>Update operators change the documents atomically. Using them can eliminate some of the problems caused by concurrent data access.</p>"},{"location":"lecture-notes/mongodb/#upsert-updating-or-inserting-a-document","title":"Upsert: updating or inserting a document","text":"<p>During update operations, we have the option to upsert (update/insert). This means that either an insertion or an update is made, depending on whether the item was in the database. The default behavior is not to upsert, we must request it explicitly.</p> <pre><code>collection.ReplaceOne(\n    filter: x =&gt; x.Id == new ObjectId(\"...\"),\n    replacement: replacementObject,\n    options: new UpdateOptions() { IsUpsert = true });\n</code></pre> <p>We can also do upsert with update operators. As we have seen, modifier operators are not concerned about missing fields. Likewise, it does not matter if the document does not exist; this is equivalent to performing a modifying operation on a completely blank document.</p> <pre><code>collection.UpdateOne(filter: ..., update: ..., options: new UpdateOptions() { IsUpsert = true });\n</code></pre> <p>The upsert operation can be a workaround for managing concurrency in the absence of a transaction. Because we do not have a transaction, we cannot verify before insertion that a particular record does not yet exist. Instead, we can use the upsert method, which allows atomic querying and insertion/modification.</p> <p><code>merge</code></p> <p>Note: In SQL, the <code>merge</code> command provides a similar solution.</p>"},{"location":"lecture-notes/mssql/server-side-programming/","title":"Microsoft SQL Server programming","text":""},{"location":"lecture-notes/mssql/server-side-programming/#microsoft-sql-server-programming","title":"Microsoft SQL Server programming","text":"<p>The language of the Microsoft SQL Server platform is T-SQL. The T-SQL language is platform-specific, meaning the language can only be used in MSSQL server - although other platforms have similar languages. The T-SQL language and the database server-side programming tools it supports extend the originally declarative SQL language with imperative tools such as variables, branches, procedures, and additional tools such as triggers and cursors.</p>"},{"location":"lecture-notes/mssql/server-side-programming/#server-side-programming","title":"Server-side programming","text":"<p>Server-side programming</p> <p>By server-side or database server-side programming, we mean that we execute not only commands to query and modify data in the database, but also carry out business logic inside the database.</p> <p>To understand when it is worthwhile to use server-side programming tools, it is first important to understand why we would consider writing business logic in the database at all.</p> <p>Why would we want to implement business logic tasks in the database?</p> <p>In a layered architecture, the lower layer provides services to the layer above it. So the upper layer \"can't get around\" the layer below; the operations have to go through the lower layer. But when you consider C#/Java/C++/ etc. code, we may not be able to guarantee such rules in the codebase. If we implement a complex set of rules and logic in a C# class, for example, it is difficult to guarantee that this class cannot be \"bypassed.\"</p> <p>However, if the logic is in the database, it cannot be bypassed or circumvented. This will also be due to the fact that server-side programming gives us tools that ensure the execution of certain logic under all circumstances (see triggers later).</p> <p>There are advantages and disadvantages to server-side programming. When considering the implementation of a functionality, in addition to knowing the layered architecture, we also need to look at what the technologies allow and which of the possible alternatives has the most benefits.</p> <p>If we implement business functionality in the database, we ensure the following benefits.</p> <ul> <li> <p>The responsibility of the database for managing consistency becomes even more evident. The relational model places great emphasis on consistency, but not all business consistency rules can be described directly in the relational model. Just think of the example of the Neptune system, where courses have an enrollment limit. This is a business rule, and if we break it, our data is inconsistent in the business sense. If the database is responsible for complying with this rule, we can ensure that the data is always consistent.</p> </li> <li> <p>We can reduce data traffic going out of the database. We often query data to display it to the user, which we cannot reduce. But if we query data only to make a decision based on it in the business logic layer, it is possible to avoid transferring the data between the database and the business logic if we bring the logic into the database instead. This is also more secure because no data is sent over the network unnecessarily (where sensitive data may be intercepted or outputted into error messages and log files by accident).</p> </li> <li> <p>The logic written in the database server can also be thought of as an interface that hides the details of data access and modification from the user (here: data access layer or business logic layer). On the one hand, this provides us with a level of abstraction, and on the other hand, it can aid parallel, faster development. While one development team builds the logic in the database, another team can write the application on top of it because the interface is defined earlier. Fixing errors is also more straightforward when the error is in the database. In this case, it is enough to fix the code in the database. Any system built on top of it will work correctly right away (unlike fixing a bug in Java code, because then a new version of the Java application has to be released and installed too).</p> </li> </ul> <p>Of course, there are disadvantages to server-side programming.</p> <ul> <li> <p>The language we use is platform-dependent. We cannot transfer solutions from one database system to another. Moreover, programming knowledge itself is not easily transferable. A C++ programmer can code in C# more quickly than if he did not have such knowledge. But this is not true for server-side programming. One platform does not support the same tools as the other. The syntax of the languages also differs significantly. Database server-side programming requires an entirely new approach and different techniques.</p> </li> <li> <p>The load of the database server is increased. If a server performs more tasks, it will require more resources. Databases are critical points of data-driven systems, primarily since classical relational databases do not support horizontal scaling too well (load balancing between multiple servers). If the database server is responsible for more tasks, it can quickly become the bottleneck.</p> </li> <li> <p>These techniques are no longer evolving. We might even call them outdated used only in legacy applications. This server-side world is less common nowadays in software development projects.</p> </li> </ul>"},{"location":"lecture-notes/mssql/server-side-programming/#basics-of-the-t-sql-language","title":"Basics of the T-SQL language","text":"<p>The T-SQL language is the language of Microsoft SQL Server, which, in addition to the standard SQL statements, allows you to:</p> <ul> <li>use variables,</li> <li>write branches and cycles,</li> <li>create stored procedures (\"methods\"),</li> <li>use cursors (iterators),</li> <li>define triggers (event-handling procedures),</li> <li>and much more.</li> </ul> <p>Let\u2019s look at the syntax of the language through examples. See the official documentation for the detailed syntax.</p> <p>The following examples can be executed on the sample database.</p>"},{"location":"lecture-notes/mssql/server-side-programming/#variables","title":"Variables","text":"<p>Variables must be declared before use. By convention, variable names begin with <code>@</code>. Uninitialized variables are all <code>NULL</code>.</p> <pre><code>DECLARE @num int\n\nSELECT @num\n-- NULL\n</code></pre> <p>Value assignment is possible with the <code>SET</code> statement or directly in the declaration:</p> <pre><code>DECLARE @num int = 5\n\nSELECT @num\n-- 5\n\nSET @num = 3\n\nSELECT @num\n-- 3\n</code></pre> <p>The scope of the variable is not bound to the instruction block (between <code>BEGIN-END</code>). The variable is available within the so-called batch or stored procedure:</p> <pre><code>BEGIN\n  DECLARE @num int\n  SET @num = 3\nEND\n\nSELECT @num\n-- This works, the variable is also available outside the instruction block.\n-- 3\n\nGO -- starts a new batch\n\nSELECT @num\n-- Error: Must declare the scalar variable \"@num\".\n</code></pre> <p>You can also assign value to a variable via a query:</p> <pre><code>DECLARE @name nvarchar (max)\n\nSELECT @name = Name\nFROM Customer\nWHERE ID = 1\n</code></pre> <p>If the query returns more than one row, the last value remains in the variable:</p> <pre><code>DECLARE @name nvarchar (max)\n\nSELECT @name = Name\nFROM Customer\n-- there are multiple  matching rows\n-- the last result of SELECT is stored in the variable\n</code></pre> <p>If the query does not yield any result, the value of the variable does not change:</p> <pre><code>DECLARE @name nvarchar (max)\nSET @name = 'aaa'\n\nSELECT @name = Name\nFROM Customer\nWHERE ID = 99999999\n-- no matching row\n\nSELECT @name\n-- aaa\n</code></pre>"},{"location":"lecture-notes/mssql/server-side-programming/#instruction-blocks-and-control-structures","title":"Instruction blocks and control structures","text":"<p>An instruction block is written between <code>BEGIN-END</code> commands:</p> <pre><code>BEGIN\n  DECLARE @num int\n  SET @num = 3\nEND\n</code></pre> <p>Branching is possible by using the <code>IF-ELSE</code> structure:</p> <pre><code>DECLARE @name nvarchar (max)\n\nSELECT @name = Name\nFROM Customer\nWHERE ID = 123\n\nIF @name IS NOT NULL -- If the user exists\nBEGIN\n  PRINT 'Updating email'\n  UPDATE Customer\n    SET Email = 'agh*******@gmail.com'\n    WHERE ID = 123\nEND\nELSE\nBEGIN\n  PRINT 'No such customer'\nEND\n</code></pre> <p>We use the <code>WHILE</code> condition and a <code>BEGIN-END</code> statement block for looping:</p> <pre><code>-- Generate at least 1000 products (e.g., for testing)\nWHILE (SELECT COUNT (*) FROM Product) &lt;1000\nBEGIN\n    INSERT INTO Product (Name, Price, Stock, VATID, CategoryID)\n    VALUES ('Abc', 1, 1, 3, 13)\nEND\n</code></pre>"},{"location":"lecture-notes/mssql/server-side-programming/#built-in-functions","title":"Built-in functions","text":"<p>There are numerous built-in functions are available in T-SQL. Below are a few examples.</p> <p>In the following examples, the results of the functions are queried with <code>select</code>. This is for the sole purpose of seeing the result. A function can be used anywhere in the language where a scalar value can be used.</p> <p>String functions:</p> <pre><code>-- Concatenation\nSELECT CONCAT('Happy ', 'Birthday!')\n-- Happy Birthday!\n\n-- N characters from the left\nSELECT LEFT('ABCDEF', 2)\n-- AB\n\n-- Text length\nSELECT LEN('ABCDEF')\n-- 6\n\n-- Substring replacement\nSELECT REPLACE('Happy Birthday!', 'day', 'month')\n-- Happy Birthmonth!\n\n-- Lowercase conversion\nSELECT LOWER('ABCDEF')\n-- abcdef\n</code></pre> <p>Manage dates:</p> <pre><code>-- Current date and time\nSELECT GETDATE()\n-- 2021-09-28 10: 43: 59.120\n\n-- Date's year component\nSELECT YEAR(GETDATE ())\n-- 2021\n\n-- Specific component of the date\nSELECT DATEPART(day, '12 / 20/2021 ')\nSELECT DATEPART(month, '12 / 20/2021 ')\n-- 20\n-- 12\n\n-- Difference between dates measured in a given unit (here: day)\nSELECT DATEDIFF(day, '2021-09-28 12:10:09', '2021-11-04 13:45:09')\n-- 37\n</code></pre> <p>Data type conversion:</p> <pre><code>SELECT CAST('12' as int)\n-- 12\n\nSELECT CONVERT(int, '12')\n-- 12\n\nSELECT CONVERT(int, 'aa')\n-- Error: Conversion failed when converting the varchar value 'aa' to data type int.\n\nSELECT TRY_CONVERT(int, 'aa')\n-- NULL\n</code></pre> <p><code>ISNULL</code>: result is the first argument if it is not null, otherwise the second argument (which can be null).</p> <pre><code>DECLARE @a int\nDECLARE @b int = 5\nSELECT ISNULL(@a, @b)\n-- 5\n</code></pre> <p>Not to be confused with the <code>is null</code> condition, e.g., <code>UPDATE Product SET Price = 111 WHERE Price is null</code></p>"},{"location":"lecture-notes/mssql/server-side-programming/#cursors","title":"Cursors","text":"<p>A cursor is an iterator used to scroll through a set of records item by item. We use it when a query returns multiple items, and we want to process them individually.</p> <p>Using a cursor consists of the following steps:</p> <ol> <li>The cursor must be declared and then opened.</li> <li>The iteration takes place in a cycle.</li> <li>The cursor is closed and released.</li> </ol>"},{"location":"lecture-notes/mssql/server-side-programming/#declaration-and-opening","title":"Declaration and opening","text":"<p>A cursor is created with the <code>DECLARE</code> statement. We also provide the query yielding the results in the declaration. The full syntax is:</p> <pre><code>DECLARE cursor name CURSOR\n  [FORWARD_ONLY | SCROLL]\n  [STATIC | KEYSET DYNAMIC FAST_FORWARD]\n  [READ_ONLY | SCROLL_LOCKS | OPTIMISTIC]\nFOR query\n[FOR UPDATE [OF column name [, ... n]]]\n</code></pre> <p>The meaning of optional flags in the declaration are (for more details, see the documentation):</p> <ul> <li><code>FORWARD_ONLY</code>: only <code>FETCH NEXT</code> is possible</li> <li><code>SCROLL</code>: you are free to move forward and backward in the cursor</li> <li><code>STATIC</code>: works from a copy: the results are snapshotted when opening the cursor</li> <li><code>KEYSET</code>: the database state at opening the cursor yields the row ids and their order, but the contents of the records are queried when fetching them</li> <li><code>DYNAMIC</code>: each fetch gets up to date data; allows access to changes of competing transactions</li> <li><code>READ_ONLY</code>: the contents of the cursor cannot be updated</li> <li><code>SCROLL_LOCKS</code>: fetching locks the rows, thus guaranteeing that any subsequent <code>update</code> or <code>delete</code> statement is successful</li> <li><code>OPTIMISTIC</code>: does not lock, uses optimistic concurrency management (to check for any changes between the time of <code>FETCH</code> and subsequent <code>update</code>)</li> <li><code>FOR UPDATE</code>: list of columns that can be updated</li> </ul> <p>The declaration is not enough to use use the cursor; it must be opened with the <code>OPEN</code> command. The pair of <code>OPEN</code> is the <code>CLOSE</code> command ending the use of the cursor. After closing, the cursor can be reopened, so we need to indicate when we no longer use the cursor; this is the <code>DEALLOCATE</code> command. (Typically, <code>CLOSE</code> and <code>DEALLOCATE</code> follow each other because we only use the cursor once.)</p>"},{"location":"lecture-notes/mssql/server-side-programming/#advancing-the-cursor","title":"Advancing the cursor","text":"<p>The current element of the cursor is accessed by \"copying\" the values \u200b\u200binto local variable(s) using the <code>FETCH</code> command. The variables used here must be declared in advance. The <code>FETCH</code> statement typically get the following element (<code>FETCH NEXT</code>), but if the cursor is not <code>FORWARD_ONLY</code>, you can move back and forward too:</p> <pre><code>FETCH\n  [NEXT | PRIORITY FIRST | LAST\n      | ABSOLUTE {n | @nvar}\n      | RELATIVE {n | @nvar}\n  ]\nFROM cursor_name\nINTO @variable_name [, ... n]\n</code></pre> <p>We can determine whether the <code>FETCH</code> statement was successful by querying the implicit variable <code>@@FETCH_STATUS</code>. The value of the variable <code>@@FETCH_STATUS</code> is:</p> <ul> <li>0 for a successful FETCH,</li> <li>-1 for a failed FETCH,</li> <li>-2 if the requested row is missing (when using <code>KEYSET</code>).</li> </ul> <p>The complete iteration thus requires two <code>FETCH</code> statements and one <code>WHILE</code> loop:</p> <pre><code>-- declare, open ...\nFETCH NEXT FROM cur INTO @var1, @var2\nWHILE @@FETCH_STATUS = 0\nBEGIN\n  -- ... custom logic\n  FETCH NEXT FROM cur INTO @var1, @var2\nEND\n</code></pre> <p>Note that the <code>FETCH</code> statement appears twice here. This is because the first one outside of the loop is used to query the very first record, and the second one inside the loop retrieves each additional record one at a time.</p>"},{"location":"lecture-notes/mssql/server-side-programming/#example","title":"Example","text":"<p>Let us see a complete example. Let us query products that have few items in stock left, and if the last sale was more than a year ago, discount the product price:</p> <pre><code>-- Extract the data from the cursor into these variables\nDECLARE @ProductName nvarchar(max)\nDECLARE @ProductID int\nDECLARE @LastOrder datetime\n\nDECLARE products_cur CURSOR SCROLL SCROLL_LOCKS -- Lock for guaranteed update\nFOR\n  SELECT Id, Name FROM Product WHERE Stock &lt; 3 -- Cursor query\nFOR UPDATE OF Price -- We also want to update the records\n\n-- Typical opening, fetch, loop\nOPEN products_cur\nFETCH FROM products_cur INTO @ProductID, @ProductName\nWHILE @@FETCH_STATUS = 0\nBEGIN\n\n  -- We can perform any operation in the cycle\n  -- Find the time of the last purchase\n  SELECT @LastOrder = MAX([Order].Date)\n    FROM [Order] JOIN OrderItem ON [Order].Id = OrderItem.OrderId\n    WHERE OrderItem.ProductID = @ProductId\n\n  -- Diagnostic display\n  PRINT CONCAT('ProductID:', convert(nvarchar, @ProductID), 'Last order:', ISNULL(convert(nvarchar, @LastOrder), 'No last order'))\n\n  IF @LastOrder IS NULL OR @LastOrder &lt; DATEADD(year, -1, GETDATE())\n  BEGIN\n    UPDATE Product\n      SET Price = Price * 0.75\n      WHERE CURRENT OF products_cur\n      -- Update current cursor record\n      -- Alternative: WHERE Id = @ProductID\n  END\n\n  -- Query next record and then go to the WHILE loop to verify if it was successful\n  FETCH FROM products_cur INTO @ProductID, @ProductName\nEND\n-- Stop using the cursor\nCLOSE products_cur\nDEALLOCATE products_cur\n</code></pre>"},{"location":"lecture-notes/mssql/server-side-programming/#stored-procedures-and-functions","title":"Stored procedures and functions","text":"<p>The codes written in the previous examples were sent to the server and executed immediately. We can also write code that is stored by the server and can be called at any later time. In a modular programming environment, we usually call these functions, and in an object-oriented world, we call them methods. In Microsoft SQL Server, these are called stored procedures and stored functions. Stored in the name indicates that the procedure code is stored in the database along with the data (and will be included in backups, for example).</p> <p>The difference between a procedure and a function is that procedures typically have no return value, while functions do. An additional restriction in the MSSQL platform is that functions can only read the database but not make changes.</p>"},{"location":"lecture-notes/mssql/server-side-programming/#procedures","title":"Procedures","text":"<p>You can create a stored procedure with the following syntax:</p> <pre><code>CREATE [OR ALTER] PROC[EDURE] procedure_name\n  [{@ parameter data_type}] [, ... n]\nAS\n[BEGIN]\n  sql_instructions [... n]\n[END]\n</code></pre> <p>The result of the <code>CREATE OR ALTER</code> statement is the creation of the stored procedure, if it does not exist, or else its update with the new contents. Prior to MSSQL Server 2016, there was no <code>CREATE OR ALTER</code>, only <code>CREATE PROC</code> and <code>ALTER PROC</code>. We can delete a stored procedure with the <code>DROP PROCECURE</code> statement, which removes the procedure from the server.</p> <p>For example, Let us create a new tax percentage record in the <code>VAT</code> table, guaranteeing that only unique percentages can be added:</p> <pre><code>create or alter procedure InsertNewVAT -- create a stored procedure\n    @Percentage int -- stored procedure parameters\nas\n  begin\n  -- this is where the code begins, which the system executes when the procedure is called\n  begin tran -- to avoid non-repeatable reading\n  set transaction isolation level repeatable read\n\n  declare @Count int\n\n  select @Count = count(*)\n  from VAT\n  where Percentage = @Percentage\n\n  if @Count = 0\n      insert into VAT values \u200b\u200b(@Percentage)\n  else\n      print 'error';\n\ncommit\nend\n</code></pre> <p>The stored procedure is created by executing the former command, and then it can be called as follows:</p> <pre><code>exec InsertNewVAT 27\n</code></pre> <p>Stored procedures are part of our database. For example, in Microsoft SQL Server Management Studio, it is visible here:</p> <p></p>"},{"location":"lecture-notes/mssql/server-side-programming/#scalar-functions","title":"Scalar functions","text":"<p>The declaration of a function is similar to a procedure, but we must also specify the return type:</p> <pre><code>CREATE [OR ALTER] FUNCTION name\n([{@ parameter data_type}] [, ... n])\nRETURNS data type\n[ AS ]\nBEGIN\n  instructions\n  RETURN scalar_value\nEND\n</code></pre> <p>Let us see a function with return value <code>int</code> that has no input parameters:</p> <pre><code>CREATE OR ALTER FUNCTION LargestVATPercentage()\nRETURNS int\nBEGIN\nRETURN (SELECT MAX(Percentage) FROM VAT)\nEND\n</code></pre> <p>Here's how to use this function:</p> <pre><code>select dbo.LargestVATPercentage()\n-- The dbo prefix is \u200b\u200bthe name of the schema, indicating that this is not a built-in function\n-- Without this, the function is not found\n\n-- or for example\nDECLARE @maxvat int = dbo.LargestVATPercentage()\nselect @maxvat\n</code></pre>"},{"location":"lecture-notes/mssql/server-side-programming/#table-functions","title":"Table functions","text":"<p>A function can also yield a table as the result. In this case, the declaration looks like this:</p> <pre><code>CREATE [OR ALTER] FUNCTION name\n([{@ parameter data type}] [, ... n])\nRETURNS TABLE\n[ AS ]\nRETURN select statement\n</code></pre> <p>For example, consider retrieving VAT rates above a certain percentage:</p> <pre><code>CREATE FUNCTION VATPercentages(@min int)\nRETURNS TABLE\nAS RETURN\n(\n    SELECT ID, Percentage FROM VAT\n    WHERE Percentage &gt; @min\n)\n</code></pre> <p>This function returns a table, so you can use the function anywhere a table can appear, for example:</p> <pre><code>SELECT * FROM VATPercentages(20)\n</code></pre> <p>Since the function returns a table, we can even <code>join</code> it:</p> <pre><code>SELECT VAT.Percentage, count(*)\nFROM VAT JOIN VATPercentages(20) p on VAT.ID = p.Id\nGROUP BY VAT.Percentage\n</code></pre>"},{"location":"lecture-notes/mssql/server-side-programming/#error-handling","title":"Error handling","text":"<p>In the stored procedure example, we wanted to prevent duplicate records from being inserted into a table. This was accomplished above by not executing the instruction. However, it would be more appropriate to report the error to the caller. This is what structured error handling is about. In case of an error, you can use the <code>throw</code> command to raise an error. This command interrupts code execution and returns control to the caller (where the error can be handled or passed on). The error has a number (between 50000 and 2147483647), a text, and an error status identifier between 0-255.</p> <p>The updated procedure for recording the VAT key looks like this:</p> <pre><code>create or alter procedure InsertNewVAT\n    @Percentage int\nas\nbegin\n\n  begin tran\n  set transaction isolation level repeatable read\n\n  declare @Count int\n\n  select @Count = count(*)\n  from VAT\n  where Percentage = @Percentage\n\n  if @Count = 0\n      insert into VAT values \u200b\u200b(@Percentage)\n  else\n      throw 51000, 'error', 1;\n\n  commit\nend\n</code></pre> <p>To handle (catch) an error, you can use the following syntax:</p> <pre><code>begin try\n  exec InsertNewVAT 27\nend try\nbegin catch\n  -- access the error details with the following functions (similar to stack trace in other languages)\n  SELECT\n    ERROR_NUMBER () AS ErrorNumber,\n    ERROR_SEVERITY () AS ErrorSeverity,\n    ERROR_STATE () AS ErrorState,\n    ERROR_PROCEDURE () AS ErrorProcedure,\n    ERROR_LINE () AS ErrorLine,\n    ERROR_MESSAGE () AS ErrorMessage;\nend catch\n</code></pre> <p>Of course, it's not just user code that can throw errors. The system also signals errors identically, and we can handle them using the same tools.</p>"},{"location":"lecture-notes/mssql/server-side-programming/#triggers","title":"Triggers","text":"<p>The tools and language elements described so far have similar counterparts in other platforms. However, triggers are unique to databases. Triggers are event-handling stored procedures. We can subscribe to various events in the database, and when the event occurs, the system will execute our code defined in the trigger.</p> <p>We will only discuss DML triggers. These are triggers that run due to data modification (<code>insert</code>, <code>update</code>, <code>delete</code>) operations. There are other triggers as well; e.g., you can create triggers for system events. Check the official documentation for more details.</p>"},{"location":"lecture-notes/mssql/server-side-programming/#dml-triggers","title":"DML triggers","text":"<p>Using triggers, we can solve several tasks that would be difficult otherwise. Consider, for example, an audit logging requirement: when a change is made to a particular table, let us record a log entry. We could solve this task in C#/Java/Python by creating a class or methods for accessing the database table in question. However, nothing prevents the programmer from \"bypassing\" this logic and accessing the database directly. We cannot prevent this with triggers, but we can create a trigger that performs the required logging instead of the C#/Java/Python code.</p> <p>Let us look at this example: logging the deletion of any products in a dedicated table:</p> <pre><code>-- Create the auditing table\ncreate table AuditLog([Description] [nvarchar](max) NULL)\ngo\n\n-- Logging trigger\ncreate or alter trigger ProductDeleteLog\n  on Product\n  for delete\nas\ninsert into AuditLog (Description)\nselect 'Product deleted: ' + convert(nvarchar, d.Name) from deleted d\n</code></pre> <p>Executing the commands above creates a trigger in the database (just as a stored procedure is created). This trigger is then executed automatically. So the trigger is not called by us but by the system. Nevertheless, we give the trigger a name to reference it (e.g., if we want to delete it with the <code>DROP TRIGGER</code> statement). The trigger is linked to the table in the database:</p> <p></p> <p>The syntax for defining a DML trigger is as follows:</p> <pre><code>CREATE TRIGGER trigger_name\nON { table | view }\n FOR {[DELETE] [,] [INSERT] [,] [UPDATE]}\nAS\nsql_instruction [... n]\n</code></pre> <p>Note that in the trigger definition, we specify the table or view. So a trigger listens for events of a single table. The events are set by listing the requested modifying operations (e.g., <code>for update, insert</code>). Note that three possible options cover all types of changes; also note, that there is no <code>select</code> event \u2014 since it is not a change.</p> <p>The instructions defined in the trigger code are executed after the specified events occur. This means that the changes are already performed (for example, new rows are already inserted into the table), but the transaction of the operation is not yet finished. Thus, we can make further changes as part of the same transaction (and consequently, seeing the result of the \"original\" command and the trigger as an atomic change) or even aborting the transaction. A particular use case for triggers is to check the consistency of data (that cannot be verified otherwise) and to abort the modification in the event of a violation. We will see an example of this soon.</p> <p>Triggers are executed per instruction, which means they are called once per DML operation. In other words, the trigger does not handle the changes per row; instead, all changes caused by a single operation are handled at once. So, for example, if an <code>update</code> statement changes 15 rows, the trigger is called once, and we will see all 15 changes. Of course, this is also true for inserting and deleting - a deletion operation can delete multiple rows, and we can insert multiple records with a single insert command.</p> <p>There is no row-level trigger</p> <p>Other database platforms have row-level triggers, where the trigger is called individually for all the modified rows. Microsoft SQL Server platform does not have such a trigger!</p> <p>How do we know what changes are handled in the trigger? Inside the trigger, we have access to two log tables through the implicit variables <code>inserted</code> and <code>deleted</code>. The structure of these tables is identical to the table on which the trigger is defined. These tables exist only during the trigger execution and can only be accessed from within the trigger. Their content depends on the type of operation that invoked the trigger:</p> insert delete update inserted new records empty new values of records deleted empty deleted records old values \u200b\u200bof records <p>When inserting, the inserted records can be found in the database table (but there, we do not \"see\" that they have been newly inserted), and they are also available in the <code>inserted</code> table. In the case of deletion, <code>deleted</code> contains the rows already deleted from the table. Finally, in the case of <code>update</code>, we see the states before and after the change in the two log tables. We need to work with these log tables as tables; we should always expect to have more than one record in them.</p> <p>The <code>inserted</code> and <code>deleted</code> are tables</p> <p>The <code>inserted</code> and <code>deleted</code> tables can only be treated as tables! For example, it does not make sense to use <code>select @id=inserted.ID</code>; instead, we can use a cursor on these tables or <code>join</code> them.</p> <p>We have already seen an example of audit logging implemented with a trigger. Let us look at other use-cases. Let us have a table with an email address column. When inserting and modifying, we need to check the email address value, and we must not accept text that does not look like an email address. Here we validate a rule of consistency with the trigger.</p> <pre><code>-- Create a function to check the email address\nCREATE FUNCTION [IsEmailValid](@ email nvarchar(1000))\nRETURNS bit -- true / false return value\nAS\nBEGIN\n  IF @email is null RETURN 0 -- Cannot be null\n  IF @email = '' RETURN 0 -- Cannot be an empty string\n  IF @email LIKE '%_@%_._%' RETURN 1 -- Looks like an email\n  RETURN 0\n  -- The same in one line:\n  -- RETURN CASE WHEN ISNULL(@email, '') &lt;&gt; '' AND @email LIKE '%_@%_._%' THEN 1 ELSE 0 END\nEND\n\n-- The trigger\ncreate or alter trigger CustomerEmailSyntaxCheck\n  on Customer\n  for insert, update -- Check both inserting and modifying\nas\n-- For both insertion and modification, the new data is in the inserted table\n-- Is there an item there for which the new email address is not valid?\nif exists(select 1 from inserted i where dbo.IsEmailValid(i.Email) = 0)\n  throw 51234, 'invalid email address', 1 -- abort the transaction by raising the error\n</code></pre> <p>The above trigger runs after insertion or modification in the same transaction. So if we throw an error, the transaction will be aborted (unless handled by the caller). By running the trigger at the instruction level, a single faulty record interrupts the entire operation. Of course, this is what we expect due to atomicity: the indivisibility of the transaction is satisfied for the instruction as a whole, i.e., for inserting/modifying several records at once.</p> <p>Another common use of triggers is maintenance of denormalized data. Although we try to avoid denormalization in a relational database, in practice, it may be necessary to store computed data for performance reasons. Let us look at an example of this as well. Suppose customers have two email addresses: one to sign in with, an optional second one to use for notifications. To avoid always having to query both email addresses and choosing between the two, let us make sure the effective email address is available in the database \"calculated\" from the previous two:</p> <pre><code>-- Additional email address columns for customers\nalter table Customer\nadd [NotificationEmail] nvarchar(max), [EffectiveEmail] nvarchar(max)\ngo\n\n-- Trigger to update the effective email address\ncreate or alter trigger CustomerEmailUpdate\n  on Customer\n  for insert, update\nas\nupdate Customer -- We modify the Customer table, not the inserted implicit table\nset EffectiveEmail = ISNULL(i.NotificationEmail, i.Email) -- Copy one or the other value to the EffectiveEmail column\nfrom Customer c join inserted i on c.ID = i.ID -- Records must be retrieved from the Customer table based on the inserted rows\n</code></pre> <p>Trigger recursion</p> <p>Note that in this trigger, an update is executed in response to an update event. This is a recursion. Recursion of DML triggers is disabled by default, so the above example does not invoke trigger recursion. However, if trigger recursion were enabled in the database, we would need to handle it.</p> <p>Let us look at another example of denormalized data maintenance. In the order table, let us add a grand total column, which is the total net price of the order. We need a trigger to keep the value updated automatically:</p> <pre><code>create or alter trigger OrderTotalUpdateTrigger\n  on OrderItem\n  for insert, update, delete\nas\n\nupdate Order\nset Total = isnull(Total,0) + TotalChange\nfrom Order inner join\n        (select i.OrderID, sum(Amount*Price) as TotalChange\n        from inserted i\n        group by i.OrderID) OrderChange\n    on Order.ID = OrderChange.OrderID\n\nupdate Order\nset Total = isnull(Total,0) \u2013 TotalChange\nfrom Order inner join\n        (select d.OrderID, sum(Amount*Price) as TotalChange\n        from deleted d\n        group by d.OrderID) OrderChange\n    on Order.ID = OrderChange.OrderID\n</code></pre> <p>In this trigger, it is worth noting that while the event occurs in the <code>OrderItem</code> table, the content to be updated is in the <code>Order</code> table. This is fine, a trigger can read and write any part of the database, and all changes are executed in the same transaction. Furthermore, we do not recalculate the total amount in the trigger but alter it in response to the changes. Although this makes the trigger code more complex, it is more effective this way.</p> <p>Sequence of triggers</p> <p>We can define multiple triggers for an event. But the order of their execution cannot be specified. We can set the first and last triggers, but we cannot make assumptions regarding their sequence otherwise - it is considered ill-advised to design functionality where triggers need to build on each other.</p>"},{"location":"lecture-notes/mssql/server-side-programming/#instead-of-triggers","title":"Instead of triggers","text":"<p>A special type of trigger is the so-called instead of trigger. Such triggers can be defined for both tables and views. Let us look at using them on tables first. An instead of trigger defined on a table, as its name suggests, runs the instruction we define in the trigger instead of the actual operation's <code>insert / update / delete</code>. E.g., when inserting, the new rows are not added to the table, and when deleting, rows are not deleted. Instead, we can define in the trigger how to perform these operations. In the overridden process, we can access the table itself and execute the necessary actions in this table. These operations do not cause recursion in the trigger. These triggers can be considered as before triggers, i.e., we can perform checks before making the changes and abort the operation in case of an error.</p> <p>A typical use case for an instead of trigger is, for example, when we do not want to perform a deletion. This is also called soft delete:,instead of deleting, we only mark the records as deleted:</p> <pre><code>-- Soft delete flag column in the table with a default value of 0 (i.e., false)\nalter table Product\nadd [IsDeleted] bit NOT NULL CONSTRAINT DF_Product_IsDeleted DEFAULT 0\ngo\n\n-- Instead of trigger, the delete command does not perform the deletion\n-- the following code runs instead\ncreate or alter trigger ProductSoftDelete\n  on Product\n  instead of delete\nas\nupdate Product\n  set IsDeleted = 1\n  where ID in (select ID from deleted)\n</code></pre> <p>Another typical use case for instead of triggers is views. A view is the result of a query, so inserting new data into the view does not make sense. However, you can use an instead of trigger to define what to do instead of \"inserting into view.\" Let us look at an example. In the view below, we combine data from the product and VAT tables so that the VAT percentage is displayed in the view instead of the ID of the referenced VAT record. We can insert into this view by inserting the data into the product table instead:</p> <pre><code>-- Define the view\ncreate view ProductWithVatPercentage\nas\nselect p.Id, p.Name, p.Price, p.Stock, v.Percentage\nfrom Product p join Vat v is p.VATID = v.Id\n\n-- Instead of trigger for the view\ncreate or alter trigger ProductWithVatPercentageInsert\non ProductWithVatPercentage\ninstead of insert\nas\n  -- The insertion goes into the Product table: a new row is created for each inserted record\n  -- And we find the VAT record corresponding to the provided percentage\n  -- The solution is not complete because it does not handle if there is no matching VAT record\n  insert into Product(Name, Price, Stock, VATID, CategoryID)\n  select i.Name, i.Price, i.Stock, v.ID, 1\n    from inserted i join VAT v on v.Percentage = i.Percentage\n\n-- The trigger can be tested by inserting data into the view\ninsert into ProductWithVatPercentage(Name, Price, Stock, Percentage)\nvalues ('Red ball', 1234, 22, 27)\n</code></pre>"},{"location":"lecture-notes/mssql/sql/","title":"SQL language, MSSQL platform-specific SQL","text":""},{"location":"lecture-notes/mssql/sql/#sql-language-mssql-platform-specific-sql","title":"SQL language, MSSQL platform-specific SQL","text":"<p>You can run these queries on the sample database.</p>"},{"location":"lecture-notes/mssql/sql/#simple-queries","title":"Simple queries","text":"<p>Which product costs less than 2000 and have less than 50 in stock?</p> <pre><code>select Name, Price, Stock\nfrom Product\nwhere Price&lt;2000 and Stock&lt;50\n</code></pre> <p>Which product has no description?</p> <pre><code>select *\nfrom Product\nwhere Description is null\n</code></pre>"},{"location":"lecture-notes/mssql/sql/#joining-tables","title":"Joining tables","text":"<p>Customers with a main site in Budapest (the two alternatives are equivalent).</p> <pre><code>select *\nfrom Customer c, CustomerSite s\nwhere c.MainCustomerSiteID=s.ID and City='Budapest'\n\nselect *\nfrom Customer c inner join CustomerSite s on c.MainCustomerSiteID=s.ID\nwhere City='Budapest'\n</code></pre> <p>List the products that start with letter M, the ordered amounts and deadlines. Include the products that have not been ordered yet.</p> <pre><code>select p.Name, sum(oi.Amount)\nfrom Product p\n     left outer join OrderItem oi on p.id=oi.ProductID\nwhere p.Name like 'M%'\ngroup by p.Name\n</code></pre>"},{"location":"lecture-notes/mssql/sql/#sorting","title":"Sorting","text":"<pre><code>select *\nfrom Product\norder by Name\n</code></pre> <p>Microsoft SQL Server specific: collation specifies the rules for sorting</p> <pre><code>select *\nfrom Product\norder by Name collate SQL_Latin1_General_Cp1_CI_AI\n</code></pre> <p>Sort by multiple fields</p> <pre><code>select *\nfrom Product\norder by Stock desc, Price\n</code></pre>"},{"location":"lecture-notes/mssql/sql/#subqueries","title":"Subqueries","text":"<p>List the order dates, deadlines and Statuses</p> <pre><code>select o.Date, o.Deadline, s.Name\nfrom [Order] o inner join Status s on o.StatusId=s.ID\n</code></pre> <p>An alternative, but the two are not equivalent: the subquery is the equivalent of the left outer join and not the inner join!</p> <pre><code>select o.Date, o.Deadline,\n       (select s.Name\n        from Status s\n        where o.StatusId=s.ID)\nfrom [Order] o\n</code></pre> <p><code>[Order]</code></p> <p><code>[Order]</code> is in brackets, because this signals that this is a table name and not the beginning of the <code>order by</code> SQL language element.</p>"},{"location":"lecture-notes/mssql/sql/#filter-duplicates","title":"Filter duplicates","text":"<p>Which products have been ordered in batches of more than 3? One product may have been ordered multiple times, but we want the name only once.</p> <pre><code>select distinct p.Name\nfrom Product p inner join OrderItem oi on oi.ProductID=p.ID\nwhere oi.Amount&gt;3\n</code></pre>"},{"location":"lecture-notes/mssql/sql/#aggregate-functions","title":"Aggregate functions","text":"<p>How much is the most expensive product?</p> <pre><code>select max(Price)\nfrom Product\n</code></pre> <p>Which are the most expensive products?</p> <pre><code>select *\nfrom Product\nwhere Price=(select max(Price) from Product)\n</code></pre> <p>What was the min, max and average selling price of each product with name containing Lego having an average selling price more than 10000</p> <pre><code>select p.Id, p.Name, min(oi.Price), max(oi.Price), sum(oi.Price*oi.Amount)/sum(oi.Amount)\nfrom Product p\n     inner join OrderItem oi on p.ID=oi.ProductID\nWhere p.Name like '%Lego%'\ngroup by p.Id, p.Name\nhaving avg(oi.Price)&gt;10000\norder by 2\n</code></pre>"},{"location":"lecture-notes/mssql/sql/#inserting-records","title":"Inserting records","text":"<p>Inserting a single record by assigning value to all columns (except identity)</p> <pre><code>insert into Product\nvalues ('aa', 100, 0, 3, 2, null)\n</code></pre> <p>Set values of selected columns only</p> <pre><code>insert into Product (Name,Price)\nvalues ('aa', 100)\n</code></pre> <p>Insert the result of a query</p> <pre><code>insert into Product (Name, Price)\nselect Name, Price\nfrom InvoiceItem\nwhere Amount&gt;2\n</code></pre> <p>MSSQL specific: identity column</p> <pre><code>create table VAT\n(\n   ID int identity primary key,\n   Percentage int\n)\n\ninsert into VAT(Percentage)\nvalues (27)\n\nselect @@identity\n</code></pre> <p>MSSQL specific: setting the value of identity column</p> <pre><code>set identity_insert VAT on\n\ninsert into VAT (ID, Percentage)\nvalues (123, 27)\n\nset identity_insert VAT off\n</code></pre>"},{"location":"lecture-notes/mssql/sql/#updating-records","title":"Updating records","text":"<p>Raise the price of LEGOs by 10% and add 5 to stock</p> <pre><code>update Product\nset Price=1.1*Price,\n    Stock=Stock+5\nwhere Name like '%Lego%'\n</code></pre> <p>Update based on filtering by referenced table content: raise the price by 10% for those products that are subject to 20% VAT, and have more then 10 pcs in stock</p> <pre><code>update Product\nset Price=1.1*Price\nwhere Stock&gt;10\nand VATID in\n(\n    select ID\n    from VAT\n    where Percentage=20\n)\n</code></pre> <p>MSSQL Server specific solution to the same task</p> <pre><code>update Product\nset Price=1.1*Price\nfrom Product p\n     inner join VAT v on p.VATID=v.ID\nwhere Stock&gt;10\n      and Percentage=20\n</code></pre>"},{"location":"lecture-notes/mssql/sql/#deleting-records","title":"Deleting records","text":"<pre><code>delete\nfrom Product\nwhere ID&gt;10\n</code></pre>"},{"location":"lecture-notes/mssql/sql/#assigning-ranks","title":"Assigning ranks","text":"<p>Assigning ranks by ordering</p> <pre><code>select p.*,\n       rank() over (order by Name) as r,\n       dense_rank() over (order by Name) as dr\nfrom Product p\n</code></pre> <p>Ranking within groups</p> <pre><code>select p.*\n       ,rank() over (partition by CategoryID order by Name) as r\n       ,dense_rank() over (partition by CategoryID order by Name) as dr\nfrom Product p\n</code></pre> <p>Rank and dense_rank</p> <p>Unlike dense_rank , Rank skips positions after equal rankings. The number of positions skipped depends on how many rows had an identical ranking. For example, Mary and Lisa sold the same number of products and are both ranked as 1. With Rank,  the next position is 3; with dense_rank, the next position is 2.</p>"},{"location":"lecture-notes/mssql/sql/#cte-common-table-expression","title":"CTE (Common Table Expression)","text":"<p>Motivation: subqueries often make queries complex</p> <p>First three products sorted by name alphabetically</p> <pre><code>select *\nfrom\n(\n    select p.*\n            ,rank() over (order by Name) as r\n            ,dense_rank() over (order by Name) as dr\n    from Product p\n) a\nwhere a.dr&lt;=3\n</code></pre> <p>Same solution using CTE</p> <pre><code>with q1\nas\n(\n    select *\n           ,rank() over (order by Name) as r\n          ,dense_rank() over (order by Name) as dr\n    from Product\n)\nselect *\nfrom q1\nwhere q1.dr&lt;=3\n</code></pre> <p>How many pieces have been sold from the second most expensive product?</p> <pre><code>with q\nas\n(\n    select *\n            , dense_rank() over (order by Price desc) dr\n    from Product\n)\nselect q.ID, q.Name, sum(Amount)\nfrom q\n     inner join OrderItem oi on oi.ProductID=q.ID\nwhere q.dr = 2\ngroup by q.ID, q.Name\n</code></pre> <p>Paging: list products alphabetically from 3. to 8. record</p> <pre><code>with q\nas\n(\n    select *\n            , rank() over (order by Name) r\n    from Product\n)\nselect *\nfrom q\nwhere q.r between 3 and 8\n</code></pre> <p>Paging using MSSQL Server (2012+) specific syntax</p> <pre><code>select *\nfrom Product\norder by Name\noffset 2 rows\nfetch next 6 rows only\n\nselect top 3 *\nfrom Product\norder by Name\n</code></pre>"},{"location":"lecture-notes/mssql/sql/#querying-xml-documents","title":"Querying XML documents","text":"<p>In a relational database, in addition to relational data, semi-structured data (e.g., XML) can also be stored - but relational is the main content. For example, in the sample database, the <code>Description</code> field of the <code>Product</code> table is XML.</p>"},{"location":"lecture-notes/mssql/sql/#xpath","title":"XPath","text":"<p>An XML document has a tree structure. The XPath language allows navigating this tree and selecting specific content. The following table illustrates the capabilities of the XPath language.</p> XPath expression Meaning tagname Node with specified name / Search starts from the root // In a descendend at any level . Current node .. Parent node @name Specific attribute /library/book[k] The book at index k within the library node (indexes start at 1) /library/book[last()] Last child /library/book[position()&lt;k] The first k-1 child nodes //title[@lang=\"hu\"] Title elements that have lang attribute with value \"hu\" //title[text()] The text content of the title nodes /library/book[price&gt;5000] Books within the library node that have a price more than 5000 <p>XQuery and XPath</p> <p>XPath has many other capabilities in addition to the ones above, including expressing more complex queries.</p> <p>In the following examples, we will specify the data to be queried using XQuery. XQuery builds on XPath and adds additional functionality. Both XPath and XQuery are platform-independent languages \u200b\u200bbased on W3C standards.</p>"},{"location":"lecture-notes/mssql/sql/#queries","title":"Queries","text":"<p>Let us have a table with an XML column. In addition to querying the entire XML value, we can query content from within the XML document. In order to do this, we need to use T-SQL functions capable of working on the XML content: <code>query(XQuery)</code> , <code>value(XQuery, SQLType)</code> and <code>exist(XQuery)</code>. Let's look at a few examples of these.</p> <p>Let us query how many packages the products consist of.</p> <pre><code>select Description.query('/product/package_parameters/number_of_packages')\nfrom Product\n</code></pre> <p>For example, this could yield:</p> <pre><code>&lt;number_of_packages&gt;1&lt;/number_of_packages&gt;\n</code></pre> <p>The function <code>query()</code> returns XML; if it is only the value that is needed, we can use the <code>value()</code> function. The <code>value()</code> function must also specify the type of data queried as a string literal.</p> <pre><code>select Description.value('(/product/package_parameters/number_of_packages)[1]', 'int')\nfrom Product\n</code></pre> <p>The result will be 1.</p> <p>SQLType</p> <p>The type passed as a parameter cannot be xml. Conversion to the specified type is performed with the T-SQL <code>CONVERT</code> function.</p> <p>Let us query the names of the recommended products for ages 0-18 months.</p> <pre><code>select Name\nfrom Product\nwhere Description.exist('(/product)[(./recommended_age)[1] eq \"0-18 m\"]')=1\n</code></pre> <p>Function <code>exist()</code> returns 1 if the XQuery expression evaluation yields a non-empty result; or 0 if the query result is empty.</p> <p>We can also use the <code>value()</code> method instead of <code>exist()</code> here.</p> <pre><code>select Name\nfrom Product\nwhere Description.value('(/product/recommended_age)[1]', 'varchar(max)')='0-18 m'\n</code></pre>"},{"location":"lecture-notes/mssql/sql/#manipulating-queries","title":"Manipulating queries","text":"<p>We can not only query XML data, but also modify it in place. The modification in the database is performed in an atomic way, i.e., there is no need to fetch the XML into a client application, modify it and then write it back. Instead, following the philosophy of server-side programming, we bring the logic (here: modification) into the database. Data modification queries can be performed with the <code>modify(XML_DML)</code> function, where we use the so-called XML DML language to describe the desired change. Let's look at a few examples.</p> <p>In the product called Lego City harbor, let us change the recommended age to 6-99 years.</p> <pre><code>update Product\nset Description.modify(\n'replace value of (/product/recommended_age/text())[1]\nwith \"6-99 y\"')\nwhere Name='Lego City harbour'\n</code></pre> <p>The XML DML expression consists of two parts: in the first part (<code>replace value of</code>) the element to be modified is selected; in the second part (<code>with</code>) the new value is specified. Only one element can be modified within an XML, so the path must be specified to match only one element - thus the <code>[1]</code> at the end of the example.</p> <p>Let us insert a <code>weigth</code> tag into the XML description of product Lego City harbor after the <code>package_size</code> tag.</p> <pre><code>update Product\nset Description.modify(\n'insert &lt;weight&gt;2.28&lt;/weight&gt;\nafter (/product/package_parameters/package_size)[1]')\nwhere Name='Lego City harbour'\n</code></pre> <p>The expression has of two parts here too: the first one (<code>insert</code>) specifies the new element, and the second one describes where to insert the new element. The new item can be added as a sibling or child of the specified item.</p> <p>Let us remove the <code>description</code> tag(s) from the description of every product.</p> <pre><code>update Product\nset Description.modify('delete /product/description')\nwhere Description is not null\n</code></pre> <p>When deleting, we specify the path of the items to be deleted after <code>delete</code>.</p>"},{"location":"lecture-notes/transactions/","title":"Transactions in databases","text":""},{"location":"lecture-notes/transactions/#transactions-in-databases","title":"Transactions in databases","text":"<p>Context</p> <p>When we talk about transactions, we mean relational databases. The problem and the solutions, however, are generic and are not specific to relational databases.</p>"},{"location":"lecture-notes/transactions/#concurrent-data-access","title":"Concurrent data access","text":"<p>Database management systems are based on a client-server architecture. The client (the software we write) connects to the database and executes queries. We should always remember that there is a single database, but multiple clients involved here. The purpose of the database system is to serve as many requests as possible; consequently, it executes the queries concurrently. In such a concurrent system, data access can overlap in the following ways.</p> <ul> <li>If the concurrent data access (either read or write) concerns independent data, there is no problem, and the operations may proceed concurrently.</li> <li>If all operations only read data, there is no issue either; multiple readers can access the same data.</li> <li>However, if the same data is accessed simultaneously and there is at least one writer, a concurrency problem may manifest itself.</li> </ul> <p>This concurrency issue is analogous to the mutual exclusion problem known in operating systems and the various programming languages and frameworks. Concurrent data access in these scenarios usually involve mutual access to shared memory space, and the solution is ensuring mutual exclusion using some kind of guard.</p> <p>In database management systems, concurrency is related to the records (rows) of database tables, and the guards are transactions.</p>"},{"location":"lecture-notes/transactions/#transactions","title":"Transactions","text":"<p>Definition</p> <p>A transaction is a logical unit of a process, a series of operations that only make sense together.</p> <p>A transaction combines operations into one unit, and the system guarantees the following properties:</p> <ul> <li>atomic execution,</li> <li>consistency,</li> <li>isolation from each other,</li> <li>and durability.</li> </ul> <p>Let us examine these basic properties to understand how concurrent data access issues are resolved with their help.</p> <p>A transaction is just a tool</p> <p>A transaction, similarly to mutexes provided by an operating system or programming framework, is just a tool provided to the software developer. The proper usage is the responsibility of the developer.</p>"},{"location":"lecture-notes/transactions/#transactions-basic-properties","title":"Transactions basic properties","text":""},{"location":"lecture-notes/transactions/#atomicity","title":"Atomicity","text":"<p>Atomic execution means that we have a sequence of operations, and this sequence is meaningful only when all of it is executed. In other words, partial execution must be prohibited. In database systems, we often need multiple statements to achieve our goal, hence the sequence of steps.</p> <p>Let us imaging the checkout process in a webshop:</p> <ol> <li>The order is recorded in the database with the provided data</li> <li>The amount of stock is decreased by one since one piece was sold</li> </ol> <p>These steps only make sense together. Given that an order has been recorded, the amount of stock must be compensated; otherwise, the data becomes invalid, and we sell more products than we have. Thus, we must not abort the sequence of steps in the middle.</p> <p>This is what atomicity guarantees: if executing a sequence of steps has begun, all steps have to complete successfully or the initial state before the modification must be restored.</p>"},{"location":"lecture-notes/transactions/#consistency","title":"Consistency","text":"<p>The database's consistency rules are described by the integrity requirements, such as the record referenced by a foreign key must exist. There are other types of consistency requirements; e.g., there cannot be more students registered for an exam than the limit in the Neptun system.</p> <p>Transactions ensure that our database is always in a consistent state. While a transaction is in progress, temporary inconsistencies may arise, similarly to the interim state between the two steps of the sequence of the operation above. However, at the end of the transaction, consistency must be restored. In other words: transactions enforce transition between consistent states.</p>"},{"location":"lecture-notes/transactions/#durability","title":"Durability","text":"<p>Durability prescribes that the effect of a transaction is durable, that is, the results are not lost. Practically it means that the modifications performed by a transaction must be flushed to persistent storage (i.e., disk).</p> <p>There are two types of errors in database systems that can lead to data corruption: soft crash and hard crash. Soft crash means the database process terminates, and the content of memory is lost. Transactions offer protection from these kinds of crashes. A hard crash means that the disk is also affected. Only a backup can provide protection here.</p>"},{"location":"lecture-notes/transactions/#isolation","title":"Isolation","text":"<p>By isolation, we mean to isolate the effect of transactions from each other. That is, when writing our query, we do not need to concern ourselves with other concurrent transactions; the system will handle this aspect. The developer can write queries as if they were executed in the system alone, and the system will guarantee that it will prohibit those concurrency issues that we do not want to deal with.</p> <p>The system will still run transactions concurrently. However, it guarantees to schedule the transactions to not violate the rules of the isolation level requested by the transaction. Therefore, all transactions need to specify the requested isolation level.</p>"},{"location":"lecture-notes/transactions/#isolation-problems-and-isolation-levels","title":"Isolation problems and isolation levels","text":"<p>Before we can discuss the isolation levels, we need to first understand the types of problems that concurrency can cause.</p>"},{"location":"lecture-notes/transactions/#problems","title":"Problems","text":""},{"location":"lecture-notes/transactions/#dirty-read","title":"Dirty read","text":"<p>A dirty read means that a transaction accesses the uncommitted data of another transaction:</p> <ol> <li>A transaction modifies a record in the database but does not commit yet.</li> <li>Another transaction reads the same record (in its changed state).</li> <li>The first transaction is aborted, and the system restores the record to the state it was in before the change.</li> </ol> <p>The transaction that read the record in the second step is now working with invalid, non-existent data. It should not have read it.</p> <p></p> <p>Source</p> <p>Source of images: https://vladmihalcea.com/2014/01/05/a-beginners-guide-to-acid-and-database-transactions/</p> <p>Should be avoided</p> <p>Dirty read should almost always be avoided.</p>"},{"location":"lecture-notes/transactions/#lost-update","title":"Lost update","text":"<p>During a lost update, two writes conflict:</p> <ol> <li>A transaction changes a record.</li> <li>Another transaction overwrites the same record.</li> </ol> <p>The database has the result of the second write as if the first did not even happen.</p> <p></p>"},{"location":"lecture-notes/transactions/#non-repeatable-read","title":"Non-repeatable read","text":"<p>A non-repeatable read means that the result of the query depends on the time it was issued:</p> <ol> <li>A transaction queries a record.</li> <li>A different transaction changes the same record.</li> <li>If the first transaction re-executes the same query as before, it gets a different result.</li> </ol> <p></p>"},{"location":"lecture-notes/transactions/#phantom-records-phantom-read","title":"Phantom records / phantom read","text":"<p>We face the problem of phantom records when we work with recordsets:</p> <ol> <li>A transaction executes a query that yields multiple records as a result.</li> <li>Meanwhile, a different transaction deletes a record that is included in the previous result set.</li> <li>The first transaction starts processing its result set (e.g., iterates over the records one by one).</li> </ol> <p>Should the deleted record be processed now? We can imagine a similar scenario when a record is altered in the second step. Which state should the reader transaction in step three see? The one before, or the one after the modification?</p> <p></p>"},{"location":"lecture-notes/transactions/#isolation-levels","title":"Isolation levels","text":"<p>The problems discussed before can be avoided by using the right isolation level. We should consider, though, that the \"higher\" level of isolation we prescribe, the lower the throughput of the database system will be. Also, we might face deadlocks (see below). Our goal, thus, is a compromise between a suitable isolation level and performance.</p> <p>The ANSI/ISO SQL standard defines the following isolation levels:</p> <ul> <li>Read uncommitted: offers no protection.</li> <li>Read committed: no dirty read.</li> <li>Repeatable read: no dirty read and no non-repeatable read.</li> <li>Serializable: prohibits all issues.</li> </ul> <p>What to use?</p> <p>Read uncommitted is seldom used. Serializable, similarly, is avoided if possible. The default, usually, is read committed.</p>"},{"location":"lecture-notes/transactions/#scheduling-enforced-with-locks","title":"Scheduling enforced with locks","text":"<p>The database enforces isolation through locks: when a record is accessed (read or write), it is locked by the system. The lock is placed on the record when it is first accessed and is removed at the end of the transaction. The type of lock (e.g., shared lock or mutually exclusive) depends on the isolation level and the implementation of the database management system.</p> <p>These locks, in effect, enforce the scheduling of the transactions. When a lock is not available, because the record it used by another transaction and concurrent access is not allowed by the isolation level, the transaction will wait.</p> <p>We know that when we use locks, deadlock can occur. This is no different in databases. A deadlock may occur when two transactions are competing for the same locks. See the figure below; a continuous line represents an owned lock, while the dashed ones represent a lock the transaction would like to acquire. Neither of these requests can be fulfilled, resulting in both transactions being unable to move forward.</p> <p></p> <p>Deadlocks cannot be prevented in database management systems, but they can be recognized and dealt with. The system monitors locks, and when a deadlock is detected one of the transactions is aborted and all its modifications are rolled back. All applications using a database must be prepared to handle this.</p> <p>Retry is the only resolution</p> <p>When a deadlock happens, there is usually no other resolution than retrying the operation later (e.g., automatically, or manually requested by the end-user).</p>"},{"location":"lecture-notes/transactions/#transaction-boundaries","title":"Transaction boundaries","text":"<p>A transaction combines a sequence of steps. It is, therefore, necessary to mark the beginning and the end of the transaction. The way transaction boundaries are signaled may depend on the platform, but generally:</p> <ol> <li> <p>All operations are executed within the scope of a transaction. If the transaction boundary is not marked explicitly, each statement is a transaction in itself.</p> <p>Simple statements are transactions too</p> <p>Since all SQL statements run within a transaction scope, the transaction properties are automatically guaranteed for all statements. For example, a <code>delete</code> statement affecting multiple records cannot abort and delete only half of the records.</p> </li> <li> <p>The developer executes a <code>begin transaction</code> SQL statement to start a transaction, and completes it either with <code>commit</code> or <code>rollback</code>. Commit completes the transaction and saves its changes, while rollback aborts the transaction and undoes its changes.</p> <p>Nested transactions</p> <p>Some database management systems enable nested transactions too. Completing transactions follow the nesting: each level needs to be committed.</p> </li> </ol>"},{"location":"lecture-notes/transactions/#transaction-logging","title":"Transaction logging","text":"<p>So far, we have covered what transactions are used for. Let us understand how they work internally.</p> <p>Transactional logging is the process used by the database management system to track the pending modifications of running transactions allowing rolling back these changes in case of abort or soft crash.</p> <p>To understand transactional logging, let us consider the following system model.</p> <p></p> <p>This conceptual model includes the following operations:</p> <ul> <li>Begin T(x): Start of transaction</li> <li>Input(A): Read data from the durable database store (disk)</li> <li>Output(A): Write data to durable database store (disk)</li> <li>Read(A): Transaction reads the data from the memory buffer</li> <li>Write(A): Transaction writes the data to the memory buffer</li> <li>FLUSH_LOG: Write the transaction log to disk</li> </ul> <p>The process of transactional logging is demonstrated in the following example. In this example, a transaction modifies two data elements: A is decreases by 2, and B is increased by 2.</p>"},{"location":"lecture-notes/transactions/#undo-transaction-log","title":"Undo transaction log","text":"<p>We begin with an empty memory buffer. Every data is on disk. The process starts by reading the data from disk.</p> Operation A (database) B (database) A (buffer) B (buffer) Transactional log Begin(T1) 10 20 - - Begin T1 Input(A) 10 20 10 - Input(B) 10 20 10 20 <p>The transaction has all the necessary data in the memory buffer. The modification is performed, and the data is written back to the buffer. At the same time, the original values are written to the transaction log.</p> Operation A (database) B (database) A (buffer) B (buffer) Transactional log Read(A) 10 20 10 20 Write(A) 10 20 8 20 T1, A, 10 Read(B) 10 20 8 20 Write(B) 10 20 8 22 T1, B, 20 <p>The transaction completes, and it saves the changes. The transaction commits, which first flushes the transaction log to disk, then the changes are persisted to disk.</p> Operation A (database) B (database) A (buffer) B (buffer) Transactional log Flush_LOG 10 20 8 22 Output(A) 8 20 8 22 Output(B) 8 22 8 22 Commit T1 <p>How can the consistent state be restored in case of a soft crash?</p> <ul> <li>Suppose the transaction is aborted before the commit. There is no action needed, as the database files on disk contain the original values, and the memory buffer is lost during the crash.</li> <li>If the transaction is in the middle of the commit procedure, some data could already be written to disk. These need to be reverted. The transaction log is processed starting from the end, and for all transactions that have no commit mark in the log, the values must be restored to their original state.</li> </ul> <p>To summarize, when using undo logging:</p> <ul> <li>the database cannot be modified until the transaction log is flushed,</li> <li>and the commit mark must be placed into the log once the database writes are finished.</li> </ul> <p>The key is to flush the transaction log before the changes are persisted. The drawback of this method is that the transaction log is flushed twice, which is a performance issue due to the cost of disk access.</p>"},{"location":"lecture-notes/transactions/#redo-transaction-log","title":"Redo transaction log","text":"<p>The process starts with reading the data from disk, followed by performing the modifications, but this time the final values are written to the transaction log.</p> Operation A (database) B (database) A (buffer) B (buffer) Transactional log Read(A) 10 20 10 20 Write(A) 10 20 8 20 T1, A, 8 Read(B) 10 20 8 20 Write(B) 10 20 8 22 T1, B, 22 <p>To finalize the transaction, the log is flushed first to register the modified values - but no modification is made to the database files yet. Thus, the transaction log needs to be written to disk only once (compared to the undo logging scheme).</p> Operation A (database) B (database) A (buffer) B (buffer) Transactional log Commit T1 Flush_LOG 10 20 8 22 <p>After the transaction log is persisted, the changes are committed to the database files.</p> Operation A (database) B (database) A (buffer) B (buffer) Transactional log Output(A) 8 20 8 22 Output(B) 8 22 8 22 <p>How can the consistent state be restored in case of a soft crash?</p> <ul> <li>Suppose the transaction is aborted before the commit. In that case, there is no action needed, as the database files on disk contain the original values, and the memory buffer is lost during the crash.</li> <li>If the transaction is in the middle of the commit procedure, the commit mark is flushed to the log, but no changes were made to the database yet. Restoring from an aborted state at this stage is performed by processing the transaction log from the beginning and redoing all committed transactions.</li> </ul> <p>To summarize, when using redo logging:</p> <ul> <li>the database cannot be modified until the transaction log is flushed,</li> <li>commit mark must be placed into the transaction log before writing the database files.</li> </ul> <p>There are fewer transaction log flushes in this scheme compared to undo logging; however, the restore procedure is longer.</p>"},{"location":"lecture-notes/transactions/#undoredo-logging","title":"Undo/redo logging","text":"<p>As the name suggests, this is the combination of the two schemes. The process starts just like in the previous cases. The difference is in writing the transaction log: both the original and the modified values are written to the log.</p> Operation A (database) B (database) A (buffer) B (buffer) Transactional log Read(A) 10 20 10 20 Write(A) 10 20 8 20 T1, A, 10, 8 Read(B) 10 20 8 20 Write(B) 10 20 8 22 T1, B, 20, 22 <p>The commit procedure is simpler. The order of writing the database files and writing the commit mark into the transaction log is no longer fixed - however, flushing the transaction log must still be performed first. The simplification, therefore, is that the place of the commit mark is not fixed.</p> Operation A (database) B (database) A (buffer) B (buffer) Transactional log Flush_LOG 10 20 8 22 Output(A) 8 20 8 22 Commit T1 Output(B) 8 22 8 22 <p>Restore needs to combine the procedures discussed before:</p> <ul> <li>committed transactions are replayed (just like in redo logging),</li> <li>while aborted transactions are reverted (just like in undo logging).</li> </ul> <p>This solution has the following advantages:</p> <ul> <li>there is less synchronization during the commit procedure (with regards to writing the transaction log and the database files),</li> <li>the changes can be persisted in the database files sooner (no need to wait for writing the commit mark).</li> </ul>"},{"location":"lecture-notes/transactions/#reducing-the-transaction-log","title":"Reducing the transaction log","text":"<p>The transaction log needs to be emptied periodically. Transactions that are committed and persisted into the database files can be purged from the log. Similarly, aborted transactions that were reverted can also be removed. This is performed automatically by the system, but can also be triggered manually.</p> <p>Long-running transactions and the transaction log</p> <p>Long-running transactions can significantly increase the size of the log. The larger the log is, the longer the purging process will take.</p>"},{"location":"lecture-notes/transactions/#extracting-deadlock-information-from-mssql-database","title":"Extracting deadlock information from MSSQL database","text":"<p>Deadlock</p> <p>A deadlock in a system can occur if there are locks. A deadlock can occur if at least two transactions want to obtain the same locks simultaneously.</p> <p>Let there be transactions A and B and resources a and b. Transaction A already locks resource a, while transaction B locks resource b. Then, let us assume that transaction A wants to lock resource b and transaction B also wants to lock resource a. In this case, a deadlock will occur.</p> <p>Let us take the previous example and see how we can diagnose the deadlock once it occurs in MSSQL. To do this, first, we need to cause the deadlock artificially.</p> <ol> <li> <p>Let us create two tables on which to generate the deadlock artificially.</p> <p>Create the first table called <code>Lefty</code>, which will have an attribute called <code>Numbers</code>:</p> <pre><code>CREATE TABLE dbo.Lefty (Numbers INT PRIMARY KEY CLUSTERED);\nINSERT INTO dbo.Lefty VALUES (1), (2), (3); \n</code></pre> <p>Create a second table called <code>Righty</code>, which will also have an attribute, <code>Numbers</code>:</p> <pre><code>CREATE TABLE dbo.Righty (Numbers INT PRIMARY KEY CLUSTERED);\nINSERT INTO dbo.Righty VALUES (1), (2), (3); \n</code></pre> </li> <li> <p>The two transactions must run simultaneously for a deadlock to occur. If we test manually, this is difficult to achieve, so the order of execution is:</p> <ol> <li>Execute the first <code>UPDATE</code> statement from the first transaction</li> <li>From the second transaction, executed both <code>UPDATE</code> statements</li> <li>Execute the second <code>UPDATE</code> statement from the first transaction</li> </ol> <p>First transaction:</p> <pre><code>BEGIN TRAN\nUPDATE dbo.Lefty\nSET Numbers = Numbers * 2;\nGO\n\nUPDATE dbo.Righty\nSET Numbers = Numbers * 2;\nGO\n</code></pre> <p>Second transaction:</p> <pre><code>BEGIN TRAN\nUPDATE dbo.Righty\nSET Numbers = Numbers + 1;\nGO\n\nUPDATE dbo.Lefty\nSET Numbers = Numbers + 1;\nGO\n</code></pre> </li> </ol> <p>Now we have a deadlock. The system will automatically resolve this soon. Before that happens, we can check what we see in the system.</p> <p>The locks placed by the transactions can be queried in the database with the following query:</p> <pre><code>SELECT\n    OBJECT_NAME(P.object_id) AS TableName,\n    Resource_type, request_status,  request_session_id\nFROM\n    sys.dm_tran_locks dtl\n    join sys.partitions P\nON dtl.resource_associated_entity_id = p.hobt_id\n</code></pre> <p>In our example, the result of this query is:</p> TableName Resource_type request_status request_session_id 1 Righty KEY GRANT 54 2 Lefty KEY GRANT 53 <p>So the first transaction placed a lock on the <code>Lefty</code> table, while the second transaction placed it on table <code>Righty</code>.</p> <p>The database also provides information data about blocked transactions that we can query with the following SQL statement:</p> <pre><code>SELECT blocking_session_id AS BlockingSessionID,\n       session_id AS VictimSessionID,\n       wait_time/1000 AS WaitDurationSecond\nFROM sys.dm_exec_requests\nCROSS APPLY sys.dm_exec_sql_text([sql_handle])\nWHERE blocking_session_id &gt; 0 \n</code></pre> <p>In our example, the result of this query is:</p> BlockingSessionID VictimSessionID WaitDurationSecond 1 54 53 0 2 53 54 72 <p>This means that the transaction with ID 53 waits for the transaction with ID 54, and the transaction with ID 54 waits for the transaction with ID 53.</p> <p>The deadlock is soon eliminated automatically by the database. If we want to intervene manually, we can do so with the <code>kill</code> command, selecting the transaction to stop (e.g., <code>kill 53</code>).</p>"},{"location":"lecture-notes/transactions/#questions-to-test-your-knowledge","title":"Questions to test your knowledge","text":"<ul> <li>What type of concurrent data access problems do you know?</li> <li>List the isolation levels. Which problems does each of the levels prohibit?</li> <li>What are the basic properties of transactions?</li> <li>Decide whether the following statements are true or false:<ul> <li>The serializable isolation level executes the transactions one after the other.</li> <li>Deadlock can be prevented by using the right isolation level.</li> <li>The default isolation level is usually read committed.</li> <li>If we are not using explicit transactions, then we are protected from the issue of dirty read.</li> <li>The transaction log offers protection against all kinds of data losses.</li> <li>In the redo transaction logging scheme, the transaction log starts with the commit mark.</li> </ul> </li> </ul>"},{"location":"seminar/ef/","title":"Entity Framework","text":""},{"location":"seminar/ef/#entity-framework","title":"Entity Framework","text":"<p>The goal of the seminar is to practice writing Linq queries and working with Entity Framework.</p> <p>Entity Framework Core</p> <p>In this seminar, we are using .NET 8 (former .NET Core) available as a cross-platform .NET version for Windows, Linux and Mac.</p>"},{"location":"seminar/ef/#pre-requisites","title":"Pre-requisites","text":"<p>Required tools to complete the tasks: </p> <ul> <li>Microsoft Visual Studio 2022</li> <li>Microsoft SQL Server (LocalDB or Express edition)</li> <li>SQL Server Management Studio</li> <li>Database initialization script: mssql.sql</li> </ul> <p>Recommended to review:</p> <ul> <li>C# language</li> <li>Entity Framework Core and LINQ</li> </ul>"},{"location":"seminar/ef/#how-to-work-during-the-seminar","title":"How to work during the seminar","text":"<p>The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits.</p> <p>This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!</p>"},{"location":"seminar/ef/#exercise-0-createcheck-the-database","title":"Exercise 0: Create/check the database","text":"<p>The database resides on each machine; thus, the database you created previously might not be available. First, check if your database exists, and if it does not, create and initialize it. (See the instructions in the first seminar material.)</p>"},{"location":"seminar/ef/#exercise-1-create-a-project-and-map-the-database","title":"Exercise 1: Create a project and map the database","text":"<p>Let us create a new C# .NET console application in Visual Studio. (NOT the \".NET Framework\" version!)</p> <p></p> <p>Create a new project; you may work in directory <code>c:\\work</code>.</p> <ol> <li> <p>Create the initial EF Core Code First model. We will do Reverse Engineering Code First as we already have a database and we generate C# Code-First model.</p> <ul> <li>Install the EF Core NuGet package from the UI or copy these lines in the project file:</li> </ul> <pre><code>&lt;ItemGroup&gt;\n    &lt;PackageReference Include=\"Microsoft.EntityFrameworkCore.SqlServer\" Version=\"6.0.8\" /&gt;\n    &lt;PackageReference Include=\"Microsoft.EntityFrameworkCore.Design\" Version=\"6.0.8\"&gt;\n        &lt;PrivateAssets&gt;all&lt;/PrivateAssets&gt;\n        &lt;IncludeAssets&gt;runtime; build; native; contentfiles; analyzers; buildtransitive&lt;/IncludeAssets&gt;\n    &lt;/PackageReference&gt;\n    &lt;PackageReference Include=\"Microsoft.EntityFrameworkCore.Tools\" Version=\"6.0.8\"&gt;\n        &lt;PrivateAssets&gt;all&lt;/PrivateAssets&gt;\n        &lt;IncludeAssets&gt;runtime; build; native; contentfiles; analyzers; buildtransitive&lt;/IncludeAssets&gt;\n    &lt;/PackageReference&gt;\n&lt;/ItemGroup&gt;\n</code></pre> <ul> <li>Run this EF Core PowerShell script in VS in the Package Manager Console which generates the database context and entity model:</li> </ul> <pre><code>Scaffold-DbContext 'Data Source=(localdb)\\MSSQLLocalDB;Initial Catalog=[neptun]' Microsoft.EntityFrameworkCore.SqlServer -Context AdatvezDbContext -OutputDir Entities\n</code></pre> <p>EF Core .NET CLI</p> <p>In the future, we will continue to use the commands available from the Package Manager Console, which is installed with the <code>Microsoft.EntityFrameworkCore.Tools</code> package. If anyone wants to use the conventional CLI outside of VS, the documentation can be found at link below.</p> </li> <li> <p>Let's examine the generated code-first model.</p> <ul> <li>The database is accessed through the ``AdatvezDbContext'' class</li> <li>Database tables are accessible via <code>DbSet</code> properties.</li> <li>The connection is configured in the <code>`OnConfiguring</code>' method. In a live application, this typically comes from a configuration file, which is why the <code>AdatvezDbContext(DbContextOptions&lt;AdatvezDbContext&gt; options)</code> constructor was generated</li> <li>The database model was configured in the <code>`OnModelCreating</code>' method.</li> </ul> </li> <li> <p>Make changes to the model</p> <p>Rename the <code>Customer</code> navigation property of the <code>CustomerSite</code> entity to <code>MainCustomer</code> both in the entity and in <code>OnModelCreating</code>. This modification to the code-first model does not change the database schema.</p> CustomerSite.cs<pre><code>public virtual Customer? MainCustomer { get; set; }\n</code></pre> AdatvezDbContext.cs<pre><code>protected override void OnModelCreating(ModelBuilder modelBuilder)\n{\n    // ...\n\n    modelBuilder.Entity&lt;CustomerSite&gt;(entity =&gt;\n    {\n        // ...\n\n        entity.HasOne(d =&gt; d.MainCustomer)\n            .WithMany(p =&gt; p.CustomerSites)\n            .HasForeignKey(d =&gt; d.CustomerId)\n            .HasConstraintName(\"FK__CustomerS__Custo__32E0915F\");\n    });\n\n    // ...\n}\n</code></pre> </li> <li> <p>Change the database schema - Migrations</p> <p>Currently, we have scaffolded our code-first model from the existing database, but we no longer want to maintain the schema with a database-first approach. Instead, use code-first migrations to change the database schema.</p> <ul> <li> <p>Let's create an initial migration called `Init', which will contain our initial schema. In the Package Manager Console, issue the following command.</p> <pre><code>Add-Migration Init\n</code></pre> </li> <li> <p>Let's try to run this migration on the database with the following command.</p> <pre><code>Update-Database\n</code></pre> <p>This fails by definition, because the commands in the migration want to migrate the schema compared to an empty database, but we already have this schema in our database. EF keeps track of which migrations are already applied to the database in a special table called <code>__EFMigrationHistory</code>.</p> </li> <li> <p>Let's manually add the ``Init'' migration to this table, with which we indicate to EF that it has essentially already run. Pay attention to the name of the migration, which must also include the date.</p> <p></p> </li> <li> <p>Let's change the database schema in our code-first model.</p> <ul> <li> <p>Let the <code>Price' property of our</code>Product' entity be <code>decimal' instead of</code>double', which is more useful for storing amounts of money. It should also be mandatory (cannot be null).</p> Product.cs<pre><code>public decimal Price { get; set; }\n</code></pre> </li> <li> <p>Set the constraint and precision of the SQL field with <code>modelBuilder</code>.</p> DatavezDbContext.cs<pre><code>protected override void OnModelCreating(ModelBuilder modelBuilder)\n{\n    // ...\n\n    modelBuilder.Entity&lt;Product&gt;(entity =&gt;\n    {\n        // ...\n\n        entity.Property(e =&gt; e.Price).HasPrecision(18, 2).IsRequired();\n\n        // ...\n    }\n\n    // ...\n}\n</code></pre> </li> <li> <p>Create a migration of our change and check the generated migration</p> <pre><code>Add-Migration ProductPriceDecimal\n</code></pre> </li> <li> <p>Run the migration on the database and check its effect in the database</p> <pre><code>Update-Database\n</code></pre> </li> </ul> </li> </ul> </li> </ol>"},{"location":"seminar/ef/#task-2-queries","title":"Task 2: Queries","text":"<p>Formulate the following queries using LINQ on the mapped data model. Print the results to the console.</p> <p>Use the debugger to see what kind of SQL statement is generated: by dragging the mouse over the variable of type `IQueryable', you can see the generated SQL as soon as the iteration of the result set begins.</p> <ol> <li> <p>List the names and stock of products of which there are more than 30 in stock!</p> </li> <li> <p>Write a query that lists the products that have been ordered at least twice!</p> </li> <li> <p>Create a query that lists orders with a total value of more than HUF 30,000! When listing the result set, the individual items (Product name, amount, net price) should be listed line by line after the customer's name.</p> </li> <li> <p>List the data of the most expensive product!</p> </li> <li> <p>List the buyer pairs that have locations in the same city. A pair should be listed only once.</p> </li> </ol> Solution <pre><code>using ConsoleApp3.Entities;\n\nusing Microsoft.EntityFrameworkCore;\n\nConsole.WriteLine(\"***** M\u00e1sodik feladat *****\");\nusing (var db = new AdatvezDbContext())\n{\n// 2.1\nConsole.WriteLine(\"\\t2.1:\");\n// Query szintaktika\nvar productStockQuery = from p in db.Products\n                        where p.Stock &gt; 30\n                        select p;\n\n// Fluent / Method Chaining szintaktika\n// var productStockQuery = db.Products.Where(p =&gt; p.Stock &gt; 30);\n\nforeach (var p in productStockQuery)\n{\n    Console.WriteLine($\"\\t\\tName={p.Name}\\tStock={p.Stock}\");\n}\n\n// 2.2\nConsole.WriteLine(\"\\t2.2:\");\nvar productOrderQuery = db.Products.Where(p =&gt; p.OrderItems.Count &gt;= 2);\n\n// query szintaktika\n//var productOrderQuery = from p in db.Products\n//                        where p.OrderItems.Count &gt;= 2\n//                        select p;\n\nforeach (var p in productOrderQuery)\n{\n    Console.WriteLine($\"\\t\\tName={p.Name}\");\n}\n\n// 2.3\nConsole.WriteLine(\"\\t2.3 helytelen megold\u00e1s\");\nvar orderTotalQuery = db.Orders.Where(o =&gt; o.OrderItems.Sum(oi =&gt; oi.Amount * oi.Price) &gt; 30000);\n\n// query szintaktika\n//var orderTotalQuery = from o in db.Orders\n//                      where o.OrderItems.Sum(oi =&gt; oi.Amount * oi.Price) &gt; 30000\n//                      select o;\n\n//foreach (var o in orderTotalQuery)\n//{\n//    // Ez az\u00e9rt fog elsz\u00e1llni, mert EF Core-ban nincs alap\u00e9rtelmezetten Lazy Loading,\n//    // \u00edgy a navig\u00e1ci\u00f3s propertyk nem lesznek felt\u00f6ltve\n//    Console.WriteLine(\"\\t\\tName={0}\", o.CustomerSite.MainCustomer.Name);\n//    foreach (var oi in o.OrderItems)\n//    {\n//        Console.WriteLine($\"\\t\\t\\tProduct={oi.Product.Name}\\tPrice={oi.Price}\\tAmount={oi.Amount}\");\n//    }\n//}\n\n// 2.3 m\u00e1sodik megold\u00e1s\n// Include-oljuk a hi\u00e1nyz\u00f3 navig\u00e1ci\u00f3s tulajdons\u00e1gokat.\n// Expression alap\u00fa Include-hoz sz\u00fcks\u00e9g van a k\u00f6vetkez\u0151 n\u00e9vt\u00e9r import\u00e1l\u00e1s\u00e1ra: (CTRL + . is felaj\u00e1nlja a haszn\u00e1lat sor\u00e1n)\n// using Microsoft.EntityFrameworkCore;\n\n// Csak egy lek\u00e9rdez\u00e9st fog gener\u00e1lni, a Navigation Propertyket is felt\u00f6lti r\u00f6gt\u00f6n\nConsole.WriteLine(\"\\tc 2.3 helyes megold\u00e1s:\");\nvar orderTotalQuery2 = db.Orders\n    .Include(o =&gt; o.OrderItems)\n        .ThenInclude(oi =&gt; oi.Product)\n    .Include(o =&gt; o.CustomerSite)\n    .Include(o =&gt; o.CustomerSite.MainCustomer)\n    .Where(o =&gt; o.OrderItems.Sum(oi =&gt; oi.Amount * oi.Price) &gt; 30000);\n\n// query szintaktika\n//var orderTotalQuery2 = from o in db.Orders\n//                       .Include(o =&gt; o.OrderItems)\n//                           .ThenInclude(oi =&gt; oi.Product)\n//                       .Include(o =&gt; o.CustomerSite)\n//                       .Include(o =&gt; o.CustomerSite.MainCustomer)\n//                   where o.OrderItems.Sum(oi =&gt; oi.Amount * oi.Price) &gt; 30000\n//                   select o;\n\nforeach (var o in orderTotalQuery2)\n{\n    Console.WriteLine(\"\\t\\tName={0}\", o.CustomerSite.MainCustomer.Name);\n    foreach (var oi in o.OrderItems)\n    {\n        Console.WriteLine($\"\\t\\t\\tProduct={oi.Product.Name}\\tPrice={oi.Price}\\tAmount={oi.Amount}\");\n    }\n}\n\n// 2.4\nConsole.WriteLine(\"\\t2.4:\");\nvar maxPriceQuery = db.Products.Where(p =&gt; p.Price == db.Products.Max(a =&gt; a.Price));\n\n// query szintaktika\n//var maxPriceQuery = from p in db.Products\n//                    where p.Price == db.Products.Max(a =&gt; a.Price)\n//                    select p;\n\nforeach (var t in maxPriceQuery)\n{\n    Console.WriteLine($\"\\t\\tName={t.Name}\\tPrice={t.Price}\");\n}\n\n// 2.5\nConsole.WriteLine(\"\\t2.5:\");\nvar cityJoinQuery = db.CustomerSites\n    .Join(db.CustomerSites, s1 =&gt; s1.City, s2 =&gt; s2.City, (s1, s2) =&gt; new { s1, s2 })\n    .Where(x =&gt; x.s1.CustomerId &gt; x.s2.CustomerId)\n    .Select(x =&gt; new { c1 = x.s1.MainCustomer, c2 = x.s2.MainCustomer });\n\n// query szintaktika\n//var cityJoinQuery = from s1 in db.CustomerSites\n//                    join s2 in db.CustomerSites on s1.City equals s2.City\n//                    where s1.CustomerId &gt; s2.CustomerId\n//                    select new { c1 = s1.MainCustomer, c2 = s2.MainCustomer };\n\nforeach (var v in cityJoinQuery)\n{\n    Console.WriteLine($\"\\t\\tCustomer 1={v.c1.Name}\\tCustomer 2={v.c2.Name}\");\n}\n}\n</code></pre>"},{"location":"seminar/ef/#task-3-data-changes","title":"Task 3: Data changes","text":"<p>The <code>DbContext</code> can be used not only for queries, but also for insertions, modifications and deletions.</p> <ol> <li> <p>Write a LINQ-based C# code that increases the price of \"LEGO\" products by 10 percent!</p> </li> <li> <p>Create a new category called Expensive toys and reclassify here all the products whose price is greater than HUF 8,000!</p> </li> </ol> Solution <pre><code>using Microsoft.EntityFrameworkCore;\nusing [project name].Entities;\n\nConsole.WriteLine(\"***** Third Task *****\");\nusing (var db = new DatavezDbContext())\n{\n    // 3.1\n    Console.WriteLine(\"\\t3.1:\");\n    var legoProductsQiery = db.Products.Where(p =&gt; p.Category.Name == \"LEGO\");\n\n    Console.WriteLine(\"\\tBefore change:\");\n    foreach (var p in legoProductsQiery.ToList())\n    {\n        Console.WriteLine($\"\\t\\t\\tName={p.Name}\\tStock={p.Stock}\\tPrice={p.Price}\");\n        p.Price = 1.1m * p.Price;\n    }\n\n    db.SaveChanges();\n\n    Console.WriteLine(\"\\tAfter modification:\");\n    // ToList induces a database request\n    foreach (var p in legoProductsQiery.ToList())\n    {\n        Console.WriteLine($\"\\t\\t\\tName={p.Name}\\tStock={p.Stock}\\tPrice={p.Price}\");\n    }\n\n    // 3.2\n    Console.WriteLine(\"\\t3.2:\");\n    var expensiveToysCategory = db.Categories\n        .Where(c =&gt; c.Name == \"Expensive Toys\")\n        .SingleOrDefault();\n\n    if (expensiveToysCategory == null)\n    {\n        expensiveToysCategory = new Category { Name = \"Expensive toys\" };\n\n        // This is not necessary: if there is an unordered product, we add the category entity to it\n        // and it is automatically included in the category table. However, if we take it explicitly, (1) it better\n        // expresses our intention; and (2) we insert the category even if there are no reclassified product.\n        db.Categories.Add(expensiveToysCategory);\n    }\n\n    var expensiveProductsQuery = db.Products.Where(p =&gt; p.Price &gt; 8000);\n\n    foreach (var p in expensiveProductsQuery.ToList())\n    {\n        p.Category = expensiveToysCategory;\n    }\n\n    db.SaveChanges();\n\n    expensiveProductsQuery = db.Products\n        .Include(p =&gt; p.Category)\n        .Where(p =&gt; p.Category.Name == \"Expensive toys\");\n\n    foreach (var p in expensiveProductsQuery)\n    {\n        Console.WriteLine($\"\\t\\tName={p.Name}\\tPrice={p.Price}\\tCategory={p.Category.Name}\");\n    }\n}\n</code></pre>"},{"location":"seminar/ef/#task-4-using-stored-procedures","title":"Task 4: Using stored procedures","text":"<p>Create a stored procedure using a new code first migration that lists the products of which at least a specified number of units have been sold. Call the stored procedure from C# code!</p> <ol> <li> <p>Create a new empty migration named <code>PopularProducts_SP</code>.</p> <pre><code>Add-Migration PopularProducts_SP\n</code></pre> </li> <li> <p>Create the stored procedure with the code below. Let's ignore the writing of the backward migration for now, where the stored procedure should to be deleted.</p> <pre><code>public partial class PopularProducts_SP : Migration\n{\n    protected override void Up(MigrationBuilder migrationBuilder)\n    {\n        migrationBuilder.Sql(\n@\"CREATE OR ALTER PROCEDURE dbo.PopularProducts (@MinAmount int = 10)\nAS\nSELECT Product.* \nFROM Product \nINNER JOIN\n(\n    SELECT OrderItem.ProductID\n    FROM OrderItem\n    GROUP BY OrderItem.ProductID\n    HAVING SUM(OrderItem.Amount) &gt; @MinAmount\n) a ON Product.ID = a.ProductID\");\n    }\n\n    protected override void Down(MigrationBuilder migrationBuilder)\n    {\n    }\n}\n</code></pre> </li> <li> <p>Update the database and check the result!</p> <pre><code>Update-Database\n</code></pre> </li> <li> <p>Call the stored procedure starting from the <code>Product</code> <code>DbSet</code> of the context using the <code>FromSqlInterpolated</code> or <code>FromSqlRaw</code> methods</p> Solution <pre><code>using Microsoft.EntityFrameworkCore;\nusing [project name].Entities;\n\nConsole.WriteLine(\"***** Fourth Task *****\");\nusing (var db = new DatavezDbContext())\n{\n    var popularProducts = db.Products.FromSqlInterpolated($\"EXECUTE dbo.PopularProducts @MinAmount={5}\");\n    foreach (var p in popularProducts)\n    {\n        Console.WriteLine($\"\\tName={p.Name}\\tStock={p.Stock}\\tPrice={p.Price}\");\n    }\n}\n</code></pre> <p><code>FromSqlInterpolated</code> vs. <code>FromSqlRaw</code></p> <p>In the above solution, the call is defined with the <code>FromSqlInterpolated</code> function, where, due to its name, the string to be interpolated is still processed by EF and the interpolation is not performed traditionally as a string, but replaces <code>SqlParameters</code> in order to protect against SQL injection.</p> <p>On the other hand, when using the <code>FromSqlraw</code> function it is prohibited to use string interpolation, instead we have to manually create the `SqlParameters' and define placeholders in the instruction</p> </li> </ol>"},{"location":"seminar/jpa/","title":"JPA & Spring Data","text":""},{"location":"seminar/jpa/#jpa-spring-data","title":"JPA &amp; Spring Data","text":"<p>To goal of this seminar is to practice working with JPA and Spring Data. Main topics of focus: working with entities, querying the database with various techniques, updating the database. The code is integrated into a skeleton web application with a UI for testing.</p>"},{"location":"seminar/jpa/#pre-requisites","title":"Pre-requisites","text":"<p>Required tools to complete the tasks:</p> <ul> <li>Spring Tool Suite (an IDE based on Eclipse)</li> <li>Microsoft SQL Server Express edition (localdb does not work here)</li> <li>SQL Server Management Studio</li> <li>Database initialization script: mssql.sql</li> <li>Starter code: https://github.com/bmeviauac01/gyakorlat-jpa-kiindulo</li> </ul> <p>Recommended to review:</p> <ul> <li>JPA lecture</li> <li>EJB, Spring lecture</li> </ul>"},{"location":"seminar/jpa/#how-to-work-during-the-seminar","title":"How to work during the seminar","text":"<p>The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits.</p> <p>This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!</p>"},{"location":"seminar/jpa/#tips-for-using-the-ide","title":"Tips for using the IDE","text":"<ul> <li>Search Type(class, interface, enum): Ctrl+Shift+T (instead of opening folders in Project explorer)</li> <li>Search file: Ctrl+Shift+R</li> <li>Fix missing imports:Ctrl+Shift+O</li> <li>Format code: Ctrl+Shift+F</li> <li>In Java Resources right-click a package / New Class/Interfaces will create the source in this package</li> <li>Restore default layout of views: Window &gt; Reset perspective</li> <li>Increase font size:<ul> <li>Window menu / Preferences, start typing font to locate Fonts and Colors</li> <li>Select it and under Basic choose Text Font and increase the size</li> </ul> </li> </ul>"},{"location":"seminar/jpa/#exercise-0-create-a-database","title":"Exercise 0: Create a database","text":"<ol> <li> <p>Use Microsoft SQL Server Management Studio to connect to the database. We are not using localdb here; the address is: <code>localhost\\sqlexpress</code> and use SQL Server Authentication with username and password <code>sa</code>.</p> </li> <li> <p>Create a new database with the name <code>adatvez</code>. You should use this exact name or will have to update the Java project. To create a new database see the instructions in the first seminar material. If a database with this name already exists, no need to re-create it.</p> </li> <li> <p>Run the database initialization script on this database. If the database exists on this machine, run the script anyway to reset any changes made in the schema.</p> </li> </ol>"},{"location":"seminar/jpa/#exercise-1-start-the-ide","title":"Exercise 1: Start the IDE","text":"<ol> <li>Start Spring Tool Suite from here: <code>c:\\Tools\\hatteralkalmazasok\\eclipse\\SpringToolSuite4.exe</code>.</li> <li>It will ask for a workspace, select: <code>c:\\Tools\\hatteralkalmazasok\\workspaces\\adatvez</code></li> <li>If there is a webshop project in the Project Explorer already, delete it: right-click the project / Delete, and check Delete project contents on disk</li> </ol>"},{"location":"seminar/jpa/#exercise-2-import-project","title":"Exercise 2: Import project","text":"<ol> <li>Download the project skeleton!<ul> <li>Open a new command prompt</li> <li>Navigate to a directory, e.g. <code>c:\\work\\NEPTUN</code></li> <li>Execute <code>git clone --depth 1 https://github.com/bmeviauac01/gyakorlat-jpa-kiindulo.git</code></li> </ul> </li> <li>Import the downloaded project into the workspace:<ul> <li>Open File / Import...</li> <li>Start typing Existing Maven Projects and choose it</li> <li>Locate the downloaded webshop project (the <code>webshop</code> folder in the checked-out repository), OK, check the webshop project in the dialog</li> <li>Finish</li> </ul> </li> <li>Overview of the projects<ul> <li>It is a maven based project. Maven is a command-line build tool that can be integrated with IDEs as well. It can download the libraries our projects depend on from public repositories. After opening the <code>pom.xml</code> file, the maven project's config file, you can see some dependency tags that will transitively download Hibernate, our JPA implementation, Spring Boot, Spring Data, Spring MVC and Thymeleaf.</li> <li>The application.properties file contains some basic settings. Let us verify the database name (spring.datasource.url), the user name (spring.datasource.username) and password (spring.datasource.password) for the DB access here. In classic Java EE web applications, this JNDI name of the database should be defined in the <code>persistence.xml</code>, but Spring Boot supports XML-less configuration.</li> <li><code>WebshopApplication</code> is the entry point and configuration of the Spring Boot application. A traditional web application should be deployed to a web container (e.g., Tomcat, Jetty) running in a separate process. In the case of Spring Boot, however, Spring Boot itself will start an embedded web container (Tomcat, by default).</li> <li>The web interface is one page: <code>src\\main\\resources\\templates\\testPage.html</code>. We will not modify it. It contains standard HTML and some Thymeleaf attributes.</li> <li><code>WebshopController</code>: the controller class implementing the web layer (its methods handle the HTTP requests). These methods typically call a query implemented in a repository or a service method and put the result into the model with a name that we can reference via Thymeleaf. You should call the methods implementing the tasks at the <code>//TODO</code> comments.</li> </ul> </li> </ol>"},{"location":"seminar/jpa/#exercise-3-overview-of-the-entities","title":"Exercise 3: Overview of the entities","text":"<ul> <li>The  entities can be found in the <code>hu.bme.aut.adatvez.webshop.model</code> package. We could have written them by hand, but in this case, they were generated from the DB tables via the JPA plugin of Eclipse.</li> <li>Open an entity class, e.g., Vat, and check the JPA-related code. You can see the <code>@Entity</code>, <code>@Id</code> annotations, and <code>@OneToMany</code> or <code>@ManyToOne</code> for defining relationships.</li> </ul>"},{"location":"seminar/jpa/#exercise-4-queries","title":"Exercise 4: Queries","text":"<p>Implement the following queries on the data model. In JPA and Spring Data, you can write queries by different means. In the following tasks, we specify how to write the query so that multiple ways can be demonstrated.</p> <p>It is important to note that each task could be written using any technology and style. The requirements are provided for demonstrating all technologies.</p> <p>The methods implementing the queries should always be called in the <code>WebshopController</code> class, at the corresponding <code>//TODO</code> comment, run the application and test the query from a browser at address http://localhost:9080.</p> <p>a) List the names and stock of those products of which we have more than 30 pieces in stock! Method: Spring Data repository interface with method name-derived query.</p> <p>b) Write a query that lists those products that were ordered at least twice! Method: JPQL query created with an injected EntityManager in a Spring Data custom repository implementation.</p> <p>c) List the data of the most expensive product! Method: Named query, called from Spring Data repository or with an injected EntityManager.</p> <p>When running the application, the SQL statements generated by Hibernate can be observed in the Console view of Eclipse, due to this config line in application.properties: <code>spring.jpa.show-sql=true</code></p>"},{"location":"seminar/jpa/#running-the-application","title":"Running the application","text":"<p>Right-click on the webshop project in the Project Explorer &gt; Debug As &gt; Spring Boot App. This starts the application in debug mode, which starts an embedded web container, and the application is available at http://localhost:9080 from a browser. Having done this once, we can do it more easily: Click on the Debug icon on the toolbar, and you will see the webshop run there.</p> <p></p> <p>If under the Debug icon you find webshop run, the method above is unnecessary.</p> <p>The running application can be stopped with the red Terminate icon in the Console view. If we run the application twice, without terminating the first run, the second run will report a port collision on the port 9080 and stop. This second execution will be visible in the Console view, and the Terminate command will be inactive, as this copy has been terminated already. Click on the gray double C icon next to Terminate to close this view, and only the active running process will be visible.</p> <p>If we close the Console view by mistake, use shortcut Alt+Shift+Q, C or menu Window / Show View / Console to reopen.</p> <p>We can re-run the application using F11 as well. The workspace is configured to automatically terminate the running instance first in this case, so pressing the Terminate button manually is not needed.</p> <p>When running the application in debug mode, the modifications in HTML files and some Java ode modifications are immediately actualized, so we only have to refresh the browser to see the effect of the code modification. But the application has to be restarted if we modify the Java code in either of the following ways</p> <ul> <li>adding a new type</li> <li>adding/removing/modifying an annotation</li> <li>adding a new class-or member variable, or method</li> <li>we changed the signature of a method.</li> </ul> <p>Simply put, when modifying code that is not inside of an existing method, a restart will be needed.</p> Solution <p>4.a exercise</p> <p>In the <code>dao</code> package open <code>ProductRepository</code> interface that implements the Spring Data <code>JpaRepository</code>. There are a few methods for other exercises. Some define a <code>@Query</code> annotation and the query as text, some work without such annotation. We will not need the <code>@Query</code> annotation, but rather have Spring Data infer the SQL instruction from the method name as follows:</p> <pre><code>package hu.bme.aut.adatvez.webshop.dao;\n\nimport java.math.BigDecimal;\nimport java.util.List;\nimport hu.bme.aut.adatvez.webshop.model.Product;\nimport org.springframework.data.jpa.repository.JpaRepository;\n\npublic interface ProductRepository extends JpaRepository&lt;Product, Long&gt;, ProductRepositoryCustom {\n  ...\n  List&lt;Product&gt; findByStockGreaterThan(BigDecimal limit);\n}\n</code></pre> <p><code>WebshopController</code> already contains an injected <code>ProductRepository</code>; let us call this method at TODO 4.a:</p> <pre><code>@Controller\npublic class WebshopController {\n\n  @Autowired\n  ProductRepository productRepository;\n\n  //...\n  // 4.a\n  private List&lt;Product&gt; findProductsOver30() {\n    return productRepository.findByStockGreaterThan(BigDecimal.valueOf(30));\n  }\n}\n</code></pre> <p>4.b exercise</p> <p>In the <code>dao</code> package find <code>ProductRepositoryCustom</code> interface add a new method <code>findProductsOrderedAtLeastTwice</code>:</p> <pre><code>package hu.bme.aut.adatvez.webshop.dao;\n\nimport hu.bme.aut.adatvez.webshop.model.Product;\nimport java.util.List;\n\npublic interface ProductRepositoryCustom {\n  List&lt;Product&gt; findProductsOrderedAtLeastTwice();\n}\n</code></pre> <p>The implementation class <code>ProductRepositoryImpl</code> will contain an error now as it does not implement <code>ProductRepositoryCustom</code>. Let us open this class, and line of the class declaration there will be a light bulb we can click to generate the method skeleton:</p> <p></p> <p>Add the method's implementation as follows: use the injected EntityManager to create and run the query.</p> <pre><code>package hu.bme.aut.adatvez.webshop.dao;\n\nimport hu.bme.aut.adatvez.webshop.model.Product;\n\nimport java.util.List;\n\nimport jakarta.persistence.EntityManager;\nimport jakarta.persistence.PersistenceContext;\n\npublic class ProductRepositoryImpl implements ProductRepositoryCustom {\n\n  @PersistenceContext\n  EntityManager em;\n\n  @Override\n  public List&lt;Product&gt; findProductsOrderedAtLeastTwice(){\n    return em.createQuery(\"SELECT DISTINCT p FROM Product p\n                          LEFT JOIN FETCH p.orderitems\n                          WHERE size(p.orderitems) &gt;= :itemsMin\", Product.class)\n          .setParameter(\"itemsMin\", 2)\n          .getResultList();\n  }\n}\n</code></pre> <p>Note: we might try this command: <code>SELECT p FROM Product p WHERE size(p.orderitems) /= :itemsMin</code>, which will yield an <code>org.hibernate.LazyInitializationException</code> error, hence the <code>LEFT JOIN FETCH</code> above.</p> <p>Call this in <code>WebshopController</code>:</p> <pre><code>// 4.b\nprivate List&lt;Product&gt; findProductsOrderedAtLeastTwice() {\n  // TODO\n  return productRepository.findProductsOrderedAtLeastTwice();\n}\n</code></pre> <p>4.c exercise</p> <p>Open entity class <code>Product</code> where we can find a few named querys; we need the second one:</p> <pre><code>@NamedQueries({\n@NamedQuery(name=\"Product.findAll\", query=\"SELECT p FROM Product p\"),\n@NamedQuery(name=\"Product.findMostExpensive\", query=\"SELECT p FROM Product p WHERE p.price IN (SELECT MAX(p2.price) FROM Product p2)\")\n})\n</code></pre> <p>This named query can be called in two ways. The first is to create a method in <code>ProductRepository</code> with the same name (without the Product. prefix.), that is:</p> <pre><code>public List&lt;Product&gt; findMostExpensive();\n</code></pre> <p>The second option is to execute it manually in <code>ProductRepositoryImpl</code> using <code>EntityManager</code>:</p> <pre><code>@Override\npublic List&lt;Product&gt; findMostExpensiveProducts(){\n  return em.createNamedQuery(\"Product.findMostExpensive\", Product.class).getResultList();\n}\n</code></pre> <p>This method also needs to be added to the <code>ProductRepositoryCustom</code> interface. E.g. right-click / Refactor / Pull up</p> <p>Finally, call the method in <code>WebshopController</code>:</p> <pre><code>// 4.c\nprivate List&lt;Product&gt; findMostExpensiveProducts() {\n  // TODO\n  // return productRepository.findMostExpensiveProducts();\n  return productRepository.findMostExpensive();\n}\n</code></pre>"},{"location":"seminar/jpa/#exercise-5-data-modification","title":"Exercise 5: Data modification","text":"<p>JPA can also be used to modify the database content.</p> <p>a) Write a JPQL query into the <code>ProductRepository</code> interface that raises the price of \"Building items\" by 10 percent!</p> <p>b) Write a method that creates a new category called \"Expensive toys\", if it does not exist yet, and move all the products with a price higher than 8000 into this category!</p> <p>c) Simple individual task: create a <code>CategoryRepository</code> interface, and implement a method name-derived query that you can use in task 5.b) instead of the query created with the injected EntityManager.</p> Solution <p>5.a exercise</p> <p>Crate an UPDATE query in <code>ProductRepository</code> interface. We have to denote that this is a <code>@Modifying</code> query, and also add @Transactional<code>(from package</code>org.springframework...`):</p> <pre><code>@Modifying\n@Transactional\n@Query(\"UPDATE Product p SET p.price=p.price*1.1 WHERE p.id IN\n(SELECT p2.id FROM Product p2 WHERE p2.category.name=:categoryName)\")\nvoid categoryRaisePrice(@Param(\"categoryName\") String categoryName);\n</code></pre> <p>Call in <code>WebshopController</code>:</p> <pre><code>// 5.a\n@RequestMapping(value = \"/raisePriceOfBuildingItems\", method = {\n        RequestMethod.POST, RequestMethod.GET })\nprivate String raisePriceOfBuildingItems() {\n  // TODO\n  productRepository.categoryRaisePrice(\"Building items\");\n  return \"redirect:/\";\n}\n</code></pre> <p>In the browser, the changes are visible after clicking the button.</p> <p>5.b exercise</p> <p>In the <code>dao</code> package add a new class <code>CategoryService</code> with a <code>@Service</code> annotation with a <code>@Transactional</code> method:</p> <pre><code>@Service\npublic class CategoryService {\n\n  @PersistenceContext\n  private EntityManager em;\n\n  @Autowired\n  ProductRepository productRepository;\n\n  @Transactional\n  public void moveToExpensiveToys(double priceLimit){\n    String name = \"Expensive toys\";\n    Category categoryExpensive = null;\n    List&lt;Category&gt; resultList =\n      em.createQuery(\"SELECT c from Category c WHERE c.name=:name\", Category.class)\n        .setParameter(\"name\", name)\n        .getResultList();\n\n    if(resultList.isEmpty()){\n      // 0 or null id triggers @GeneratedValue; this is a scalar, hence use 0\n      categoryExpensive = new Category(0, name);\n      em.persist(categoryExpensive);\n    }else{\n      categoryExpensive = resultList.get(0);\n    }\n\n    List&lt;Product&gt; expensiveProducts = productRepository.findByPriceGreaterThan(priceLimit);\n\n    for (Product product : expensiveProducts) {\n      categoryExpensive.addProduct(product);\n    }\n  }\n}\n</code></pre> <p>Let us note that the managed entities (fetched through queries within the transaction, or added as new with persist) need no explicit save; the transaction saves them to DB automatically.</p> <p>Call in <code>WebshopController</code>:</p> <pre><code>@Autowired\nCategoryService categoryService;\n...\n\n// 5.b\n@RequestMapping(value = \"/moveToExpensiveToys\", method = {\n        RequestMethod.POST, RequestMethod.GET })\nprivate String moveToExpensiveToys() {\n  // TODO\n  categoryService.moveToExpensiveToys(8000.0);\n  return \"redirect:/\";\n}\n</code></pre> <p>In the browser, the changes are visible after clicking the button.</p> <p>5.c exercise</p> <p>In <code>dao</code> package add a new interface <code>CategoryRepository</code>, similar to <code>ProductRepository</code> (without the Custom inheritance) with one method:</p> <pre><code>public interface CategoryRepository extends JpaRepository&lt;Category, Long&gt;{\n  List&lt;Category&gt; findByName(String name);\n}\n</code></pre> <p>This simplifies the <code>CategoryService</code> as follows:</p> <pre><code>@Service\npublic class CategoryService {\n...\n\n  @Autowired\n  CategoryRepository categoryRepository;\n\n  @Transactional\n  public void moveToExpensiveToys(double priceLimit){\n    // ...\n    List&lt;Category&gt; resultList = categoryRepository.findByName(name);\n    //  ...\n  }\n}\n</code></pre>"},{"location":"seminar/jpa/#exercise-6-using-stored-procedures","title":"Exercise 6: Using stored procedures","text":"<p>Use the <code>CreatePaymentMethod</code> stored procedure to create a new <code>Paymentmethod</code>!</p> <ul> <li> <p>Check in SQL Server Management Studio, whether the database contains the stored procedure with the name <code>CreatePaymentMethod</code>!</p> </li> <li> <p>If not, create the procedure with the code below!</p> <pre><code>CREATE PROCEDURE CreateNewPaymentMethod\n(\n@Method nvarchar(20),\n@Deadline int\n)\nAS\ninsert into PaymentMethod\nvalues(@Method,@Deadline)\nselect scope_identity() as NewId\n</code></pre> </li> </ul> Solution <p>The <code>PaymentMethod</code> entity has the following annotation. Compare it to the stored procedure code!</p> <pre><code>@NamedStoredProcedureQueries({\n  @NamedStoredProcedureQuery(name = \"createMethodSP\",\n      procedureName = \"CreateNewPaymentMethod\",\n      parameters = {\n            @StoredProcedureParameter(mode = ParameterMode.IN, name = \"Method\", type = String.class),\n            @StoredProcedureParameter(mode = ParameterMode.IN, name = \"Deadline\", type = BigDecimal.class)\n          })\n})\npublic class Paymentmethod implements Serializable {\n...\n</code></pre> <p>The named stored procedure query can be called from a Spring Data repository (<code>dao</code> package New Interface ... / PaymentmethodRepository):</p> <pre><code>public interface PaymentmethodRepository extends JpaRepository&lt;Paymentmethod, Long&gt; {\n\n  @Procedure(name=\"createMethodSP\")\n  void newMethod(@Param(\"Method\") String method, @Param(\"Deadline\") BigDecimal deadline);\n}\n</code></pre> <p>Without Spring Data we could use <code>EntityManager</code>:</p> <pre><code>@Service\npublic class PaymentmethodService {\n\n  @PersistenceContext\n  private EntityManager em;\n\n  public void createNewMethod(Paymentmethod paymentMethod){\n    StoredProcedureQuery sp = em.createNamedStoredProcedureQuery(\"createMethodSP\");\n    sp.setParameter(\"Method\", paymentMethod.getMethod());\n    sp.setParameter(\"Deadline\", paymentMethod.getDeadline());\n    sp.execute();\n  }\n}\n</code></pre> <p>Call from the web layer:</p> <ul> <li> <p>Inject into <code>WebshopController</code> the <code>PaymentmethodRepository</code> interface:</p> <pre><code>@Autowired\nPaymentmethodRepository paymentmethodRepository;\n</code></pre> </li> <li> <p>Call the method from WebshopController at the last TODO</p> <pre><code>paymentmethodRepository.newMethod(paymentMethod.getMethod(), paymentMethod.getDeadline());\n</code></pre> </li> </ul>"},{"location":"seminar/mongodb/","title":"MongoDB","text":""},{"location":"seminar/mongodb/#mongodb","title":"MongoDB","text":"<p>The seminar's goal is to understand the concepts of the MongoDB document database and the usage of the MongoDB C#/.NET Driver.</p>"},{"location":"seminar/mongodb/#pre-requisites","title":"Pre-requisites","text":"<p>Required tools to complete the tasks:</p> <ul> <li>Microsoft Visual Studio 2022</li> <li>MongoDB Community Edition</li> <li>VSCode</li> <li>MongoDB for VSCode extension</li> <li>Database initialization script: mongo.js</li> <li>Starter code: https://github.com/bmeviauac01/gyakorlat-mongo-kiindulo</li> </ul> <p>Recommended to review:</p> <ul> <li>C# language and Linq queries</li> <li>MongoDB lecture</li> <li>Using MongoDB guide</li> </ul>"},{"location":"seminar/mongodb/#how-to-work-during-the-seminar","title":"How to work during the seminar","text":"<p>The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits.</p> <p>This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!</p>"},{"location":"seminar/mongodb/#exercise-0-create-database-open-starter-code","title":"Exercise 0: Create database, open starter code","text":"<ol> <li> <p>Open a PowerShell console (search for PowerShell in the Start menu and start it, but not the one with \"ISE\" in the same - that is not the console).</p> </li> <li> <p>Copy and paste the script into the console and run it by pressing enter. Please note, that you might need to change the the directory name in the last command, e.g., if the app version is different.</p> <pre><code>Remove-Item c:\\work\\mongodatabase -Recurse -ErrorAction Ignore\nNew-Item -Type Directory c:\\work\\mongodatabase\nc:\\tools\\mongodb\\bin\\mongod.exe --dbpath c:\\work\\mongodatabase\n</code></pre> <p>Keep this window open because the server is running here. You can stop it by pressing Ctrl+C at the end of the class.</p> </li> <li> <p>Launch VSCode and connect to the MongoDB server with the extension.</p> <p></p> </li> <li> <p>Let us create a new database by right-clicking the connection (localhost). This opens a playground script window, where we paste our database creation script from innen, and run it with the black \u201cplay\u201d button found in the header. The name of the database should be <code>datadriven</code>. As a result, the collections should be created - open the elements of the database to check this.</p> <p></p> <p></p> </li> <li> <p>Download the starter solution!</p> <ul> <li>Open a new command prompt or PowerShell console (do not use the one the server is running in)</li> <li>Navigate to a folder, e.g. <code>c:\\work\\NEPTUN</code></li> <li> <p>Execute the following command:</p> <pre><code>git clone https://github.com/bmeviauac01/gyakorlat-mongo-kiindulo.git\n</code></pre> </li> </ul> </li> <li> <p>Open the sln file from the newly created folder using Visual Studio. Let us examine this project.</p> <ul> <li>This is a .NET console application. The structure resembles the structure of the Entity Framework project seen before: directory <code>Entities</code> contains the database entities while our code will be written into <code>Program.cs</code>.</li> <li><code>Program.cs</code> already contains the initialization of the connection to MongoDB.<ul> <li>Interface <code>IMongoClient</code> is used for all communication with the database. We will not use this directly.</li> <li>Interface <code>IMongoDatabase</code> represents the database <code>datadriven</code> within the MongoDB server.</li> <li>And the <code>IMongoCollection&lt;TEntity&gt;</code> interfaces represent the specific collections we can use to execute queries and modification commands.</li> </ul> </li> <li>The database documents are mapped to the C# entity classes in folder <code>Entities</code>. A major difference compared to the behavior previously seen in Entity Framework is that these classes were not generated by written manually.<ul> <li>Most entities are already mapped.</li> <li>We will create one more class during an exercise.</li> </ul> </li> </ul> </li> </ol>"},{"location":"seminar/mongodb/#exercise-1-queries","title":"Exercise 1: Queries","text":"<p>Write C# code using the MongoDB C#/.NET Driver in the following exercises. Print the results to the console.</p> <ol> <li> <p>List the names and the amount of stock of all products that we have more than 30 in stock!</p> </li> <li> <p>List the orders that consist of at least two items!</p> </li> <li> <p>List the orders that have a total value of at least 30.000! For each order, print the customer name, and list all items of the order (with the product name, amount, and price).</p> </li> <li> <p>Find the most expensive product!</p> </li> <li> <p>List the products that have been ordered at least twice!</p> </li> </ol> Solution <ol> <li> <p>We need only the product collection and execute a simple query. The filter criteria can be written as a Lambda-expression and with the builder syntax too.</p> <pre><code>Console.WriteLine(\"***** Exercise one *****\");\n\n// 1.1 first solution\nConsole.WriteLine(\"\\t1.1 First solution:\");\nvar qProductAndStock1 = productsCollection\n    .Find(p =&gt; p.Stock &gt; 30)\n    .ToList();\n\nforeach (var p in qProductAndStock1)\n    Console.WriteLine($\"\\t\\tName={p.Name}\\tStock={p.Stock}\");\n\n// 1.1 second solution\nConsole.WriteLine(\"\\t1.1 Second solution:\");\nvar qProductAndStock2 = productsCollection\n    .Find(Builders&lt;Product&gt;.Filter.Gt(p =&gt; p.Stock, 30))\n    .ToList();\n\nforeach (var p in qProductAndStock2)\n    Console.WriteLine($\"\\t\\tName={p.Name}\\tStock={p.Stock}\");\n</code></pre> </li> <li> <p>This is similar to the previous one. We may note that we would have needed a join in a relational database, but we have everything at hand here.</p> <pre><code>// 1.2 first solution\nConsole.WriteLine(\"\\t1.2 First solution:\");\nvar qOrderItems1 = ordersCollection\n    .Find(o =&gt; o.OrderItems.Length &gt;= 2)\n    .ToList();\n\nforeach (var o in qOrderItems1)\n    Console.WriteLine($\"\\t\\tCustomerID={o.CustomerID}\\tOrderID={o.ID}\\tItems={o.OrderItems.Length}\");\n\n// 1.2 second solution\nConsole.WriteLine(\"\\t1.2 Second solution:\");\nvar qOrderItems2 = ordersCollection\n    .Find(Builders&lt;Order&gt;.Filter.SizeGte(o =&gt; o.OrderItems, 2))\n    .ToList();\n\nforeach (var o in qOrderItems2)\n    Console.WriteLine($\"\\t\\tCustomerID={o.CustomerID}\\tOrderID={o.ID}\\tItems={o.OrderItems.Length}\");\n</code></pre> </li> <li> <p>A simple query is not sufficient for this exercise; thus, we need the aggregation pipeline. We may still note that every information we need is still available in one collection.</p> <pre><code>// 1.3\nConsole.WriteLine(\"\\t1.3:\");\nvar qOrderTotal = ordersCollection\n    .Aggregate()\n    .Project(order =&gt; new\n    {\n        CustomerID = order.CustomerID,\n        OrderItems = order.OrderItems,\n        Total = order.OrderItems.Sum(oi =&gt; oi.Amount * oi.Price)\n    })\n    .Match(order =&gt; order.Total &gt; 30000)\n    .ToList();\n\nforeach (var o in qOrderTotal)\n{\n    Console.WriteLine($\"\\t\\tCustomerID={o.CustomerID}\");\n    foreach (var oi in o.OrderItems)\n        Console.WriteLine($\"\\t\\t\\tProductID={oi.ProductID}\\tPrice={oi.Price}\\tAmount={oi.Amount}\");\n}\n</code></pre> </li> <li> <p>To find the most expensive product, we need two queries: first, find the largest price value, then find the products with this price.</p> <pre><code>// 1.4\nConsole.WriteLine(\"\\t1.4:\");\nvar maxPrice = productsCollection\n    .Find(_ =&gt; true)\n    .SortByDescending(p =&gt; p.Price)\n    .Limit(1)\n    .Project(p =&gt; p.Price)\n    .Single();\n\nvar qProductMax = productsCollection\n    .Find(p =&gt; p.Price == maxPrice)\n    .ToList();\n\nforeach (var t in qProductMax)\n    Console.WriteLine($\"\\t\\tName={t.Name}\\tPrice={t.Price}\");\n</code></pre> </li> <li> <p>This exercise is complicated with our current database scheme because we do not have everything at hand within one collection. We need the product information from one collection, and the order details from another one.</p> <p>We will be doing a \"join\" in the client-side, that is, in C# code. The solution's outline is to query the orders, then in C# gather the orders by product, and finally, query the product details.</p> <pre><code>// 1.5\nConsole.WriteLine(\"\\t1.5:\");\nvar qOrders = ordersCollection\n    .Find(_ =&gt; true)\n    .ToList();\n\nvar productOrders = qOrders\n    .SelectMany(o =&gt; o.OrderItems) // All order items into one list\n    .GroupBy(oi =&gt; oi.ProductID)\n    .Where(p =&gt; p.Count() &gt;= 2);\n\nvar qProducts = productsCollection\n    .Find(_ =&gt; true)\n    .ToList();\nvar productLookup = qProducts.ToDictionary(p =&gt; p.ID);\n\nforeach (var p in productOrders)\n{\n    var product = productLookup.GetValueOrDefault(p.Key);\n    Console.WriteLine($\"\\t\\tName={product?.Name}\\tStock={product?.Stock}\\tOrders={p.Count()}\");\n}\n</code></pre> <p>This solution is very elegant and works only for small databases. Suppose we face a similar task under real-life circumstances. We have two choices: denormalize the database scheme and copy product details into the orders, or create an aggregation pipeline executed by the server that does something similar to the code above (MongoDB can do that, but it will not be very fast).</p> </li> </ol>"},{"location":"seminar/mongodb/#exercise-2-create-a-new-entity-class","title":"Exercise 2: Create a new entity class","text":"<ol> <li> <p>Examine the classes <code>Product</code> and <code>VAT</code>. Why is there a field with a <code>[BsonId]</code> attribute in class <code>Product</code> and not in class <code>VAT</code>?</p> </li> <li> <p>Create a new entity class for mapping <code>Category</code> document, then add and initialize a <code>IMongoCollection&lt;Category&gt;</code> field next to the others.</p> </li> </ol> Solution <ol> <li> <p>Class <code>Product</code> represents the <code>products</code> collection; therefore each item has a unique <code>ObjectID</code>. On the other hand, class <code>VAT</code> is an embedded field used by <code>Product</code> and has no collection on its own; hence it needs no id.</p> </li> <li> <p>Create our new <code>Category</code> POCO class.</p> <p>Let us check a few sample documents using VSCode in <code>categories</code> collection.</p> <p></p> <p>Create a new class <code>Category</code> in folder <code>Entities</code> with matching fields as below.</p> <pre><code>using MongoDB.Bson;\nusing MongoDB.Bson.Serialization.Attributes;\n\nnamespace BME.DataDriven.Mongo.Entitites\n{\n    public class Category\n    {\n        [BsonId]\n        public ObjectId ID { get; set; }\n        public string Name { get; set; }\n        public ObjectId? ParentCategoryID { get; set; }\n    }\n}\n</code></pre> <p>Add a new collection interface field in <code>Program.cs</code> as follows.</p> <pre><code>private static IMongoCollection&lt;Category&gt; categoriesCollection;\n</code></pre> <p>And assign the value in the <code>initialize</code> method to get the collection.</p> <pre><code>categoriesCollection = database.GetCollection&lt;Category&gt;(\"categories\");\n</code></pre> </li> </ol>"},{"location":"seminar/mongodb/#exercise-3-data-modification","title":"Exercise 3: Data modification","text":"<p>The collection classes <code>IMongoColection&lt;TEntity&gt;</code> can also be used to execute modification operations.</p> <ol> <li> <p>Write C# code that increases the price of all products in category \"LEGO\" by 10 percent!</p> </li> <li> <p>Create a new category named Expensive toys and move all products here that cost more than 8000!</p> </li> <li> <p>Delete all categories that contain no products.</p> </li> </ol> Solution <ol> <li> <p>Find the ID of the category then update all products that have this category id.</p> <pre><code>Console.WriteLine(\"***** Exercise three *****\");\n\n//3.1\nConsole.WriteLine(\"\\t3.1:\");\nvar categoryLegoId = categoriesCollection\n    .Find(c =&gt; c.Name == \"LEGO\")\n    .Project(c =&gt; c.ID)\n    .Single();\n\nvar qProductLego = productsCollection\n    .Find(p =&gt; p.CategoryID == categoryLegoId)\n    .ToList();\nConsole.WriteLine(\"\\t\\tBefore modification:\");\nforeach (var p in qProductLego)\n    Console.WriteLine($\"\\t\\t\\tName={p.Name}\\tStock={p.Stock}\\t\u00c1r={p.Price}\");\n\nproductsCollection.UpdateMany(\n    filter: p =&gt; p.CategoryID == categoryLegoId,\n    update: Builders&lt;Product&gt;.Update.Mul(p =&gt; p.Price, 1.1));\n\nqProductLego = productsCollection\n    .Find(p =&gt; p.CategoryID == categoryLegoId)\n    .ToList();\nConsole.WriteLine(\"\\t\\tAfter modification:\");\nforeach (var p in qProductLego)\n    Console.WriteLine($\"\\t\\t\\tName={p.Name}\\tStock={p.Stock}\\t\u00c1r={p.Price}\");\n</code></pre> </li> <li> <p>MongoDB can execute the following sequence of steps in a single atomic step: \"Get me category <code>Expensive toys</code>. If it does not exist, create it.\" We will use <code>FindOneAndUpdate</code> to achieve this.</p> <pre><code>//3.2\nConsole.WriteLine(\"\\t3.2:\");\nvar catExpensiveToys = categoriesCollection.FindOneAndUpdate&lt;Category&gt;(\n    filter: c =&gt; c.Name == \"Expensive toys\",\n    update: Builders&lt;Category&gt;.Update.SetOnInsert(c =&gt; c.Name, \"Expensive toys\"),\n    options: new FindOneAndUpdateOptions&lt;Category, Category&gt; { IsUpsert = true, ReturnDocument = ReturnDocument.After });\n\nproductsCollection.UpdateMany(\n    filter: p =&gt; p.Price &gt; 8000,\n    update: Builders&lt;Product&gt;.Update.Set(p =&gt; p.CategoryID, catExpensiveToys.ID));\n\nvar qProdExpensive = productsCollection\n    .Find(p =&gt; p.CategoryID == catExpensiveToys.ID)\n    .ToList();\nforeach (var p in qProdExpensive)\n    Console.WriteLine($\"\\t\\tName={p.Name}\\tPrice={p.Price}\");\n</code></pre> </li> <li> <p>Query categories that contain any product, then delete the ones that do not belong among this list.</p> <pre><code>//3.3\nConsole.WriteLine(\"\\t3.3:\");\nConsole.WriteLine($\"\\t\\tBefore modification: {categoriesCollection.CountDocuments(_ =&gt; true)} categories\");\n\nvar qProductCategory = new HashSet&lt;ObjectId&gt;(\n    productsCollection\n        .Find(_ =&gt; true)\n        .Project(p =&gt; p.CategoryID)\n        .ToList());\n\ncategoriesCollection.DeleteMany(c =&gt; !qProductCategory.Contains(c.ID));\n\nConsole.WriteLine($\"\\t\\tAfter modification: {categoriesCollection.CountDocuments(_ =&gt; true)} categories\");\n</code></pre> <p>Let us note that this is not an atomic operation. If a product was added concurrently, we could have deleted its category.</p> </li> </ol>"},{"location":"seminar/mssql/","title":"Microsoft SQL Server programming","text":""},{"location":"seminar/mssql/#microsoft-sql-server-programming","title":"Microsoft SQL Server programming","text":"<p>The seminar's goal is to get to know the server-side programming capabilities of the Microsoft SQL Server platform.</p>"},{"location":"seminar/mssql/#pre-requisites","title":"Pre-requisites","text":"<p>Required tools to complete the tasks:</p> <ul> <li>Microsoft SQL Server (LocalDB or Express edition)</li> <li>SQL Server Management Studio</li> <li>Database initialization script: mssql.sql</li> </ul> <p>Recommended to review:</p> <ul> <li>SQL language</li> <li>Microsoft SQL Server programming (stored procedures, triggers)</li> <li>Microsoft SQL Server usage guide</li> </ul>"},{"location":"seminar/mssql/#how-to-work-during-the-seminar","title":"How to work during the seminar","text":"<p>The first four exercises are solved together with the instructor. The final exercise is individual work if time permits.</p> <p>This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!</p>"},{"location":"seminar/mssql/#exercise-0-createcheck-the-database","title":"Exercise 0: Create/check the database","text":"<p>The database resides on each machine; thus, the database you created previously might not be available. First, check if your database exists, and if it does not, create and initialize it. (See the instructions in the first seminar material.)</p>"},{"location":"seminar/mssql/#exercise-1-sql-commands-review","title":"Exercise 1: SQL commands (review)","text":"<p>Write SQL commands/queries for the following exercises.</p> <ol> <li> <p>How many uncompleted orders are there (look for a status other than \"Delivered\")?</p> Solution <pre><code>SELECT COUNT(*)\nFROM [Order] o JOIN Status s ON o.StatusID = s.ID\nWHERE s.Name != 'Delivered'\n</code></pre> <p>We see a <code>join</code> and an aggregation here. (There are other syntaxes for joining tables; refer to the lecture notes.)</p> </li> <li> <p>Which payment methods have not been used at all?</p> Solution <pre><code>SELECT p.Method\nFROM [Order] o RIGHT OUTER JOIN PaymentMethod p ON o.PaymentMethodID = p.ID\nWHERE o.ID IS NULL\n</code></pre> <p>The key in the solution is the <code>outer join</code>, through which we can see the payment method records that have no orders.</p> </li> <li> <p>Let us insert a new customer and query the auto-assigned primary key!</p> Solution <pre><code>INSERT INTO Customer(Name, Login, Password, Email)\nVALUES ('Test Test', 'tt', '********', 'tt@email.com')\n\nSELECT @@IDENTITY\n</code></pre> <p>It is recommended (though not required) to name the columns after <code>insert</code> to be unambiguous. No value was assigned to the ID column, as the definition of that column mandates that the database automatically assigns a new value upon insert. We can query this ID after the insert is completed.</p> </li> <li> <p>One of the categories has the wrong name. Let us change Tricycle to Tricycles!</p> Solution <pre><code>UPDATE Category\nSET Name = 'Tricycles'\nWHERE Name = 'Tricycle'\n</code></pre> </li> <li> <p>Which category contains the largest number of products?</p> Solution <pre><code>SELECT TOP 1 Name, (SELECT COUNT(*) FROM Product WHERE Product.CategoryID = c.ID) AS cnt\nFROM Category c\nORDER BY cnt DESC\n</code></pre> <p>There are many ways this query can be formulated. This is only one possible solution. It also serves as an example of the usage of subqueries.</p> </li> </ol>"},{"location":"seminar/mssql/#exercise-2-inserting-a-new-product-category","title":"Exercise 2: Inserting a new product category","text":"<p>Create a new stored procedure that helps inserting a new product category. The procedure's inputs are the name of the new category, and optionally the name of the parent category. Raise an error if the category already exists, or the parent category does not exist. Let the database generate the primary key for the insertion.</p> Solution <p>Stored procedure</p> <pre><code>CREATE OR ALTER PROCEDURE AddNewCategory\n    @Name NVARCHAR(50),\n    @ParentName NVARCHAR(50)\nAS\n\nBEGIN TRAN\n\n-- Is there a category with identical name?\nDECLARE @ID INT\nSELECT @ID = ID\nFROM Category WITH (TABLOCKX)\nWHERE UPPER(Name) = UPPER(@Name)\n\nIF @ID IS NOT NULL\nBEGIN\n    ROLLBACK;\n    DECLARE @ErrorMessage NVARCHAR(255) = 'Category ' + @Name + ' already exists';\n    THROW 51000, @ErrorMessage, 1;\nEND\n\nDECLARE @ParentID INT\nIF @ParentName IS NOT NULL\nBEGIN\n    SELECT @ParentID = ID\n    FROM Category\n    WHERE UPPER(Name) = UPPER(@ParentName)\n\n    IF @ParentID IS NULL\n    BEGIN\n        ROLLBACK;\n        DECLARE @ParentErrorMessage NVARCHAR(255) = 'Category ' + @ParentName + ' does not exist';\n        THROW 51000, @ParentErrorMessage, 1;\n    END\nEND\n\nINSERT INTO Category\nVALUES(@Name, @ParentID)\n\nCOMMIT\n</code></pre> <p>Testing</p> <p>Let us open a new Query window and execute the following testing instructions.</p> <p><code>exec AddNewCategory 'Beach balls', NULL</code></p> <p>This shall succeed. Let us verify the table contents afterward.</p> <p>Let us repeat the same command; it shall fail now.</p> <p>We can also try with a parent category.</p> <p><code>exec AddNewCategory 'LEGO Star Wars', 'LEGO'</code></p>"},{"location":"seminar/mssql/#exercise-3-maintenance-of-order-status","title":"Exercise 3: Maintenance of order status","text":"<p>Create a trigger that updates the status of each item of an order when the status of the order changes. Do this only for those items of the order that have the same status the order had before the change. Other items in the order should not be affected.</p> Solution <p>Trigger</p> <pre><code>CREATE OR ALTER TRIGGER UpdateOrderStatus\nON [Order]\nFOR UPDATE\nAS\n\nUPDATE OrderItem\nSET StatusID = i.StatusID\nFROM OrderItem oi\nINNER JOIN inserted i ON i.Id = oi.OrderID\nINNER JOIN deleted d ON d.ID = oi.OrderID\nWHERE i.StatusID != d.StatusID\n  AND oi.StatusID = d.StatusID\n</code></pre> <p>Let us make sure we understand the <code>update ... from</code> syntax. The behavior is as follows. We use this command when some of the changes we want to make during the update require data from another table. The syntax is based on the usual <code>update ... set...</code> format extended with a <code>from</code> part, which follows the same syntax as a <code>select from</code>, including the <code>join</code> to gather information from other tables. This allows us to use the joined records and their content in the <code>set</code> statement (that is, a value from a joined record can be on the right side of an assignment).</p> <p>Testing</p> <p>Let us check the status of the order and each item in the order:</p> <pre><code>SELECT OrderItem.StatusID, [Order].StatusID\nFROM OrderItem JOIN [Order] ON OrderItem.OrderID = [Order].ID\nWHERE OrderID = 1\n</code></pre> <p>Let us change the status of the order:</p> <pre><code>UPDATE [Order]\nSET StatusID = 4\nWHERE ID = 1\n</code></pre> <p>Check the status now (the update should have updated all stauses):</p> <pre><code>SELECT OrderItem.StatusID, [Order].StatusID\nFROM OrderItem JOIN [Order] ON OrderItem.OrderID = [Order].ID\nWHERE OrderID = 1\n</code></pre>"},{"location":"seminar/mssql/#exercise-4-summing-the-total-purchases-of-a-customer","title":"Exercise 4: Summing the total purchases of a customer","text":"<p>Let us calculate and store the value of all purchases made by a customer!</p> <ol> <li>Add a new column to the table: <code>ALTER TABLE Customer ADD Total FLOAT</code></li> <li>Calculate the current totals. Let us use a cursor for iterating through all customers.</li> </ol> Solution <pre><code>DECLARE cur_customer CURSOR\n    FOR SELECT ID FROM Customer\nDECLARE @CustomerId INT\nDECLARE @Total FLOAT\n\nOPEN cur_customer\nFETCH NEXT FROM cur_customer INTO @CustomerId\nWHILE @@FETCH_STATUS = 0\nBEGIN\n\n    SELECT @Total = SUM(oi.Amount * oi.Price)\n    FROM CustomerSite s\n    INNER JOIN [Order] o ON o.CustomerSiteID = s.ID\n    INNER JOIN OrderItem oi ON oi.OrderID = o.ID\n    WHERE s.CustomerID = @CustomerId\n\n    UPDATE Customer\n    SET Total = ISNULL(@Total, 0)\n    WHERE ID = @CustomerId\n\n    FETCH NEXT FROM cur_customer INTO @CustomerId\nEND\n\nCLOSE cur_customer\nDEALLOCATE cur_customer\n</code></pre> <p>To verify check the contents of the <code>Customer</code> table.</p>"},{"location":"seminar/mssql/#exercise-5-maintenance-of-the-total-value-individual-exercise","title":"Exercise 5: Maintenance of the total value (individual exercise)","text":"<p>The values calculated in the previous exercise contain the current state. Create a trigger that updates this value whenever a related order is changed. Instead of re-calculating the value, update it with the changes made!</p> Solution <p>The key in the solution is recognizing which table the trigger should be placed on. We are interested in changes in order, but the total value actually depends on the items registered for an order; thus the trigger should react to changes in the order items.</p> <p>The exercise is complicated because the <code>inserted</code> and <code>deleted</code> tables may contain multiple records, possibly even related to multiple customers. A solution for overcoming this obstacle is to use a cursor to process all changes; another option, as below, is aggregating the changes by customer.</p> <p>Trigger</p> <pre><code>CREATE OR ALTER TRIGGER CustomerTotalUpdate\nON OrderItem\nFOR INSERT, UPDATE, DELETE\nAS\n\nUPDATE Customer\nSET Total = ISNULL(Total, 0) + TotalChange\nFROM Customer\nINNER JOIN\n    (SELECT s.CustomerId, SUM(Amount * Price) AS TotalChange\n    FROM CustomerSite s\n    INNER JOIN [Order] o ON o.CustomerSiteID = s.ID\n    INNER JOIN inserted i ON i.OrderID = o.ID\n    GROUP BY s.CustomerId) CustomerChange ON Customer.ID = CustomerChange.CustomerId\n\nUPDATE Customer\nSET Total = ISNULL(Total, 0) - TotalChange\nFROM Customer\nINNER JOIN\n    (SELECT s.CustomerId, SUM(Amount * Price) AS TotalChange\n    FROM CustomerSite s\n    INNER JOIN [Order] o ON o.CustomerSiteID = s.ID\n    INNER JOIN deleted d ON d.OrderID = o.ID\n    GROUP BY s.CustomerID) CustomerChange ON Customer.ID = CustomerChange.CustomerId\n</code></pre> <p>Testing</p> <p>Let us remember the total for the customers.</p> <pre><code>SELECT ID, Total\nFROM Customer\n</code></pre> <p>Change the ordered amount.</p> <pre><code>UPDATE OrderItem\nSET Amount = 3\nWHERE ID = 1\n</code></pre> <p>Check the totals again, should have changed.</p> <pre><code>SELECT ID, Total\nFROM Customer\n</code></pre>"},{"location":"seminar/rest/","title":"REST API & ASP.NET Web API","text":""},{"location":"seminar/rest/#rest-api-aspnet-web-api","title":"REST API &amp; ASP.NET Web API","text":"<p>The seminar's goal is to practice working with REST APIs and the .NET Web API technology.</p>"},{"location":"seminar/rest/#pre-requisites","title":"Pre-requisites","text":"<p>Required tools to complete the tasks:</p> <ul> <li>Microsoft Visual Studio 2022 (not VS Code)</li> <li>Microsoft SQL Server (LocalDB or Express edition)</li> <li>SQL Server Management Studio</li> <li>Postman: https://www.getpostman.com/downloads/</li> <li>Database initialization script: mssql.sql</li> <li>Starter code: https://github.com/bmeviauac01/gyakorlat-rest-kiindulo</li> </ul> <p>Recommended to review:</p> <ul> <li>C# language</li> <li>Entity Framework and Linq</li> <li>REST API and Web API lecture</li> </ul>"},{"location":"seminar/rest/#how-to-work-during-the-seminar","title":"How to work during the seminar","text":"<p>The exercises are solved together with the instructor. A few exercises we can try to solve by ourselves and then discuss the results. The final exercise is individual work if time permits.</p> <p>This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!</p>"},{"location":"seminar/rest/#exercise-0-createcheck-the-database","title":"Exercise 0: Create/check the database","text":"<p>The database resides on each machine; thus, the database you created previously might not be available. First, check if your database exists, and if it does not, create and initialize it. (See the instructions in the first seminar material.)</p>"},{"location":"seminar/rest/#exercise-1-open-starter-project","title":"Exercise 1: Open starter project","text":"<ol> <li> <p>Download the project skeleton!</p> <ul> <li>Open a new command prompt</li> <li>Navigate to a directory, e.g. <code>c:\\work\\NEPTUN</code></li> <li>Execute the following command: <code>git clone --depth 1 https://github.com/bmeviauac01/gyakorlat-rest-kiindulo.git</code></li> </ul> </li> <li> <p>Open the sln file in the <code>rest</code> folder using Visual Studio.</p> </li> <li> <p>Let us examine this project.</p> <ul> <li>This is an ASP.NET Core Web API project. This project is created for hosting REST API backends. It contains a web server internally; thus, when running it using F5 we get a fully functional API able to respond to http requests.</li> <li>Let us examine <code>Program.cs</code>. We do not need to understand everything here. This is like a console application; the <code>Main</code> method here, the entry point that starts a web server.</li> <li> <p>The Entity Framework Code First mapping of our database is in the <code>Dal</code> folder. Class <code>DataDrivenDbContext</code> is the data access class. We need to fix the connection string in the <code>OnConfiguring</code> method in this class.</p> <p>The connection string usually should not be hard-wired in the source code. This is for the sake of simplicity here.</p> </li> <li> <p>There is a test controller in folder <code>Controllers</code>. Let us open and examine the code. Let us note the <code>[ApiController]</code> and <code>[Route]</code> attributes and the inheritance. These make a class a Web API controller. The behavior is automatic: the controller's methods are invoked by the framework when they match the expected signature. This means that no additional configuration is needed here.</p> </li> </ul> </li> <li> <p>Start the application. After building the source code, a console application will start where we will see diagnostic messages. Let us open a browser and navigate to http://localhost:5000/api/values. We should receive a JSON response. Stop the application by pressing Ctrl-C in the console, or stop with Visual Studio.</p> </li> </ol>"},{"location":"seminar/rest/#exercise-2-first-controller-and-testing-with-postman","title":"Exercise 2: First controller and testing with Postman","text":"<p>Create a new Web API controller that responds with a greeting. Test the behavior using Postman.</p> <ol> <li>Delete the existing class <code>ValuesController</code>. Add a new empty Api Controller with the name <code>HelloController</code>: in Solution Explorer right-click the Controllers folder and choose Add / Controller... / API Controller - Empty. The <code>HelloController</code> should respond to URL <code>/api/hello</code>.</li> <li>The application shall respond with a text when GET request is received. Test this endpoint using Postman by sending a GET request to http://localhost:5000/api/hello.</li> <li>Change the REST endpoint by expecting an optional name as a query parameter; if such value is provided, the response greeting should include this name. Test this with Postman: send a name by calling URL http://localhost:5000/api/hello?name=apple.</li> <li>Create a new REST API endpoint that responds to URL http://localhost:5000/api/hello/apple just like the previous one, but the name is in the path here.</li> </ol> Solution <pre><code>[Route(\"api/hello\")]\n[ApiController]\npublic class HelloController : ControllerBase\n{\n    // 2.\n    //[HttpGet]\n    //public ActionResult&lt;string&gt; Hello()\n    //{\n    //    return \"Hello!\";\n    //}\n\n    // 3.\n    [HttpGet]\n    public ActionResult&lt;string&gt; Hello([FromQuery] string name)\n    {\n        if(string.IsNullOrEmpty(name))\n            return \"Hello noname!\";\n        else\n            return \"Hello \" + name;\n    }\n\n    // 4.\n    [HttpGet]\n    [Route(\"{personName}\")] // the liter inside {} in this route must match the parameter name\n    public ActionResult&lt;string&gt; HelloRoute(string personName)\n    {\n        return \"Hello route \" + personName;\n    }\n}\n</code></pre> <p>Let us summarize what we need to create a new WebAPI endpoint:</p> <ul> <li>Inherit from the <code>ControllerBase</code> class and add the <code>[ApiController]</code> attribute.</li> <li>Specify the URL route on the class or above the method (or on both) using the <code>[Route]</code> attribute.</li> <li>Define a method with the right signature (return value and parameters).</li> <li>Choose what type of http queries to respond to using one of the <code>[Http*]</code> attributes.</li> </ul>"},{"location":"seminar/rest/#exercise-3-product-search-api","title":"Exercise 3: Product search API","text":"<p>A real API does not return constant strings. Create an API for searching among the products of our webshop.</p> <ul> <li>Create a new controller.</li> <li>Enable listing products; 5 per page.</li> <li>Enable search based on the name.</li> <li>The data returned should not be the database entity; instead create a new DTO (data transfer object) class in a new folder called <code>Models</code>.</li> </ul> <p>Test the new endpoints.</p> Solution <pre><code>// *********************************\n// Models/Product.cs\n\nnamespace BME.DataDriven.REST.Models\n{\n    public class Product\n    {\n        public Product(int id, string name, double? price, int? stock)\n        {\n            Id = id;\n            Name = name;\n            Price = price;\n            Stock = stock;\n        }\n\n        // Contains only the relevant data; e.g. the database foreign keys are of no use here.\n        // Assignment only via the constructor; this makes it unambiguous\n        // that this is a snapshot of information that cannot be modified.\n\n        public int Id { get; private set; }\n        public string Name { get; private set; }\n        public double? Price { get; private set; }\n        public int? Stock { get; private set; }\n    }\n}\n\n\n\n// *********************************\n// Controllers/ProductsController.cs\n\nusing System.Linq;\nusing Microsoft.AspNetCore.Mvc;\n\nnamespace BME.DataDriven.REST.Controllers\n{\n    [Route(\"api/products\")] // it is better to explicitly specify the url\n    [ApiController]\n    public class ProductsController : ControllerBase\n    {\n        private readonly Dal.DataDrivenDbContext dbContext;\n\n        // The database is obtained through the Dependency Injection service of the framework.\n        // The DbContext is automatically disposed at the end of the request.\n        public ProductsController(Dal.DataDrivenDbContext dbContext)\n        {\n            this.dbContext = dbContext;\n        }\n\n        [HttpGet]\n        public ActionResult&lt;Models.Product[]&gt; List([FromQuery] string search = null, [FromQuery] int from = 0)\n        {\n            IQueryable&lt;Dal.Product&gt; filteredList;\n\n            if (string.IsNullOrEmpty(search)) // no search yields all products\n                filteredList = dbContext.Product;\n            else // search by name\n                filteredList = dbContext.Product.Where(p =&gt; p.Name.Contains(search));\n\n            return filteredList\n                    .Skip(from) // paging: from which product\n                    .Take(5) // 5 items on one page\n                    .Select(p =&gt; new Models.Product(p.Id, p.Name, p.Price, p.Stock)) // db to dto conversion\n                    .ToArray(); // enforce evaluating the IQueryable - otherwise would yield an error\n        }\n    }\n}\n</code></pre> <p>Let us note that we did not need to concern ourselves with JSON serialization. The API returns objects. The framework automatically handles the serialization.</p> <p>Paging is useful to limit the size of the response (and paging is also customary on UIs). Specifying a \u201cfrom\u201d is a simple and frequently used solution.</p> <p>The result of the method before the <code>ToArray</code> is an <code>IQueryable</code>. We may remember that the <code>IQueryable</code> does not contain the result; it is merely a descriptor of the query. If we had no <code>ToArray</code>, we would see an error. When the framework would begin the serialization to JSON, it would start iterating the query; but at this point, the database connection has already been closed. Therefore WebAPI endpoints should not return <code>IEnumerable</code> or <code>IQueryable</code>.</p>"},{"location":"seminar/rest/#exercise-4-editing-products-via-the-api","title":"Exercise 4: Editing products via the API","text":"<p>Add the following functionality to our API:</p> <ul> <li>Fetch the data of a particular product specified by id at url <code>/api/products/id</code>.</li> <li>Update the name, price, and stock of a product.</li> <li>Add a new product (create a new DTO class for input that contains only the name, price, and stock).</li> <li>Delete a product by specifying the id.</li> </ul> <p>Test each endpoint!</p> <p>Inserting a new product you will need the following settings in Postman:</p> <ul> <li>POST request to the correct URL</li> <li>Specify the Body: choose <code>raw</code> and then <code>JSON</code></li> <li>And use the JSON as body below:   <pre><code>{\n  \"name\": \"BME pen\",\n  \"price\": 8900,\n  \"stock\": 100\n}\n</code></pre></li> </ul> <p>Note: In our case the JSON data is deserialized into a newly introduced (see later) <code>Models.NewProduct</code> object. As the property setters are private in this class, JSON field names are mapped to the constructor parameter names of this class (in a case insensitive manner): therefore, it\u2019s important how we name the constuctor parameters in this class.</p> <p>Updating a product you will need the following settings:</p> <ul> <li>PUT request to the correct URL</li> <li>Specify the Body: choose <code>raw</code> and then <code>JSON</code></li> <li>And use the JSON as body below:   <pre><code>{\n  \"ID\": 10,\n  \"name\": \"Silence for one hour\",\n  \"price\": 440,\n  \"stock\": 10\n}\n</code></pre></li> </ul> <p>Note: In our case the JSON data is deserialized into a <code>Models.Product</code> object. As the property setters are private in this class, JSON field names are mapped to the constructor parameter names of this class (in a case insensitive manner): therefore, it\u2019s important how we name the constuctor parameters in this class.</p> <p></p> <p>Make sure to check the headers of the response too! Update and insert should add the Location header. This header should contain the URL to fetch the record.</p> Solution <pre><code>// *********************************\n// Models/NewProduct.cs\n\nnamespace BME.DataDriven.REST.Models\n{\n    public class NewProduct\n    {\n        public NewProduct(string name, double? price, int? stock)\n        {\n            Name = name;\n            Price = price;\n            Stock = stock;\n        }\n\n        public string Name { get; private set; }\n        public double? Price { get; private set; }\n        public int? Stock { get; private set; }\n    }\n}\n\n\n\n// *********************************\n// Controllers/ProductsController.cs\nnamespace BME.DataDriven.REST.Controllers\n{\n    public class ProductsController : ControllerBase\n    {\n        // ...\n\n        // GET api/products/id\n        [HttpGet]\n        [Route(\"{id}\")]\n        public ActionResult&lt;Models.Product&gt; Get(int id)\n        {\n            var dbProduct = dbContext.Product.SingleOrDefault(p =&gt; p.Id == id);\n\n            if (dbProduct == null)\n                return NotFound(); // expected response when an item is not found\n            else\n                return new Models.Product(dbProduct.Id, dbProduct.Name, dbProduct.Price, dbProduct.Stock); // in case of success return the item itself\n        }\n\n        // PUT api/products/id\n        [HttpPut]\n        [Route(\"{id}\")]\n        public ActionResult Modify([FromRoute] int id, [FromBody] Models.Product updated)\n        {\n            if (id != updated.Id)\n                return BadRequest();\n\n            var dbProduct = dbContext.Product.SingleOrDefault(p =&gt; p.Id == id);\n\n            if (dbProduct == null)\n                return NotFound();\n\n            // modifications performed here\n            dbProduct.Name = updated.Name;\n            dbProduct.Price = updated.Price;\n            dbProduct.Stock = updated.Stock;\n\n            // save to database\n            dbContext.SaveChanges();\n\n            return NoContent(); // response 204 NoContent\n        }\n\n        // POST api/products\n        [HttpPost]\n        public ActionResult Create([FromBody] Models.NewProduct newProduct)\n        {\n            var dbProduct = new Dal.Product()\n            {\n                Name = newProduct.Name,\n                Price = newProduct.Price,\n                Stock = newProduct.Stock,\n                CategoryId = 1, // not nice, temporary solution\n                VatId = 1 // not nice, temporary solution\n            };\n\n            // save to database\n            dbContext.Product.Add(dbProduct);\n            dbContext.SaveChanges();\n\n            return CreatedAtAction(nameof(Get), new { id = dbProduct.Id }, new Models.Product(dbProduct.Id, dbProduct.Name, dbProduct.Price, dbProduct.Stock)); // this will add the URL where the new item is available into the header\n        }\n\n        // DELETE api/products/id\n        [HttpDelete]\n        [Route(\"{id}\")]\n        public ActionResult Delete(int id)\n        {\n            var dbProduct = dbContext.Product.SingleOrDefault(p =&gt; p.Id == id);\n\n            if (dbProduct == null)\n                return NotFound();\n\n            dbContext.Product.Remove(dbProduct);\n            dbContext.SaveChanges();\n\n            return NoContent(); // successful delete is signaled with 204 NoContent (could be 200 OK as well if we included the entity)\n        }\n    }\n}\n</code></pre>"},{"location":"seminar/rest/#exercise-5-add-new-product-with-category-and-vat","title":"Exercise 5: Add new product with category and VAT","text":"<p>When creating the new product, we have to specify the category, as well as the value-added tax. Change the insert operation from before by allowing the category name and the tax percentage to be specified. Find the <code>VAT</code> and <code>Category</code> records based on the provided data, or create new records if needed.</p> Solution <pre><code>// *********************************\n// Models/NewProduct.cs\nnamespace BME.DataDriven.REST.Models\n{\n    public class NewProduct\n    {\n        // ...\n        // Also extend the constructor!\n        // Important note: It's important how constructor parameters are named.\n        // Our properties have private setters, and thanks to this json deserialization\n        // maps JSON object field names to constructor parameter names (in a case\n        // insensitive manner).\n\n        public int VATPercentage { get; private set; }\n        public string CategoryName { get; private set; }\n    }\n}\n\n// *********************************\n// Controllers/ProductsController.cs\nnamespace BME.DataDriven.REST.Controllers\n{\n    // ...\n\n    [HttpPost]\n    public ActionResult Create([FromBody] Models.NewProduct newProduct)\n    {\n        var dbVat = dbContext.Vat.FirstOrDefault(v =&gt; v.Percentage == newProduct.VATPercentage);\n        if (dbVat == null)\n            dbVat = new Dal.VAT() { Percentage = newProduct.VATPercentage };\n\n        var dbCat = dbContext.Category.FirstOrDefault(c =&gt; c.Name == newProduct.CategoryName);\n        if (dbCat == null)\n            dbCat = new Dal.Category() { Name = newProduct.CategoryName };\n\n        var dbProduct = new Dal.Product()\n        {\n            Name = newProduct.Name,\n            Price = newProduct.Price,\n            Stock = newProduct.Stock,\n            Category = dbCat,\n            VAT = dbVat\n        };\n\n        // save to database\n        dbContext.Product.Add(dbProduct);\n        dbContext.SaveChanges();\n\n        return CreatedAtAction(nameof(Get), new { id = dbProduct.Id }, new Models.Product(dbProduct.Id, dbProduct.Name, dbProduct.Price, dbProduct.Stock)); // this will add the URL where the new item is available into the header\n    }\n}\n</code></pre>"},{"location":"seminar/rest/#exercise-6-asynchronous-controller-method","title":"Exercise 6: Asynchronous controller method","text":"<p>Let us refactor the previous exercise code for asynchronous execution, that is, let us use <code>async-await</code>. Asynchronous execution utilizes the execution threads of the server more efficiently while waiting for database operations. We can easily make our code asynchronous by relying on the asynchronous support of Entity Framework.</p> Solution <pre><code>[HttpPost]\npublic async Task&lt;ActionResult&gt; Create([FromBody] Models.NewProduct newProduct)\n{\n    var dbVat = await dbContext.Vat.FirstOrDefaultAsync(v =&gt; v.Percentage == newProduct.VATPercentage);\n    if (dbVat == null)\n        dbVat = new Dal.VAT() { Percentage = newProduct.VATPercentage };\n\n    var dbCat = await dbContext.Category.FirstOrDefaultAsync(c =&gt; c.Name == newProduct.CategoryName);\n    if (dbCat == null)\n        dbCat = new Dal.Category() { Name = newProduct.CategoryName };\n\n    var dbProduct = new Dal.Product()\n    {\n        Name = newProduct.Name,\n        Price = newProduct.Price,\n        Stock = newProduct.Stock,\n        Category = dbCat,\n        VAT = dbVat\n    };\n\n    // save to database\n    dbContext.Product.Add(dbProduct);\n    await dbContext.SaveChangesAsync();\n\n    return CreatedAtAction(nameof(Get), new { id = dbProduct.Id }, new Models.Product(dbProduct.Id, dbProduct.Name, dbProduct.Price, dbProduct.Stock)); // this will add the URL where the new item is available into the header\n}\n</code></pre> <p>Let us see how simple this was. Entity Framework provides us the <code>...Async</code> methods, and we only have to <code>await</code> them, and update the method signature a litte. Everything else is taken care of by the framework.</p> <p>Note. The <code>async-await</code> is a .NET frmaework feature supported by both ASP.NET Core and Entity Framework. It is also supported by a lot of other libraries as well.</p>"},{"location":"seminar/transactions/","title":"Transactions","text":""},{"location":"seminar/transactions/#transactions","title":"Transactions","text":"<p>The goal is to examine transaction handling of MS SQL Server, understand the practical limits to serializable isolation level, and controlling data dependency in read committed isolation level.</p>"},{"location":"seminar/transactions/#pre-requisites","title":"Pre-requisites","text":"<p>Required tools to complete the tasks:</p> <ul> <li>Microsoft SQL Server (LocalDB or Express edition)</li> <li>SQL Server Management Studio</li> <li>Database initialization script: mssql.sql</li> </ul> <p>Recommended to review:</p> <ul> <li>Transaction properties, isolation levels</li> <li>Microsoft SQL Server usage guide</li> </ul>"},{"location":"seminar/transactions/#how-to-work-during-the-seminar","title":"How to work during the seminar","text":"<p>The seminar is lead by the instructor. After getting to know the tools we use, the exercises are solved together. Experienced behavior is summarized and explained.</p> <p>This guide summarizes and explains the behavior. Before looking at these provided answers, we should think first!</p>"},{"location":"seminar/transactions/#exercise-1-create-a-database-in-ms-sql-server","title":"Exercise 1: Create a database in MS SQL Server","text":"<p>We need a database first. Usually, the database is located on a central server, but we often run a server on our machine for development.</p> <ol> <li> <p>Connect to Microsoft SQL Server using SQL Server Management Studio. Start Management Studio and use the following connection details:</p> <ul> <li>Server name: <code>(localdb)\\mssqllocaldb</code></li> <li>Authentication: <code>Windows authentication</code></li> </ul> </li> <li> <p>Create a new database (if it does not exist yet); the name should be your Neptun code: in Object Explorer right-click Databases and choose Create Database.</p> </li> <li> <p>Instantiate the sample database using the script. Open a new Query window, paste the script into the window, then execute it. Make sure to select the right database in the toolbar dropdown.</p> <p></p> </li> <li> <p>Verify that the tables are created. If the Tables folder was open before, you need to refresh it.</p> <p>.</p> </li> </ol>"},{"location":"seminar/transactions/#exercise-2-concurrent-transactions","title":"Exercise 2: Concurrent transactions","text":"<p>To simulate concurrent transactions, you need two Query windows by clicking the New Query button twice. You can align the windows next to each other by right-clicking the Query header and choosing New Vertical Tab group:</p> <p></p> <p>Use the following scheduling. Transaction T1 checks the status of order 4, while transaction T2 changes the status.</p> <ol> <li> <p>T1 transaction</p> <pre><code>-- List the order and the related items with their status\nSELECT s1.Name, p.Name, s2.Name\nFROM [Order] o, OrderItem oi, Status s1, Status s2, Product p\nWHERE o.Id = oi.OrderID\n  AND o.ID = 4\n  AND o.StatusID = s1.ID\n  AND oi.StatusID = s2.ID\n  AND p.ID = oi.ProductID\n</code></pre> <p><code>[Order]</code></p> <p>The brackets in <code>[Order]</code> are needed to distinguish it from the <code>order by</code> command.</p> </li> <li> <p>T2 transaction</p> <pre><code>-- Change the status or the order\nUPDATE [Order]\nSET StatusID = 4\nWHERE ID = 4\n</code></pre> </li> <li> <p>T1 transaction: repeat the same command as in step 1</p> </li> <li> <p>T2 transaction</p> <pre><code>-- Change the status of each item in the order\nUPDATE OrderItem\nSET StatusID = 4\nWHERE OrderID = 4\n</code></pre> </li> <li> <p>T1 transaction: repeat the same command as in step 1</p> </li> </ol> What did you experience? Why? <p>In the beginning, everything was in status \"Packaged\", which is fine (the items in the order and the order itself had the same status). But after we changed the status of the order, the status seemed controversial: the order and the items had different statuses. We have to understand that the database itself was not inconsistent, as the database's integration requirements allow this. However, from a business perspective, there was an inconsistency.</p> <p>SQL Server, by default, runs in auto-commit mode. That is, every sql statement is a transaction by itself, which is committed when completed. Thus the problem was that our modifications were executed in separate transactions.</p> <p>In order to handle the two changes together, we would need to combine them into a single transaction.</p>"},{"location":"seminar/transactions/#exercise-3-using-transactions-read-committed-isolation-level","title":"Exercise 3: using transactions, read committed isolation level","text":"<p>Let us repeat the previous exercise so that the two modifications form a single transaction:</p> <ul> <li>T2 transaction should begin with a <code>begin tran</code> command, and finish with a <code>commit</code> statement.</li> <li>When changing the status, the new status should be 3 (to have an actual change in the data).</li> </ul> What did you experience? Why? <p>While data modification is underway in T2, the query statement in T1 will wait. It will wait until the data modification transaction is completed. The <code>select</code> statement wants to place a reader lock on the records, but the other concurrent transaction is editing these records and has an exclusive writer lock on them.</p> <p>Let us remember that the default isolation level is read committed. This isolation level on this platform means that data under modification cannot be accessed, not even for reading. This is a matter of implementation; the SQL standard does not specify this (e.g., in Oracle Server the previously committed state of each record is available). In other isolation levels, MSSQL Server behaves differently; e.g., in the snapshot isolation level, the version of the data before the modification is accessible.</p>"},{"location":"seminar/transactions/#exercise-4-aborting-transactions-rollback-in-read-committed-isolation-level","title":"Exercise 4: aborting transactions (rollback) in read committed isolation level","text":"<p>Let us repeat the same command sequence, including the transaction, but let us abort the modification operation in the middle.</p> <ol> <li> <p>T1 transaction</p> <pre><code>-- List the order and the related items with their status\nSELECT s1.Name, p.Name, s2.Name\nFROM [Order] o, OrderItem oi, Status s1, Status s2, Product p\nWHERE o.Id = oi.OrderID\n  AND o.ID = 4\n  AND o.StatusID = s1.ID\n  AND oi.StatusID = s2.ID\n  AND p.ID = oi.ProductID\n</code></pre> </li> <li> <p>T2 transaction</p> <pre><code>-- Start new transaction\nBEGIN TRAN\n\n-- Change the order status\nUPDATE [Order]\nSET StatusID = 4\nWHERE ID = 4\n</code></pre> </li> <li> <p>T1 transaction: repeat the same command as in step 1</p> </li> <li> <p>T2 transaction</p> <pre><code>-- Abort the transaction\nROLLBACK\n</code></pre> </li> </ol> What did you experience? Why? <p>Similarly to the previous exercise, the read operation was forced to wait while the modification transaction was underway. When this modification was aborted, the result of the read query arrived immediately. We are still using read committed isolation level; hence we must not see data being modified. But once the modification transaction finished, either successfully with <code>commit</code> or with a <code>rollback</code>, the data records are once again available.</p> <p>Let us understand that we have just avoided the problem of dirty read. If the read query showed us the uncommitted modification, we would have seen values that would have been invalid after the <code>rollback</code>.</p>"},{"location":"seminar/transactions/#exercise-5-placing-an-order-using-serializable-isolation-level","title":"Exercise 5: Placing an order using serializable isolation level","text":"<p>Before we begin, let us get rid of any pending transactions we may have. Let us issue a few <code>rollback</code> statements in both windows.</p> <p>Let us have two concurrent transactions, both placing an order. We must allow an order for a product only if we have enough stock left. To properly isolate the effect of the transactions, let use serializable isolation level.</p> <ol> <li> <p>T1 transaction</p> <pre><code>SET TRANSACTION ISOLATION LEVEL SERIALIZABLE;\nBEGIN TRAN;\n\n-- Check the product stock\nSELECT *\nFROM Product\nWHERE ID = 2\n</code></pre> </li> <li> <p>T2 transaction</p> <pre><code>SET TRANSACTION ISOLATION LEVEL SERIALIZABLE;\nBEGIN TRAN;\n\nSELECT *\nFROM Product\nWHERE ID = 2\n</code></pre> </li> <li> <p>T1 transaction</p> <pre><code>-- Check the registered, but unprocessed orders for the same product\nSELECT SUM(Amount)\nFROM OrderItem\nWHERE ProductID = 2\n  AND StatusID = 1\n</code></pre> </li> <li> <p>T2 transaction</p> <pre><code>SELECT SUM(Amount)\nFROM OrderItem\nWHERE ProductID = 2\n  AND StatusID = 1\n</code></pre> </li> <li> <p>T1 transaction</p> <pre><code>-- Since the order can be completed, store the order\nINSERT INTO OrderItem(OrderID, ProductID, Amount, StatusID)\nVALUES(2, 2, 3, 1)\n</code></pre> </li> <li> <p>T2 transaction</p> <pre><code>INSERT INTO OrderItem(OrderID, ProductID, Amount, StatusID)\nVALUES(3, 2, 3, 1)\n</code></pre> </li> <li> <p>T1 transaction</p> <pre><code>COMMIT\n</code></pre> </li> <li> <p>T2 transaction</p> <pre><code>COMMIT\n</code></pre> </li> </ol> What did you experience? Why? <p>A deadlock will occur due to the serializable isolation level, as both transactions require exclusive access to the table <code>OrderItem</code>. The query <code>select sum</code> - with the requirement to protect from unrepeatable reads - adds reader locks to the records. Thus the  <code>insert</code> cannot complete, as it needs write access. This effectively means that both transactions are waiting for a lock that the other one holds.</p> <p>The result of the deadlock is the abortion of one of the transactions. This is the expected and correct behavior in these cases because it prevents the same problem we are trying to avoid (to sell more products than we have in stock).</p> <p>Let us repeat the same sequence of steps, but this time, the products should be different. This simulates two concurrent orders for different products.</p> <ul> <li>Before we begin, let us get rid of any pending transactions we may have. Let us issue a few <code>rollback</code> statements in both windows.</li> <li>Let us replace the <code>ID</code> or <code>ProductID</code> values: one of the transactions should use ID 2 and the other one ID 3.</li> </ul> What did you experience? Why? <p>Even when the two orders reference different products a deadlock occurs. The locking mechanism behind the statement <code>select sum</code> locks the entire table, because it is not able to distinguish the records by <code>ProductID</code>. This is not unexpected, as the fact that two concurrent orders referencing different products are allowed, is a business requirement; the database has no knowledge of this.</p> <p>In other words, the serializable isolation level is too strict in this case. This is the reason that serializable is not frequently used in practice.</p>"},{"location":"seminar/transactions/#exercise-6-order-registration-with-read-committed-isolation-level","title":"Exercise 6: Order registration with read committed isolation level","text":"<p>Let us consider what would happen in the previous exercise if the isolation level was left at default? Would there be any deadlock? Would the result be correct?</p> What would we expect? Why? <p>If we used the default isolation level, the behavior would be incorrect. The read committed isolation level would not protect us from a concurrent transaction inserting a new record that could potentially result in selling more products than available. This would be a manifestation of the unrepeatable read concurrency problem.</p> <p>We can conclude thus that the serializable isolation level was not unnecessary. It did in fact, protect us from a valid concurrency problem.</p>"},{"location":"seminar/transactions/#exercise-7-manual-locking","title":"Exercise 7: Manual locking","text":"<p>Before we begin, let us get rid of any pending transactions we may have. Let us issue a few <code>rollback</code> statements in both windows.</p> <p>Using read committed isolation level, let us find a solution that only prohibits concurrent orders of the same product. You can assume that all copies of the concurrent program run the same logic.</p> <p>The solution is based on manual locks we can place on records. These locks |(similarly to the automatic ones) have a lifespan identical to the transaction.</p> <pre><code>SELECT *\nFROM tablename WITH(XLOCK)\n...\n</code></pre> Where do we place this lock? How does the order registration process look like? <p>The key to this exercise is understanding where the lock should be placed. The question is what to lock. The answer is the product: we want to avoid placing orders of the same product. Thus we place a lock on the product record.</p> <p>The order process is as follows:</p> <ol> <li> <p>T1 transaction</p> <pre><code>SET TRANSACTION ISOLATION LEVEL READ COMMITTED;\nBEGIN TRAN;\n\nSELECT *\nFROM Product WITH (XLOCK)\nWHERE ID = 2\n</code></pre> </li> <li> <p>T2 transaction</p> <pre><code>SET TRANSACTION ISOLATION LEVEL READ COMMITTED;\nBEGIN TRAN;\n\nSELECT *\nFROM Product WITH (XLOCK)\nWHERE ID = 3\n</code></pre> </li> <li> <p>T1 transaction</p> <pre><code>SELECT SUM(Amount)\nFROM OrderItem\nWHERE ProductID = 2\n  AND StatusID = 1\n</code></pre> </li> <li> <p>T2 transaction</p> <pre><code>SELECT SUM(Amount)\nFROM OrderItem\nWHERE ProductID = 3\n  AND StatusID = 1\n</code></pre> </li> <li> <p>T1 transaction</p> <pre><code>INSERT INTO OrderItem(OrderID, ProductID, Amount, StatusID)\nVALUES(2, 2, 3, 1)\n</code></pre> </li> <li> <p>T2 transaction</p> <pre><code>INSERT INTO OrderItem(OrderID, ProductID, Amount, StatusID)\nVALUES(3, 3, 3, 1)\n</code></pre> </li> <li> <p>T1 transaction</p> <pre><code>COMMIT\n</code></pre> </li> <li> <p>T2 transaction</p> <pre><code>COMMIT\n</code></pre> </li> </ol>"},{"location":"seminar/transactions/#exercise-8-table-locking","title":"Exercise 8: Table locking","text":"<p>There is another option for manual locking by locking entire tables:</p> <pre><code>SELECT *\nFROM tablename WITH(TABLOCKX)\n...\n</code></pre> This might seem a simple solution, but why is this a not recommended option? <p>In our scenario, the table lock should be placed on the order item table. But effectively, this would mean the same thing as using serializable isolation level: there would be no deadlocks; however, there would be no concurrency allowed either.</p>"}]}